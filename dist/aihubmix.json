{
  "id": "aihubmix",
  "name": "AIHubMix",
  "display_name": "AIHubMix",
  "updated_at": "2025-12-12T15:06:41.824Z",
  "models": [
    {
      "id": "claude-opus-4-5",
      "name": "claude-opus-4-5",
      "display_name": "claude-opus-4-5",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 5,
        "output": 25,
        "cache_read": 0.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Claude Opus 4.5 is Anthropic’s latest frontier reasoning model, optimized for complex engineering, agentic workflows, and long-horizon computer use. It features strong multimodal capabilities, improved resistance to prompt injection, and a new Verbosity parameter to control token efficiency. With advanced tool use, extended context, and multi-agent support, Opus 4.5 excels in autonomous research, debugging, planning, and spreadsheet/browser operations.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.5,
          "cache_write": 6.25,
          "input": 5,
          "output": 25
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-3-pro-image-preview",
      "name": "gemini-3-pro-image-preview",
      "display_name": "gemini-3-pro-image-preview",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 12,
        "cache_read": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemini-3-Pro-Image-Preview (Nano Banana Pro) is a high-performance image generation and editing model built on Gemini 3 Pro. It delivers enhanced multimodal understanding and real-world semantic reasoning, enabling fast creation of well-structured visual content such as infographics, product sketches, and multi-subject scenes. It can also leverage real-time knowledge through Search grounding. The model excels in text rendering, consistent multi-image blending, and identity preservation, while offering fine-grained creative controls like localized edits, lighting and focus adjustments, camera transformations, and flexible aspect ratios. It’s ideal for rapid design, concept previews, product visualization, and everyday image generation workflows.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 2,
          "input": 2,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.2",
      "name": "gpt-5.2",
      "display_name": "gpt-5.2",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.75,
        "output": 14,
        "cache_read": 0.175
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT‑5.2 builds on GPT‑5.1 with improvements that make interactions more reliable, flexible, and user-friendly. The focus is on delivering clearer responses, better adaptability, and enhanced control for diverse tasks. Key highlights include:\n\nAccelerate agent development: Build AI agents for analytics, decision support, code modernization, and data pipelines.\nGreater consistency: Produces more predictable and accurate outputs aligned with user instructions.\nAdaptive reasoning: Handles complex queries with improved depth while remaining efficient for simpler tasks.\nExpanded context handling: Supports longer inputs for multi-step workflows and richer conversations.\nRefined customization: Offers better control over tone, style, and structured output for tailored experiences.\nBroader applicability: Strengthens performance across a wide range of domains while maintaining clarity and safety.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "function_calling",
          "structured_outputs",
          "web",
          "tools"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.175,
          "input": 1.75,
          "output": 14
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.2-chat-latest",
      "name": "gpt-5.2-chat-latest",
      "display_name": "gpt-5.2-chat-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.75,
        "output": 14,
        "cache_read": 0.175
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "GPT-5.2Chat refers to the GPT-5.2 snapshot currently used in ChatGPT and is optimized for conversational use cases. While GPT-5.2 is recommended for most API applications, GPT-5.2Chat is ideal for testing the latest improvements in chat-based interactions.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.175,
          "input": 1.75,
          "output": 14
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-3-pro-preview",
      "name": "gemini-3-pro-preview",
      "display_name": "gemini-3-pro-preview",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 12,
        "cache_read": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "google state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs",
          "web",
          "deepsearch",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.5,
          "input": 2,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.2-pro",
      "name": "gpt-5.2-pro",
      "display_name": "gpt-5.2-pro",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 21,
        "output": 168,
        "cache_read": 2.1
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5.2 pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests, and other advanced API features in the future. Since GPT-5.2 pro is designed to tackle tough problems, some requests may take several minutes to finish. To avoid timeouts, try using background mode. GPT-5.2 pro supports reasoning.effort: medium, high, xhigh.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "web",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 2.1,
          "input": 21,
          "output": 168
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.1",
      "name": "gpt-5.1",
      "display_name": "gpt-5.1",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5 is OpenAI’s most advanced language model, designed for complex tasks that require step-by-step reasoning, precise instruction following, and high reliability. It improves reasoning, code generation, and prompt understanding—including test-time routing and intent cues like “think hard about this”—while reducing hallucination and sycophancy.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "web",
          "tools",
          "deepsearch",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.1-codex-max",
      "name": "gpt-5.1-codex-max",
      "display_name": "gpt-5.1-codex-max",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5.1-Codex-Max is a frontier programming model built for the agent-driven era. Powered by an upgraded core reasoning architecture, it is specially trained for complex agentic tasks in software engineering, mathematics, and scientific research. It delivers faster performance, greater stability, and higher token efficiency across the entire development lifecycle, including code generation, refactoring, debugging, and engineering collaboration. With native support for multiple context windows and a built-in compaction mechanism, the model can coherently process millions of tokens within a single task.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "function_calling",
          "structured_outputs",
          "thinking"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-3-pro-preview-search",
      "name": "gemini-3-pro-preview-search",
      "display_name": "gemini-3-pro-preview-search",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 12,
        "cache_read": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemini-3-pro-search integrates Google's official search functionality; the search feature incurs an additional separate fee log directly incorporated into the scoring, but the log details are not displayed; this will be fixed in the future to show the details; it only supports OpenAI-compatible format calls and does not support the Gemini SDK; for the Gemini native SDK, please directly set the official search parameters.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "web",
          "deepsearch",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.5,
          "input": 2,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.1-chat-latest",
      "name": "gpt-5.1-chat-latest",
      "display_name": "gpt-5.1-chat-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "GPT-5.1 Chat refers to the GPT-5.1 snapshot currently used in ChatGPT and is optimized for conversational use cases. While GPT-5.1 is recommended for most API applications, GPT-5.1 Chat is ideal for testing the latest improvements in chat-based interactions.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.1-codex",
      "name": "gpt-5.1-codex",
      "display_name": "gpt-5.1-codex",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5.1-Codex is a version of GPT-5 optimized for agentic coding tasks in Codex or similar environments. It's available in the Responses API only and the underlying model snapshot will be regularly updated.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5.1-codex-mini",
      "name": "gpt-5.1-codex-mini",
      "display_name": "gpt-5.1-codex-mini",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.25,
        "output": 2,
        "cache_read": 0.025
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5.1 Codex mini is a smaller, more cost-effective, less-capable version of GPT-5.1-Codex.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.025,
          "input": 0.25,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "mistral-large-3",
      "name": "mistral-large-3",
      "display_name": "mistral-large-3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.5,
        "output": 1.5
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Mistral Large 3 is a MoE model with 67.5B total parameters and 41B active parameters, supporting a 256K-token context window. Trained from scratch on 3,000 NVIDIA H200 GPUs, it is one of the strongest permissively licensed open-weight models available.\n\nDesigned for advanced reasoning and long-context understanding, Mistral Large 3 delivers performance on par with the best instruction-tuned open-weight models for general-purpose tasks, while also offering image understanding capabilities. Its multilingual strengths are particularly notable for non-English/Chinese languages, making it well-suited for global applications.\n\nTypical use cases include enterprise assistants, multilingual customer support, content generation and editing, data analysis over long documents, code assistance, and research workflows that require handling large corpora or complex instructions. With its MoE architecture, Mistral Large 3 balances strong performance with efficient inference, providing a versatile backbone for building reliable, production-grade AI systems.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-haiku-4-5",
      "name": "claude-haiku-4-5",
      "display_name": "claude-haiku-4-5",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 5.5,
        "cache_read": 0.11
      },
      "limit": {
        "context": 204800,
        "output": 204800
      },
      "metadata": {
        "description": "Claude Haiku 4.5 is a fast, affordable, and highly capable AI model, excelling at coding and agentic tasks. Its combination of speed and low cost makes it ideal for powering real-time applications like chatbots, high-volume free services, and specialized \"sub-agents\" for complex tasks in coding, finance, and research. It can also handle common business tasks like creating office documents and assisting with strategy and analysis.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.11,
          "cache_write": 1.375,
          "input": 1.1,
          "output": 5.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-sonnet-4-5",
      "name": "claude-sonnet-4-5",
      "display_name": "claude-sonnet-4-5",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5,
        "cache_read": 0.33
      },
      "limit": {
        "context": 1000000,
        "output": 1000000
      },
      "metadata": {
        "description": "Sonnet 4.5 is the best model in the world for agents, coding, and computer usage. It is also our most accurate and detailed model for long-running tasks, with enhanced knowledge in coding, finance, and cybersecurity.  \nThis model supports a thinking parameter to enable thinking requests in Claude mode.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.33,
          "cache_write": 4.125,
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-image",
      "name": "gemini-2.5-flash-image",
      "display_name": "gemini-2.5-flash-image",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "image",
          "text"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.3
      },
      "limit": {
        "context": 32800,
        "output": 32800
      },
      "metadata": {
        "description": "Gemini 2.5 Flash Image (Nano-Banana) is a state-of-the-art image generation and editing model that enables seamless blending of multiple images into a single composition while maintaining character consistency for rich visual storytelling. It supports precise, targeted image transformations through natural language instructions and leverages built-in world knowledge for both image generation and editing, making it well suited for creative design, content production, advertising, and visual expression workflows.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "image",
          "text"
        ],
        "pricing": {
          "cache_read": 0.3,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "glm-4.6",
      "name": "glm-4.6",
      "display_name": "glm-4.6",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 204800,
        "output": 204800
      },
      "metadata": {
        "description": "GLM-4.6 is Zhipu’s latest flagship model (total parameters 355B, activation parameters 32B), comprehensively surpassing GLM-4.5. Its coding capability is aligned with Claude Sonnet 4, making it a top domestic coding model; the context window has been expanded from 128K to 200K, better suited for long code and agent tasks; inference capabilities have been significantly enhanced and support tool invocation during processing; improvements have been made in tool calling, search agents, writing style, role play, and multilingual translation. The model is named glm-4.6 and is provided by three vendors, with calls prioritized to the Sophnet platform.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4.6v",
      "name": "glm-4.6v",
      "display_name": "glm-4.6v",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.137,
        "output": 0.411,
        "cache_read": 0.0274
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Zhipu's latest visual reasoning model achieves state-of-the-art visual understanding accuracy at the same scale upon release. It natively supports tool invocation, can automatically complete tasks, supports ultra-long 128K context length, and allows flexible toggling of reasoning.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.0274,
          "input": 0.137,
          "output": 0.411
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "grok-4-1-fast-non-reasoning",
      "name": "grok-4-1-fast-non-reasoning",
      "display_name": "grok-4-1-fast-non-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.5,
        "cache_read": 0.05
      },
      "limit": {
        "context": 2000000,
        "output": 2000000
      },
      "metadata": {
        "description": "Grok 4.1 is a new conversational model with significant improvements in real-world usability, delivering exceptional performance in creative, emotional, and collaborative interactions. It is more perceptive to nuanced user intent, more engaging to converse with, and more coherent in personality, while fully preserving its core intelligence and reliability. Built on large-scale reinforcement learning infrastructure, the model is optimized for style, personality, helpfulness, and alignment, and leverages frontier agentic reasoning models as reward evaluators to autonomously assess and iterate on responses at scale, significantly enhancing overall interaction quality.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.05,
          "input": 0.2,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "grok-4-1-fast-reasoning",
      "name": "grok-4-1-fast-reasoning",
      "display_name": "grok-4-1-fast-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.5,
        "cache_read": 0.05
      },
      "limit": {
        "context": 2000000,
        "output": 2000000
      },
      "metadata": {
        "description": "Grok 4.1 is a new conversational model with significant improvements in real-world usability, delivering exceptional performance in creative, emotional, and collaborative interactions. It is more perceptive to nuanced user intent, more engaging to converse with, and more coherent in personality, while fully preserving its core intelligence and reliability. Built on large-scale reinforcement learning infrastructure, the model is optimized for style, personality, helpfulness, and alignment, and leverages frontier agentic reasoning models as reward evaluators to autonomously assess and iterate on responses at scale, significantly enhancing overall interaction quality.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.05,
          "input": 0.2,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5",
      "name": "gpt-5",
      "display_name": "gpt-5",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5 is OpenAI’s most advanced general-purpose model, delivering major improvements in reasoning, code quality, and overall user experience. It is optimized for complex tasks that require step-by-step reasoning, precise instruction following, and high accuracy in high-stakes scenarios. The model supports test-time routing and advanced prompt understanding, including user-specified intent such as “think hard about this,” while significantly reducing hallucination and sycophancy and improving performance in coding, writing, and health-related tasks.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "deepseek-v3.2-fast",
      "name": "deepseek-v3.2-fast",
      "display_name": "deepseek-v3.2-fast",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.096,
        "output": 3.288,
        "cache_read": 1.096
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "SophNet's exclusively developed DeepSeek V3.2 Fast is the high-TPS, high-speed version of DeepSeek V3.2, achieving up to 100t/s with faster response!",
        "typeHints": [
          "llm"
        ],
        "features": [
          "web",
          "function_calling"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 1.096,
          "input": 1.096,
          "output": 3.288
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-v3.2",
      "name": "deepseek-v3.2",
      "display_name": "deepseek-v3.2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.302,
        "output": 0.453,
        "cache_read": 0.0302
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "DeepSeek-V3.2 is an efficient large language model equipped with DeepSeek Sparse Attention and reinforced reasoning performance, but its core strength lies in powerful agentic capabilities—enabled by large-scale task-synthesis that tightly integrates reasoning with real-world tool use, delivering robust, compliant, and generalizable agent behaviour. Users can toggle deeper reasoning through the reasoning_enabled switch.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0302,
          "input": 0.302,
          "output": 0.453
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-v3.2-speciale",
      "name": "deepseek-v3.2-speciale",
      "display_name": "deepseek-v3.2-speciale",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.302,
        "output": 0.453,
        "cache_read": 0.0302
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "DeepSeek-V3.2-Speciale is an enhanced long-thinking variant of DeepSeek-V3.2 that integrates the theorem-proving capabilities of DeepSeek-Math-V2. It excels in instruction following, mathematical reasoning, and logical verification, achieving performance comparable to Gemini-3.0-Pro on major reasoning benchmarks and winning gold medals at IMO 2025, CMO 2025, ICPC World Finals 2025, and IOI 2025. However, due to its long-thinking mechanism, the model may overthink simple questions, so task complexity should be carefully controlled during usage. The model only supports the thinking version.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0302,
          "input": 0.302,
          "output": 0.453
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-v3.2-think",
      "name": "deepseek-v3.2-think",
      "display_name": "deepseek-v3.2-think",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.302,
        "output": 0.453,
        "cache_read": 0.0302
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "DeepSeek-V3.2 is an efficient large language model equipped with DeepSeek Sparse Attention and reinforced reasoning performance, but its core strength lies in powerful agentic capabilities—enabled by large-scale task-synthesis that tightly integrates reasoning with real-world tool use, delivering robust, compliant, and generalizable agent behaviour. Users can toggle deeper reasoning through the reasoning_enabled switch.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "web",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0302,
          "input": 0.302,
          "output": 0.453
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-math-v2",
      "name": "deepseek-math-v2",
      "display_name": "deepseek-math-v2",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.492,
        "output": 1.968,
        "cache_read": 0.0984
      },
      "limit": {
        "context": 163000,
        "output": 163000
      },
      "metadata": {
        "description": "The mathematical reasoning of large language models has shifted from pursuing correct answers to ensuring rigorous processes. Research proposes a new paradigm of \"self-verification,\" training specialized verifiers to evaluate proof steps and using this to train generators for self-error correction. The two co-evolve, pushing the boundaries of capability. Ultimately, the model achieves gold medal level in top competitions like the IMO, demonstrating the great potential of deep reasoning.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "web"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0984,
          "input": 0.492,
          "output": 1.968
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-V3.2-Exp",
      "name": "DeepSeek-V3.2-Exp",
      "display_name": "DeepSeek-V3.2-Exp",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 0.411,
        "cache_read": 0.0274
      },
      "limit": {
        "context": 163000,
        "output": 163000
      },
      "metadata": {
        "description": "The model DeepSeek-V3.2-Exp is officially named deepseek-chat on the website. It is an experimental version. As an intermediate step towards the next-generation architecture, V3.2-Exp introduces DeepSeek Sparse Attention (a sparse attention mechanism) based on V3.1-Terminus, exploring and validating",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0274,
          "input": 0.274,
          "output": 0.411
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-V3.2-Exp-Think",
      "name": "DeepSeek-V3.2-Exp-Think",
      "display_name": "DeepSeek-V3.2-Exp-Think",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 0.411,
        "cache_read": 0.0274
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "The model DeepSeek-V3.2-Exp-Think is officially named deepseek-reasoner. It is an experimental version. As an intermediate step towards the next-generation architecture, V3.2-Exp introduces DeepSeek Sparse Attention (a sparse attention mechanism) based on V3.1-Terminus, exploring and validating exploratory optimizations for training and inference efficiency on long texts.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0274,
          "input": 0.274,
          "output": 0.411
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-5-codex",
      "name": "gpt-5-codex",
      "display_name": "gpt-5-codex",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5-Codex is a version of GPT-5 optimized for autonomous coding tasks in Codex or similar environments. It is only available in the Responses API, and the underlying model snapshots will be updated regularly. https://docs.aihubmix.com/en/api/Responses-API You can also use it in codex-cll; see https://docs.aihubmix.com/en/api/Codex-CLI for using codex-cll through Aihubmix.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "DeepSeek-V3.1-Terminus",
      "name": "DeepSeek-V3.1-Terminus",
      "display_name": "DeepSeek-V3.1-Terminus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.56,
        "output": 1.68
      },
      "limit": {
        "context": 160000,
        "output": 160000
      },
      "metadata": {
        "description": "DeepSeek-V3.1 non-thinking mode has now been updated to the DeepSeek-V3.1-Terminus version.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.56,
          "output": 1.68
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-V3.1-Think",
      "name": "DeepSeek-V3.1-Think",
      "display_name": "DeepSeek-V3.1-Think",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.56,
        "output": 1.68
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Thinking mode of DeepSeek-V3.1;  \nDeepSeek V3.1 is a text generation model provided by DeepSeek, featuring a hybrid reasoning architecture that achieves an effective integration of thinking and non-thinking modes.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.56,
          "output": 1.68
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-5-pro",
      "name": "gpt-5-pro",
      "display_name": "gpt-5-pro",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 15,
        "output": 120
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5 pro uses more compute to think harder and provide consistently better answers.\n\nGPT-5 pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests, and other advanced API features in the future. Since GPT-5 pro is designed to tackle tough problems, some requests may take several minutes to finish. To avoid timeouts, try using background mode. As our most advanced reasoning model, GPT-5 pro defaults to (and only supports) reasoning.effort: high. GPT-5 pro does not support code interpreter.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 15,
          "output": 120
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5-mini",
      "name": "gpt-5-mini",
      "display_name": "gpt-5-mini",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.25,
        "output": 2,
        "cache_read": 0.025
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5 mini is a faster, more cost-efficient version of GPT-5. It's great for well-defined tasks and precise prompts.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.025,
          "input": 0.25,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5-nano",
      "name": "gpt-5-nano",
      "display_name": "gpt-5-nano",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.4,
        "cache_read": 0.005
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, designed specifically for developer tools and environments that demand rapid interactions and ultra-low latency. While it offers a more lightweight solution with limited reasoning depth compared to its larger counterparts, GPT-5-Nano excels in core capabilities such as instruction-following and maintaining critical safety features. As the successor to GPT-4.1-nano, it provides an optimal choice for cost-sensitive or real-time applications, where efficiency and speed are paramount. Particularly well-suited for summarization and classification tasks, GPT-5-Nano is a powerful tool for developers needing a swift, reliable AI model for streamlined processes.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.005,
          "input": 0.05,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-5-chat-latest",
      "name": "gpt-5-chat-latest",
      "display_name": "gpt-5-chat-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.125
      },
      "limit": {
        "context": 400000,
        "output": 400000
      },
      "metadata": {
        "description": "GPT-5 Chat points to the GPT-5 snapshot currently used in ChatGPT. GPT-5 is our next-generation, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.125,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-opus-4-1",
      "name": "claude-opus-4-1",
      "display_name": "claude-opus-4-1",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 16.5,
        "output": 82.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Opus 4.1 is an upgraded version of Claude Opus 4, with improvements mainly in agent tasks, practical coding, and reasoning. Compared to Opus 4, there is a slight improvement in software engineering accuracy; Opus 4.1 has higher accuracy at 74.5%.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 16.5,
          "output": 82.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seedream-4-5",
      "name": "doubao-seedream-4-5",
      "display_name": "doubao-seedream-4-5",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Seedream 4.5 is ByteDance's latest multimodal image model, integrating capabilities such as text-to-image, image-to-image, and multi-image output, along with incorporating common sense and reasoning abilities. Compared to the previous 4.0 model, it significantly improves generation quality, offering better editing consistency and multi-image fusion effects, with more precise control over image details. The generation of small text and small faces is more natural.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "sora-2",
      "name": "sora-2",
      "display_name": "sora-2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Sora-2 is the next-generation text-to-video model evolved from Sora, optimized for higher visual realism, stronger physical consistency, and longer temporal coherence. It delivers more stable character consistency, complex motion rendering, camera control, and narrative continuity, while supporting higher resolutions and minute-level video generation for film production, advertising, virtual content creation, and creative multimedia workflows.",
        "typeHints": [
          "video"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "sora-2-pro",
      "name": "sora-2-pro",
      "display_name": "sora-2-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "OpenAI video model Sora2-pro official API.",
        "typeHints": [
          "video"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-audio-preview",
      "name": "gpt-4o-audio-preview",
      "display_name": "gpt-4o-audio-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "audio"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "OpenAI voice input and output model, with prices consistent with the official ones. For now, only the text portion prices are displayed; voice prices can be found on the official OpenAI website. Backend billing is the same as the official.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "audio"
        ],
        "pricing": {
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-mini-audio-preview",
      "name": "gpt-4o-mini-audio-preview",
      "display_name": "gpt-4o-mini-audio-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "audio"
        ]
      },
      "cost": {
        "input": 0.15,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "audio"
        ],
        "pricing": {
          "input": 0.15,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o3",
      "name": "o3",
      "display_name": "o3",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 8,
        "cache_read": 0.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "OpenAI o3 is a powerful model across multiple domains, setting a new standard for coding, math, science, and visual reasoning tasks.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.5,
          "input": 2,
          "output": 8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro",
      "name": "gemini-2.5-pro",
      "display_name": "gemini-2.5-pro",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video",
          "pdf"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini 2.5 Pro is an advanced reasoning model developed by Google, optimized for solving highly complex problems across multiple domains. It can deeply understand large-scale information from diverse sources, including text, audio, images, video, and even entire codebases. The model demonstrates strong reasoning capabilities in coding, mathematics, and STEM-related tasks, and supports long-context analysis for large datasets, codebases, and technical documentation.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context",
          "web",
          "thinking",
          "deepsearch"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video",
          "pdf"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jimeng-3.0-1080p",
      "name": "jimeng-3.0-1080p",
      "display_name": "jimeng-3.0-1080p",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "DreamVideo 3.0 Pro is a professional-grade text-to-video and image-to-video model built on the Dream framework, delivering a major breakthrough in video generation quality. This version demonstrates strong performance across multiple dimensions, including narrative coherence, instruction following, dynamic fluidity, and visual detail. It supports multi-shot storytelling and generates 1080P high-definition videos with a professional cinematic texture. The model also enables diverse and expressive stylistic rendering, making it well suited for creative production and visual storytelling.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jimeng-3.0-720p",
      "name": "jimeng-3.0-720p",
      "display_name": "jimeng-3.0-720p",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "DreamVideo 3.0 Pro is a professional-grade text-to-video and image-to-video model built on the Dream framework, delivering a major breakthrough in video generation quality. This version demonstrates strong performance across multiple dimensions, including narrative coherence, instruction following, dynamic fluidity, and visual detail. It supports multi-shot storytelling and generates 1080P high-definition videos with a professional cinematic texture. The model also enables diverse and expressive stylistic rendering, making it well suited for creative production and visual storytelling.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jimeng-3.0-pro",
      "name": "jimeng-3.0-pro",
      "display_name": "jimeng-3.0-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "DreamVideo 3.0 Pro is a professional-grade text-to-video and image-to-video model built on the Dream framework, delivering a major breakthrough in video generation quality. This version demonstrates strong performance across multiple dimensions, including narrative coherence, instruction following, dynamic fluidity, and visual detail. It supports multi-shot storytelling and generates 1080P high-definition videos with a professional cinematic texture. The model also enables diverse and expressive stylistic rendering, making it well suited for creative production and visual storytelling.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "kimi-for-coding-free",
      "name": "kimi-for-coding-free",
      "display_name": "kimi-for-coding-free",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "kimi-for-coding-free is a free and open version offered by AIHubMix specifically for Kimi users. To maintain stable service operations, the following usage limits apply: a maximum of 10 requests per minute, 1,000 total requests per day, and a daily quota of 5 million tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o3-pro",
      "name": "o3-pro",
      "display_name": "o3-pro",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 20,
        "output": 80,
        "cache_read": 20
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "o3-pro\nThis model only supports Requests API interface requests.The model's thinking time is relatively long, so the response will be slow.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 20,
          "input": 20,
          "output": 80
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "wan2.2-i2v-plus",
      "name": "wan2.2-i2v-plus",
      "display_name": "wan2.2-i2v-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The newly upgraded Tongyi Wanxiang 2.2 text-to-video offers higher video quality. It optimizes video generation stability and success rate, features stronger instruction-following capabilities, consistently maintains image text, portrait, and product consistency, and provides precise camera motion control.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "wan2.2-t2v-plus",
      "name": "wan2.2-t2v-plus",
      "display_name": "wan2.2-t2v-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The newly upgraded Tongyi Wanxiang 2.2 text-to-video offers higher video quality. It can stably generate large-scale complex motions, supports cinematic-level visual performance and control, and features enhanced instruction-following capabilities to achieve realistic physical world reproduction.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "wan2.5-i2v-preview",
      "name": "wan2.5-i2v-preview",
      "display_name": "wan2.5-i2v-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Tongyi Wanxiang 2.5 - Text-to-Video Preview features a newly upgraded technical architecture, supporting sound generation synchronized with visuals, 10-second long video generation, stronger instruction-following capabilities, and further improvements in motion ability and visual quality.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "wan2.5-t2v-preview",
      "name": "wan2.5-t2v-preview",
      "display_name": "wan2.5-t2v-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Tongyi Wanxiang 2.5 - Text-to-Video Preview, newly upgraded model architecture, supports sound generation synchronized with visuals, supports 10-second long video generation, enhanced instruction compliance, improved motion capability, and further enhanced visual quality.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "web-sora-2",
      "name": "web-sora-2",
      "display_name": "web-sora-2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This model is an unofficial reverse-engineered API of the OpenAI web version sora-2-hd, for entertainment purposes only. Charges apply regardless of generation success or failure, billed per use. Please avoid using it if you mind. It can be used via the chat interface, allowing intuitive image uploads: you can directly upload images through the chat interface as the basis for video generation.\n\nPrecise parameter control: by appending commands such as \"landscape/portrait,\" \"16:9/9:16,\" \"10 seconds/15 seconds,\" etc., at the end of the prompt, you can directly define the video's aspect ratio and duration.",
        "typeHints": [
          "video"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "web-sora-2-pro",
      "name": "web-sora-2-pro",
      "display_name": "web-sora-2-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This model is an unofficial reverse-engineered API of the OpenAI web version sora-2-hd, for entertainment purposes only. Charges apply regardless of generation success or failure, billed per use. Please avoid using it if you mind. It can be used via the chat interface, allowing intuitive image uploads: you can directly upload images through the chat interface as the basis for video generation.\n\nPrecise parameter control: by appending commands such as \"landscape/portrait,\" \"16:9/9:16,\" \"10 seconds/15 seconds,\" etc., at the end of the prompt, you can directly define the video's aspect ratio and duration.",
        "typeHints": [
          "video"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-glm-4.6",
      "name": "cc-glm-4.6",
      "display_name": "cc-glm-4.6",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.06,
        "output": 0.22
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "for claude code",
        "pricing": {
          "input": 0.06,
          "output": 0.22
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "coding-glm-4.6",
      "name": "coding-glm-4.6",
      "display_name": "coding-glm-4.6",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.06,
        "output": 0.22,
        "cache_read": 0.010998
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.010998,
          "input": 0.06,
          "output": 0.22
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "coding-glm-4.6-free",
      "name": "coding-glm-4.6-free",
      "display_name": "coding-glm-4.6-free",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "coding-glm-4.6-free is the open and free version of coding-glm-4.6. To ensure stable service performance, usage limits are in place: up to 10 requests per minute, 1,000 requests per day, and a daily token allowance of 5 million.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "coding-minimax-m2",
      "name": "coding-minimax-m2",
      "display_name": "coding-minimax-m2",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 204800,
        "output": 204800
      },
      "metadata": {
        "description": "coding-minimax-m2 is a free and open version offered by AIHubMix specifically for MiniMax users. To maintain stable service operations, the following usage limits apply: a maximum of 10 requests per minute, 1,000 total requests per day, and a daily quota of 5 million tokens.204800",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "coding-minimax-m2-free",
      "name": "coding-minimax-m2-free",
      "display_name": "coding-minimax-m2-free",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 204800,
        "output": 204800
      },
      "metadata": {
        "description": "coding-minimax-m2-free is a free and open version offered by AIHubMix specifically for MiniMax users. To maintain stable service operations, the following usage limits apply: a maximum of 10 requests per minute, 1,000 total requests per day, and a daily quota of 5 million tokens.204800",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-seed-code-free",
      "name": "doubao-seed-code-free",
      "display_name": "doubao-seed-code-free",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "doubao-seed-code-free is a free and open version offered by AIHubMix specifically for Doubao users. To maintain stable service operations, the following usage limits apply: a maximum of 10 requests per minute, 1,000 total requests per day, and a daily quota of 5 million tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "flux-2-flex",
      "name": "flux-2-flex",
      "display_name": "flux-2-flex",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "FLUX.2 is purpose-built for real-world creative production workflows. It delivers high-quality images while maintaining character and style consistency across multiple reference images, shows exceptional understanding and execution of structured prompts, and supports complex text reading and writing. It also adheres to brand guidelines, handles lighting, layout, and logo elements with stability, and enables image editing at resolutions up to 4MP — all while preserving fine details, striking a balance between creativity and professional-grade visual output.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "flux-2-pro",
      "name": "flux-2-pro",
      "display_name": "flux-2-pro",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "FLUX.2 is purpose-built for real-world creative production workflows. It delivers high-quality images while maintaining character and style consistency across multiple reference images, shows exceptional understanding and execution of structured prompts, and supports complex text reading and writing. It also adheres to brand guidelines, handles lighting, layout, and logo elements with stability, and enables image editing at resolutions up to 4MP — all while preserving fine details, striking a balance between creativity and professional-grade visual output.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-search",
      "name": "gemini-2.5-pro-search",
      "display_name": "gemini-2.5-pro-search",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video",
          "pdf"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "gemini-2.5-pro-search integrates Google's official search functionality; the search feature will have an additional separate fee log directly incorporated into the scoring, with detailed logs not displayed; this will be fixed and displayed later; only supports OpenAI-compatible formats for invocation, does not support Gemini SDK; for Gemini's native SDK, please set parameters directly using the official search parameters.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "thinking",
          "web",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video",
          "pdf"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "kimi-k2-thinking",
      "name": "kimi-k2-thinking",
      "display_name": "kimi-k2-thinking",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.548,
        "output": 2.192,
        "cache_read": 0.137
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "Kimi K2 Thinking is Moonshot AI's most advanced open-source inference model to date, extending the K2 series into intelligent agent and long-context inference domains. The model is built on the trillion-parameter mixture of experts (MoE) architecture introduced by Kimi K2, activating 32 billion parameters per forward pass and supporting a context window of 256,000 tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.137,
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Kimi-K2-0905",
      "name": "Kimi-K2-0905",
      "display_name": "Kimi-K2-0905",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.548,
        "output": 2.192
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "Kimi-K2-0905 is a large-scale Mixture of Experts (MoE) language model developed by Moonshot AI, with a total of 1 trillion parameters and 32 billion active parameters per forward pass. It supports long-context inference of up to 256k tokens, an expansion from the previous 128k.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-V3.1-Fast",
      "name": "DeepSeek-V3.1-Fast",
      "display_name": "DeepSeek-V3.1-Fast",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.096,
        "output": 3.288
      },
      "limit": {
        "context": 163000,
        "output": 163000
      },
      "metadata": {
        "description": "The model provider is the Sophon platform. DeepSeek V3.1 Fast is the high-TPS speed version of DeepSeek V3.1.\nHybrid thinking mode: By modifying the chat template, a single model can simultaneously support both thinking and non-thinking modes.\nSmarter tool usage: Through post-training optimization, the model’s performance in tool utilization and agent tasks has improved significantly.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 1.096,
          "output": 3.288
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-opus-4-0",
      "name": "claude-opus-4-0",
      "display_name": "claude-opus-4-0",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 16.5,
        "output": 82.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Alias \nclaude-opus-4-20250514",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 16.5,
          "output": 82.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-sonnet-4-0",
      "name": "claude-sonnet-4-0",
      "display_name": "claude-sonnet-4-0",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5,
        "cache_read": 0.33
      },
      "limit": {
        "context": 1000000,
        "output": 1000000
      },
      "metadata": {
        "description": "Claude Sonnet 4 is a significant upgrade to Sonnet 3.7, delivering superior performance in coding and reasoning with enhanced precision and control. Achieving a state-of-the-art 72.7% on SWE-bench, the model expertly balances advanced capability with computational efficiency. Key improvements include more reliable codebase navigation and complex instruction following, making it ideal for a wide range of applications, from routine coding to complex software development projects.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.33,
          "cache_write": 4.125,
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash",
      "name": "gemini-2.5-flash",
      "display_name": "gemini-2.5-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini 2.5 Flash is Google’s best model in terms of both performance and cost efficiency, offering a comprehensive set of capabilities. It is the first Flash model to support visible reasoning, allowing insight into the thought process behind its responses. With its strong price–performance ratio, the model is well suited for large-scale processing, low-latency, high-throughput tasks that require reasoning, as well as agent-based application scenarios.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-preview-09-2025",
      "name": "gemini-2.5-flash-preview-09-2025",
      "display_name": "gemini-2.5-flash-preview-09-2025",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:\n\nBetter agentic tool use: We've improved how the model uses tools, leading to better performance in more complex, agentic and multi-step applications. This model shows noticeable improvements on key agentic benchmarks, including a 5% gain on SWE-Bench Verified, compared to our last release (48.9% → 54%). More efficient: With thinking on, the model is now significantly more cost-efficient—achieving higher quality outputs while using fewer tokens, reducing latency and cost (see charts above).",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "glm-4.5v",
      "name": "glm-4.5v",
      "display_name": "glm-4.5v",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 0.822,
        "cache_read": 0.274
      },
      "limit": {
        "context": 64000,
        "output": 64000
      },
      "metadata": {
        "description": "GLM-4.5V is a vision-language foundational model designed for multimodal agent applications. Based on a mixture-of-experts (MoE) architecture, it has 106 billion parameters and 12 billion active parameters. It delivers outstanding performance in video understanding, image question answering, OCR, and document parsing, and achieves significant improvements in front-end web encoding, basic reasoning, and spatial reasoning.",
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.274,
          "input": 0.274,
          "output": 0.822
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-lite",
      "name": "gemini-2.5-flash-lite",
      "display_name": "gemini-2.5-flash-lite",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.025
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini 2.5 Flash-Lite is a balanced model from Google, optimized for applications that require low-latency performance. It retains the practical capabilities of the Gemini 2.5 family, including configurable reasoning based on budget, integration with tools such as grounding via Google Search and code execution, multimodal input support, and an ultra-long context window of up to 1 million tokens, delivering a strong balance between efficiency, functionality, and cost.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.025,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-lite-preview-09-2025",
      "name": "gemini-2.5-flash-lite-preview-09-2025",
      "display_name": "gemini-2.5-flash-lite-preview-09-2025",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.025
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "gemini-2.5-flash-lite latest preview version",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.025,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-nothink",
      "name": "gemini-2.5-flash-nothink",
      "display_name": "gemini-2.5-flash-nothink",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1047576,
        "output": 1047576
      },
      "metadata": {
        "description": "Gemini-2.5-flash defaults to thinking enabled; to disable thinking, request the name gemini-2.5-flash-nothink, which only supports OpenAI-compatible format calls and does not support Gemini SDK; for the native Gemini SDK, please set the parameter budget=0 directly.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-search",
      "name": "gemini-2.5-flash-search",
      "display_name": "gemini-2.5-flash-search",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "gemini-2.5-flash-search integrates Google's official search functionality; the search feature will have an additional separate fee log directly incorporated into the scoring, with detailed logs not displayed; this will be fixed and displayed later; only supports OpenAI-compatible formats for invocation, does not support Gemini SDK; for Gemini's native SDK, please set parameters directly using the official search parameters.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "web",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-preview-05-20-nothink",
      "name": "gemini-2.5-flash-preview-05-20-nothink",
      "display_name": "gemini-2.5-flash-preview-05-20-nothink",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini-2.5-flash-preview-05-20 is enabled by default for thinking; to disable it, request the name gemini-2.5-flash-preview-05-20-nothink.Only OpenAI-compatible format calls are supported; Gemini SDK is not supported. For the native Gemini SDK, please set the parameter budget=0 directly.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-preview-05-20-search",
      "name": "gemini-2.5-flash-preview-05-20-search",
      "display_name": "gemini-2.5-flash-preview-05-20-search",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 2.499,
        "cache_read": 0.075
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini-2.5 Flash Preview 05-20 Search integrates Google's official search functionality; the search feature will have an additional separate fee log directly integrated into the scoring deduction, with detailed logs not displayed. It will be fixed and displayed later. Only OpenAI-compatible formats are supported for invocation; Gemini SDK is not supported. For Gemini's native SDK, please set parameters directly using the official search parameters.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.3,
          "output": 2.499
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "DeepSeek-V3-Fast",
      "name": "DeepSeek-V3-Fast",
      "display_name": "DeepSeek-V3-Fast",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.56,
        "output": 2.24
      },
      "limit": {
        "context": 32000,
        "output": 32000
      },
      "metadata": {
        "description": "V3 Ultra-Fast Version,The current price is a limited-time 50% discount and will return to the original price on July 31st. The original price is: input: $0.55/M, output: $2.2/M. The model provider is the Sophnet platform. DeepSeek V3 Fast is a high-TPS, ultra-fast version of DeepSeek V3 0324, featuring full-precision (non-quantized) performance, enhanced code and math capabilities, and faster responses!\n\nDeepSeek V3 0324 is a powerful Mixture-of-Experts (MoE) model with a total parameter count of 671B, activating 37B parameters per token.\nIt adopts Multi-Head Latent Attention (MLA) and the DeepSeekMoE architecture to achieve efficient inference and economical training costs.\nIt innovatively implements a load balancing strategy without auxiliary loss and sets multi-token prediction training targets to enhance performance.\nThe model is pre-trained on 14.8 trillion diverse, high-quality tokens and further optimized through supervised fine-tuning and reinforcement learning stages to fully realize its capabilities.\nComprehensive evaluations show that DeepSeek V3 outperforms other open-source models and rivals leading closed-source models in performance.\nThe entire training process only requires 2.788M H800 GPU hours and remains highly stable, with no irrecoverable loss spikes or rollbacks.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.56,
          "output": 2.24
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "veo-2.0-generate-001",
      "name": "veo-2.0-generate-001",
      "display_name": "veo-2.0-generate-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Veo 2.0 is an advanced video generation model capable of producing high-quality videos based on text or image prompts. It excels in understanding real-world physics and human motion, resulting in fluid character movements and lifelike scenes. Veo 2.0 supports various visual styles and camera control options, including lens types, angles, and motion effects. Users can generate 8-second video clips at 720p resolution.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "veo3.1",
      "name": "veo3.1",
      "display_name": "veo3.1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 200,
        "output": 200
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "veo3.1 reverse model, and other available model names that can be requested include: veo3.1-pro and veo3.1-components. The price is currently tentatively set to be calculated per token, approximately $0.05 per request.",
        "typeHints": [
          "video"
        ],
        "pricing": {
          "input": 200,
          "output": 200
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "imagen-4.0",
      "name": "imagen-4.0",
      "display_name": "imagen-4.0",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Imagen 4 is a high-quality text-to-image model developed by Google, designed for strong visual fidelity, diverse artistic styles, and precise controllability. It delivers near photographic realism with sharp details and natural lighting while significantly reducing common artifacts such as distorted hands. The model supports a wide range of styles including photorealistic, illustration, anime, oil painting, and pixel art, and offers flexible aspect ratios for use cases from content covers to mobile wallpapers. It also enables image editing and secondary creation on existing images, provides fast and stable generation, and offers strong commercial usability with high visual quality and reliable content safety.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-fast-generate-001",
      "name": "imagen-4.0-fast-generate-001",
      "display_name": "imagen-4.0-fast-generate-001",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Imagen 4 is a new-generation image generation model designed to balance high-quality output, inference efficiency, and content safety. It supports image generation, digital watermarking with authenticity verification, user-configurable safety settings, and prompt enhancement via the Prompt Rewriter, while also delivering reliable person generation capabilities. The model ID is imagen-4.0-generate-001, making it suitable for professional creation, design workflows, and various generative AI applications.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-fast-generate-preview-06-06",
      "name": "imagen-4.0-fast-generate-preview-06-06",
      "display_name": "imagen-4.0-fast-generate-preview-06-06",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-generate-001",
      "name": "imagen-4.0-generate-001",
      "display_name": "imagen-4.0-generate-001",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Imagen 4 is a new-generation image generation model designed to balance high-quality output, inference efficiency, and content safety. It supports image generation, digital watermarking with authenticity verification, user-configurable safety settings, and prompt enhancement via the Prompt Rewriter, while also delivering reliable person generation capabilities. The model ID is imagen-4.0-generate-001, making it suitable for professional creation, design workflows, and various generative AI applications.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-ultra-generate-001",
      "name": "imagen-4.0-ultra-generate-001",
      "display_name": "imagen-4.0-ultra-generate-001",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-ultra",
      "name": "imagen-4.0-ultra",
      "display_name": "imagen-4.0-ultra",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-image-1",
      "name": "gpt-image-1",
      "display_name": "gpt-image-1",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 5,
        "output": 40,
        "cache_read": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Azure OpenAI’s gpt-image-1 image generation API offers both text-to-image generation and image-to-image editing with text guidance capabilities.\nBefore using this API, please ensure you have the latest OpenAI package installed by running pip install -U openai.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 5,
          "input": 5,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-image-1-mini",
      "name": "gpt-image-1-mini",
      "display_name": "gpt-image-1-mini",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 5,
        "output": 40,
        "cache_read": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "OpenAI image generation model gpt-image-1-mini\nBefore use, please run pip install -U openai to upgrade to the latest openai package.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 5,
          "input": 5,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "o4-mini",
      "name": "o4-mini",
      "display_name": "o4-mini",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 4.4,
        "cache_read": 0.275
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "o4-mini is a remarkably smart model for its speed and cost-efficiency. This allows it to support significantly higher usage limits than o3, making it a strong high-volume, high-throughput option for everyone with questions that benefit from reasoning.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tool",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.275,
          "input": 1.1,
          "output": 4.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-flash-image-preview",
      "name": "gemini-2.5-flash-image-preview",
      "display_name": "gemini-2.5-flash-image-preview",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "image",
          "text"
        ]
      },
      "cost": {
        "input": 0.3,
        "output": 1.2,
        "cache_read": 0.3
      },
      "limit": {
        "context": 32800,
        "output": 32800
      },
      "metadata": {
        "description": "Aihubmix supports the gemini-2.5-flash-image-preview model; you can add extra parameters modalities=[\"text\", \"image\"] through the OpenAI-compatible chat interface; https://docs.aihubmix.com/en/api/Gemini-Guides#gemini-2-5-flash%3A-quick-task-support",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "image",
          "text"
        ],
        "pricing": {
          "cache_read": 0.3,
          "input": 0.3,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "glm-4.5",
      "name": "glm-4.5",
      "display_name": "glm-4.5",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 1.6
      },
      "limit": {
        "context": 131072,
        "output": 131072
      },
      "metadata": {
        "description": "GLM-4.5",
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4.1",
      "name": "gpt-4.1",
      "display_name": "gpt-4.1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 8,
        "cache_read": 0.5
      },
      "limit": {
        "context": 1047576,
        "output": 1047576
      },
      "metadata": {
        "description": "The latest flagship multimodal model supports million-token context, with encoding capability (SWE-bench 54.6%) and instruction-following (Scale AI 38.3%) performance significantly surpassing GPT-4o, while reducing costs by 26%, making it suitable for complex tasks. Its automatic caching mechanism offers a 75% cost reduction on cache hits.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.5,
          "input": 2,
          "output": 8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "grok-4",
      "name": "grok-4",
      "display_name": "grok-4",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3.3,
        "output": 16.5,
        "cache_read": 0.825
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Grok, their latest and greatest flagship model, offers unparalleled performance in natural language, math, and reasoning – the perfect jack of all trades.\nThe current pointing model version is grok-4-0709.",
        "pricing": {
          "cache_read": 0.825,
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-4-fast-non-reasoning",
      "name": "grok-4-fast-non-reasoning",
      "display_name": "grok-4-fast-non-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.5,
        "cache_read": 0.05
      },
      "limit": {
        "context": 2000000,
        "output": 2000000
      },
      "metadata": {
        "description": "Grok-4-fast is a cost-effective inference model developed by xAI that delivers cutting-edge performance with excellent token efficiency. The model features a 2 million token context window, advanced Web and X search capabilities, and a unified architecture supporting both \"inference\" and \"non-inference\" modes. Compared to Grok 4, it reduces thinking tokens by an average of 40% and lowers the price by 98% while achieving the same performance.",
        "pricing": {
          "cache_read": 0.05,
          "input": 0.2,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-4-fast-reasoning",
      "name": "grok-4-fast-reasoning",
      "display_name": "grok-4-fast-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.5,
        "cache_read": 0.05
      },
      "limit": {
        "context": 2000000,
        "output": 2000000
      },
      "metadata": {
        "description": "Grok-4-fast is a cost-effective inference model developed by xAI that delivers cutting-edge performance with excellent token efficiency. The model features a 2 million token context window, advanced Web and X search capabilities, and a unified architecture supporting both \"inference\" and \"non-inference\" modes. Compared to Grok 4, it reduces thinking tokens by an average of 40% and lowers the price by 98% while achieving the same performance.",
        "pricing": {
          "cache_read": 0.05,
          "input": 0.2,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "kimi-k2-0711",
      "name": "kimi-k2-0711",
      "display_name": "kimi-k2-0711",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.54,
        "output": 2.16
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "Kimi-K2 is a MoE architecture foundational model with extremely powerful coding and agent capabilities, featuring a total of 1 trillion parameters and activating 32 billion parameters. In benchmark performance tests across major categories such as general knowledge reasoning, programming, mathematics, and agents, the K2 model outperforms other mainstream open-source models.\nThe Kimi-K2 model supports a context length of 128k tokens.\nIt does not support visual capabilities.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.54,
          "output": 2.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "kimi-k2-turbo-preview",
      "name": "kimi-k2-turbo-preview",
      "display_name": "kimi-k2-turbo-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.2,
        "output": 4.8,
        "cache_read": 0.3
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "The kimi-k2-turbo-preview model is a high-speed version of kimi-k2, with the same model parameters as kimi-k2, but the output speed has been increased from 10 tokens per second to 40 tokens per second.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.3,
          "input": 1.2,
          "output": 4.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-vl-235b-a22b-instruct",
      "name": "qwen3-vl-235b-a22b-instruct",
      "display_name": "qwen3-vl-235b-a22b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 1.096
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "The Qwen3 series open-source models include hybrid models, thinking models, and non-thinking models, with both reasoning capabilities and general abilities reaching industry SOTA levels at the same scale.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.274,
          "output": 1.096
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-vl-235b-a22b-thinking",
      "name": "qwen3-vl-235b-a22b-thinking",
      "display_name": "qwen3-vl-235b-a22b-thinking",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 2.74
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "The Qwen3 series open-source models include hybrid models, thinking models, and non-thinking models, with both reasoning capabilities and general abilities reaching industry SOTA levels at the same scale.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.274,
          "output": 2.74
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-vl-30b-a3b-instruct",
      "name": "qwen3-vl-30b-a3b-instruct",
      "display_name": "qwen3-vl-30b-a3b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.1028,
        "output": 0.4112
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "The Qwen3-VL series’ second-largest MoE model Instruct version offers fast response speed and supports ultra-long contexts such as long videos and long documents; it features comprehensive upgrades in image/video understanding, spatial perception, and universal recognition abilities; it also provides visual 2DD/3D localization capabilities, making it capable of handling complex real-world tasks.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.1028,
          "output": 0.4112
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-vl-30b-a3b-thinking",
      "name": "qwen3-vl-30b-a3b-thinking",
      "display_name": "qwen3-vl-30b-a3b-thinking",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.1028,
        "output": 1.028
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "The Qwen3-VL series’ second-largest MoE model Thinking version offers fast response speed, stronger multimodal understanding and reasoning, visual agent capabilities, and ultra-long context support for long videos and long documents; it features comprehensive upgrades in image/video understanding, spatial perception, and universal recognition abilities, making it capable of handling complex real-world tasks.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.1028,
          "output": 1.028
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "veo-3.0-generate-preview",
      "name": "veo-3.0-generate-preview",
      "display_name": "veo-3.0-generate-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Veo 3.0 Generate Preview is an advanced AI video generation model that supports text-to-video creation with synchronized audio, featuring excellent physical simulation and lip-sync capabilities. Users can generate vivid video clips from short story prompts. 🎟️ Limited-Time Deal: Save 10% Now.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "veo-3.1-fast-generate-preview",
      "name": "veo-3.1-fast-generate-preview",
      "display_name": "veo-3.1-fast-generate-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "veo-3.1-generate-preview",
      "name": "veo-3.1-generate-preview",
      "display_name": "veo-3.1-generate-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Veo 3.1 is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "DeepSeek-OCR",
      "name": "DeepSeek-OCR",
      "display_name": "DeepSeek-OCR",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8000,
        "output": 8000
      },
      "metadata": {
        "description": "DeepSeek-OCR is a vision-language model launched by DeepSeek AI, focusing on optical character recognition (OCR) and “contextual optical compression.” The model is designed to explore the limits of compressing contextual information from images, efficiently processing documents and converting them into structured text formats such as Markdown. The model requires an image as input.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "flux-kontext-max",
      "name": "flux-kontext-max",
      "display_name": "flux-kontext-max",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-router",
      "name": "aihubmix-router",
      "display_name": "aihubmix-router",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 1.6,
        "cache_read": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "New model routing capability; request aihubmix-router to automatically route models based on question complexity, so everyone no longer needs to manually switch models; in our tests comparing the use of the model router versus only using GPT-4.1, we observed up to 60% cost savings while maintaining similar accuracy.  \nThe context length of the model router depends on the base model used for each prompt. Input size is 200,000, output size is 32,768.  \nCurrently, there are four routing models: gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o4-mini.  \nPricing: Due to our current billing structure system, requests through aihubmix-router are billed at the price of gpt-4.1-mini regardless of which final model is used; future billing will be based on the actual model invoked.  \nEveryone is welcome to try it out; the interface will return the name of the actual called model.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.1,
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4.1-mini",
      "name": "gpt-4.1-mini",
      "display_name": "gpt-4.1-mini",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 1.6,
        "cache_read": 0.1
      },
      "limit": {
        "context": 1047576,
        "output": 1047576
      },
      "metadata": {
        "description": "Lightweight, high-performance model with million-token context and near-flagship-level encoding and image understanding capabilities, while reducing costs by 83%. It is suitable for rapid development and small to medium-sized applications. The automatic caching mechanism provides a 75% cost reduction on cache hits.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.1,
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-vl-plus",
      "name": "qwen3-vl-plus",
      "display_name": "qwen3-vl-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.137,
        "output": 1.37,
        "cache_read": 0.0274
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "The Qwen3 series visual understanding model achieves an effective fusion of thinking and non-thinking modes. Its visual agent capabilities reach world-class levels on public test sets such as OS World. This version features comprehensive upgrades in visual coding, spatial perception, and multimodal reasoning; visual perception and recognition abilities are greatly enhanced, supporting ultra-long video understanding.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.0274,
          "input": 0.137,
          "output": 1.37
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4.1-nano",
      "name": "gpt-4.1-nano",
      "display_name": "gpt-4.1-nano",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.025
      },
      "limit": {
        "context": 1047576,
        "output": 1047576
      },
      "metadata": {
        "description": "Ultra-lightweight model with million-token context, optimized for speed and low latency, costing only $0.10 per million input tokens. It is suitable for edge computing and real-time interaction. The automatic caching mechanism offers a 75% cost reduction on cache hits.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.025,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-preview-05-06",
      "name": "gemini-2.5-pro-preview-05-06",
      "display_name": "gemini-2.5-pro-preview-05-06",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "gemini-2.5-pro latest model",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-preview-03-25",
      "name": "gemini-2.5-pro-preview-03-25",
      "display_name": "gemini-2.5-pro-preview-03-25",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Supports high concurrency.  \nThe Gemini 2.5 Pro preview version is here, with higher limits for production testing.  \nGoogle's latest and most powerful model;",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-preview-05-06-search",
      "name": "gemini-2.5-pro-preview-05-06-search",
      "display_name": "gemini-2.5-pro-preview-05-06-search",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Integrated with Google's official search function.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "thinking",
          "web"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-preview-03-25-search",
      "name": "gemini-2.5-pro-preview-03-25-search",
      "display_name": "gemini-2.5-pro-preview-03-25-search",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Integrated with Google's official search function.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "thinking",
          "web",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-max-preview",
      "name": "qwen3-max-preview",
      "display_name": "qwen3-max-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.822,
        "output": 3.288,
        "cache_read": 0.822
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen3-Max-Preview is the latest preview model in the Qwen3 series. This version is functionally equivalent to Qwen3-Max-Thinking — simply set extra_body={\"enable_thinking\": True} to enable the thinking mode. Compared to the Qwen2.5 series, it delivers significant improvements in overall general capabilities, including English–Chinese text understanding, complex instruction following, open-ended reasoning, multilingual processing, and tool-use proficiency. The model also exhibits fewer hallucinations and stronger overall reliability.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.822,
          "input": 0.822,
          "output": 3.288
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-max",
      "name": "qwen3-max",
      "display_name": "qwen3-max",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.822,
        "output": 3.288,
        "cache_read": 0.822
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "The Tongyi Qianwen 3 series Max model has undergone special upgrades in intelligent agent programming and tool invocation compared to the preview version. The officially released model this time reaches SOTA level in the field and is adapted to more complex intelligent agent scenarios.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.822,
          "input": 0.822,
          "output": 3.288
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-next-80b-a3b-instruct",
      "name": "qwen3-next-80b-a3b-instruct",
      "display_name": "qwen3-next-80b-a3b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.138,
        "output": 0.552
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned model in the Qwen3-Next series, optimized for delivering fast, stable, and direct final answers without showing its reasoning steps (\"thinking traces\").\n\nUnlike chain-of-thought models, it focuses on generating consistent, instruction-following outputs, making it ideal for production environments. It excels at complex tasks like reasoning and coding while maintaining high throughput and stability, especially with ultra-long inputs and multi-turn dialogues.\n\nEngineered for efficiency, its performance rivals larger Qwen3 systems, making it perfectly suited for RAG, tool use, and agentic workflows where deterministic results are critical.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.138,
          "output": 0.552
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-next-80b-a3b-thinking",
      "name": "qwen3-next-80b-a3b-thinking",
      "display_name": "qwen3-next-80b-a3b-thinking",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.138,
        "output": 1.38
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that excels by outputting structured 'thinking' traces (Chain-of-Thought) by default.\n\nDesigned for hard, multi-step problems, it is ideal for tasks like math proofs, code synthesis, logic puzzles, and agentic planning. Compared to other Qwen3 variants, it offers greater stability during long reasoning chains and is tuned to follow complex instructions without getting repetitive or off-task.\n\nThis model is perfectly suited for agent frameworks, tool use (function calling), and benchmarks where a step-by-step breakdown is required. It leverages throughput-oriented techniques for fast generation of detailed, procedural outputs.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.138,
          "output": 1.38
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-235b-a22b-instruct-2507",
      "name": "qwen3-235b-a22b-instruct-2507",
      "display_name": "qwen3-235b-a22b-instruct-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.28,
        "output": 1.12
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "Qwen3-235B-A22B-Instruct-2507",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.28,
          "output": 1.12
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-235b-a22b-thinking-2507",
      "name": "qwen3-235b-a22b-thinking-2507",
      "display_name": "qwen3-235b-a22b-thinking-2507",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.28,
        "output": 2.8
      },
      "limit": {
        "context": 262144,
        "output": 262144
      },
      "metadata": {
        "description": "The open-source thinking model based on Qwen3 has significantly improved in logical ability, general capability, knowledge enhancement, and creative ability compared to the previous version (Tongyi Qianwen 3-235B-A22B). It is suitable for high-difficulty and strong reasoning scenarios.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.28,
          "output": 2.8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-coder-30b-a3b-instruct",
      "name": "qwen3-coder-30b-a3b-instruct",
      "display_name": "qwen3-coder-30b-a3b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.8,
        "cache_read": 0.2
      },
      "limit": {
        "context": 2000000,
        "output": 2000000
      },
      "metadata": {
        "description": "The code generation model based on Qwen3 has powerful Coding Agent capabilities, achieving state-of-the-art performance compared to open-source models.The model adopts tiered pricing.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.2,
          "input": 0.2,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-coder-480b-a35b-instruct",
      "name": "qwen3-coder-480b-a35b-instruct",
      "display_name": "qwen3-coder-480b-a35b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.82,
        "output": 3.28,
        "cache_read": 0.82
      },
      "limit": {
        "context": 262000,
        "output": 262000
      },
      "metadata": {
        "description": "The code generation model based on Qwen3 has powerful Coding Agent capabilities, achieving state-of-the-art performance compared to open-source models.The model adopts tiered pricing.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.82,
          "input": 0.82,
          "output": 3.28
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-235b-a22b",
      "name": "qwen3-235b-a22b",
      "display_name": "qwen3-235b-a22b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.28,
        "output": 1.12
      },
      "limit": {
        "context": 131100,
        "output": 131100
      },
      "metadata": {
        "description": "Qwen3-235B-A22B is a massive 235B parameter Mixture-of-Experts (MoE) model that operates with the efficiency of a 22B model. Its standout feature is the ability to seamlessly switch between a \"thinking\" mode for complex reasoning and a \"non-thinking\" mode for fast conversation, offering both world-class power and practical speed.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.28,
          "output": 1.12
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-coder-flash",
      "name": "qwen3-coder-flash",
      "display_name": "qwen3-coder-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.136,
        "output": 0.544,
        "cache_read": 0.136
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.136,
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-coder-plus",
      "name": "qwen3-coder-plus",
      "display_name": "qwen3-coder-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.54,
        "output": 2.16,
        "cache_read": 0.108
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "The code generation model based on Qwen3 has powerful Coding Agent capabilities, excels in tool invocation and environment interaction, and can achieve autonomous programming with outstanding coding abilities while also possessing general capabilities.The model adopts tiered pricing.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.108,
          "input": 0.54,
          "output": 2.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-coder-plus-2025-07-22",
      "name": "qwen3-coder-plus-2025-07-22",
      "display_name": "qwen3-coder-plus-2025-07-22",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.54,
        "output": 2.16,
        "cache_read": 0.54
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "The code generation model based on Qwen3 has powerful Coding Agent capabilities, excels in tool invocation and environment interaction, and can achieve autonomous programming with outstanding coding abilities while also possessing general capabilities.The model adopts tiered pricing.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.54,
          "input": 0.54,
          "output": 2.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-V3",
      "name": "DeepSeek-V3",
      "display_name": "DeepSeek-V3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.272,
        "output": 1.088
      },
      "limit": {
        "context": 1638000,
        "output": 1638000
      },
      "metadata": {
        "description": "It has been automatically upgraded to the latest released version, 250324.\nAutomatically upgraded to the latest released version 250324.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.272,
          "output": 1.088
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "LongCat-Flash-Chat",
      "name": "LongCat-Flash-Chat",
      "display_name": "LongCat-Flash-Chat",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.14,
        "output": 0.7
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Meituan has officially released and open-sourced LongCat-Flash-Chat, which utilizes an innovative Mixture of Experts (MoE) and \"zero-computation expert\" mechanism to achieve a total of 560B parameters, while only activating around 27B parameters per token as needed. At the same time, end-to-end optimization for agents (including a self-built evaluation set and multi-agent trajectory data) significantly enhances its performance in tool usage and complex task orchestration.",
        "pricing": {
          "input": 0.14,
          "output": 0.7
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.5-pro-preview-06-05-search",
      "name": "gemini-2.5-pro-preview-06-05-search",
      "display_name": "gemini-2.5-pro-preview-06-05-search",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Integrated with Google's official search function.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "thinking",
          "web",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-ultra-generate-exp-05-20",
      "name": "imagen-4.0-ultra-generate-exp-05-20",
      "display_name": "imagen-4.0-ultra-generate-exp-05-20",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Image 4.0 Beta version, for testing purposes only. For production environment, it is recommended to use imagen-4.0-generate-preview-05-20.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "Qwen2.5-VL-72B-Instruct",
      "name": "Qwen2.5-VL-72B-Instruct",
      "display_name": "Qwen2.5-VL-72B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.62,
        "output": 0.62
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophon platform. Qwen2.5-VL-72B-Instruct is the latest vision-language model released by the Qwen team. This model excels not only at recognizing common objects such as flowers, birds, fish, and insects, but also at efficiently analyzing text, charts, icons, graphics, and layouts within images. As a visual agent, it is capable of reasoning and dynamically guiding tool usage, supporting both computer and mobile operations. Moreover, it can understand videos longer than one hour and accurately locate relevant video segments.",
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.62,
          "output": 0.62
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-5.0-thinking-preview",
      "name": "ernie-5.0-thinking-preview",
      "display_name": "ernie-5.0-thinking-preview",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.822,
        "output": 3.288
      },
      "limit": {
        "context": 183000,
        "output": 183000
      },
      "metadata": {
        "description": "The new generation Wenxin model, Wenxin 5.0, is a native full-modal large model that adopts native full-modal unified modeling technology, jointly modeling text, images, audio, and video, possessing comprehensive full-modal capabilities. Wenxin 5.0's basic abilities are comprehensively upgraded, performing excellently on benchmark test sets, especially in multimodal understanding, instruction compliance, creative writing, factual accuracy, intelligent agent planning, and tool application.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "structured_outputs",
          "function_calling"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.822,
          "output": 3.288
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "inclusionAI/Ling-1T",
      "name": "inclusionAI/Ling-1T",
      "display_name": "inclusionAI/Ling-1T",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.548,
        "output": 2.192
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ling-1T is the first flagship non-thinking model in the “Ling 2.0” series, featuring 1 trillion total parameters and approximately 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient inference and scalable cognition. Ling-1T-base was pretrained on over 20 trillion high-quality, reasoning-intensive tokens, supports up to a 128K context length, and incorporates an Evolutionary Chain of Thought (Evo-CoT) process during mid-stage and post-stage training. This training regimen greatly enhances the model’s efficiency and depth of reasoning, enabling Ling-1T to achieve top performance across multiple complex reasoning benchmarks, balancing accuracy and efficiency.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "inclusionAI/Ring-1T",
      "name": "inclusionAI/Ring-1T",
      "display_name": "inclusionAI/Ring-1T",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.548,
        "output": 2.192
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ring-1T is an open-source idea model with a trillion parameters released by the Bailing team. It is based on the Ling 2.0 architecture and the Ling-1T-base foundational model for training, with a total parameter count of 1 trillion, an active parameter count of 50 billion, and supports up to a 128K context window. The model is trained via large-scale verifiable reward reinforcement learning (RLVR), combined with the self-developed Icepop reinforcement learning stabilization method and the efficient ASystem reinforcement learning system, significantly improving the model’s deep reasoning and natural language reasoning capabilities. Ring-1T achieves leading performance among open-source models on high-difficulty reasoning benchmarks such as mathematics competitions (e.g., IMO 2025), code generation (e.g., ICPC World Finals 2025), and logical reasoning.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4.5-x",
      "name": "glm-4.5-x",
      "display_name": "glm-4.5-x",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2.2,
        "output": 8.91,
        "cache_read": 0.44
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4.5-X is the high-speed version of GLM-4.5, offering powerful performance with a generation speed of up to 100 tokens per second.",
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.44,
          "input": 2.2,
          "output": 8.91
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gme-qwen2-vl-2b-instruct",
      "name": "gme-qwen2-vl-2b-instruct",
      "display_name": "gme-qwen2-vl-2b-instruct",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.138,
        "output": 0.138
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The GME-Qwen2VL series is a unified multimodal Embedding model trained based on the Qwen2-VL multimodal large language model (MLLMs). The GME model supports three types of inputs: text, images, and image-text pairs. All these input types can generate universal vector representations and exhibit excellent retrieval performance.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.138,
          "output": 0.138
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gte-rerank-v2",
      "name": "gte-rerank-v2",
      "display_name": "gte-rerank-v2",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.11
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "gte-rerank-v2 is a multilingual unified text ranking model developed by Tongyi Lab, covering multiple major languages worldwide and providing high-quality text ranking services. It is typically used in scenarios such as semantic retrieval and RAG, and can simply and effectively improve text retrieval performance. Given a query and a set of candidate texts (documents), the model ranks the candidates from highest to lowest based on their semantic relevance to the query.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.11
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "inclusionAI/Ling-flash-2.0",
      "name": "inclusionAI/Ling-flash-2.0",
      "display_name": "inclusionAI/Ling-flash-2.0",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.136,
        "output": 0.544
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ling-flash-2.0 is a language model from inclusionAI with a total of 100 billion parameters, of which 6.1 billion are activated per token (4.8 billion non-embedding). As part of the Ling 2.0 architecture series, it is designed as a lightweight yet powerful Mixture-of-Experts (MoE) model. It aims to deliver performance comparable to or even exceeding that of 40B-level dense models and other larger MoE models, but with a significantly smaller active parameter count. The model represents a strategy focused on achieving high performance and efficiency through extreme architectural design and training methods.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "inclusionAI/Ling-mini-2.0",
      "name": "inclusionAI/Ling-mini-2.0",
      "display_name": "inclusionAI/Ling-mini-2.0",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.272
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ling-mini-2.0 is a small-sized, high-performance large language model based on the MoE architecture. It has a total of 16 billion parameters, but only activates 1.4 billion parameters per token (non-embedding 789 million), achieving extremely high generation speed. Thanks to the efficient MoE design and large-scale high-quality training data, despite activating only 1.4 billion parameters, Ling-mini-2.0 still demonstrates top-tier performance on downstream tasks comparable to dense LLMs under 10 billion parameters and even larger-scale MoE models.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.272
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "inclusionAI/Ring-flash-2.0",
      "name": "inclusionAI/Ring-flash-2.0",
      "display_name": "inclusionAI/Ring-flash-2.0",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.136,
        "output": 0.544
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ring-flash-2.0 is a high-performance thinking model deeply optimized based on the Ling-flash-2.0-base. It uses a mixture-of-experts (MoE) architecture with a total of 100 billion parameters, but only activates 6.1 billion parameters per inference. The model employs the original Icepop algorithm to solve the instability issues of large MoE models during reinforcement learning (RL) training, enabling its complex reasoning capabilities to continuously improve over long training cycles. Ring-flash-2.0 has achieved significant breakthroughs on multiple high-difficulty benchmarks, including mathematics competitions, code generation, and logical reasoning. Its performance not only surpasses top dense models under 40 billion parameters but also rivals larger open-source MoE models and closed-source high-performance thinking models. Although the model focuses on complex reasoning, it also performs exceptionally well on creative writing tasks. Furthermore, thanks to its efficient architecture, Ring-flash-2.0 delivers high performance with low-latency inference, significantly reducing deployment costs in high-concurrency scenarios.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "irag-1.0",
      "name": "irag-1.0",
      "display_name": "irag-1.0",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Baidu's self-developed ERNIE iRAG (ERNIE image-based RAG), a retrieval-augmented text-to-image technology, combines Baidu Search's hundreds of millions of image resources with powerful foundational model capabilities to generate various ultra-realistic images. The overall effect far surpasses native text-to-image systems, eliminating the typical AI feel while maintaining low costs. ERNIE iRAG features no hallucinations, ultra-realism, and instant usability.",
        "typeHints": [
          "image_generation"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "jina-deepsearch-v1",
      "name": "jina-deepsearch-v1",
      "display_name": "jina-deepsearch-v1",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 1000000,
        "output": 1000000
      },
      "metadata": {
        "description": "DeepSearch combines search, reading, and reasoning capabilities to pursue the best possible answer. It's fully compatible with OpenAI's Chat API format—just replace api.openai.com with aihubmix.com to get started.  \nThe stream will return the thinking process.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "thinking",
          "web",
          "deepsearch"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-embeddings-v4",
      "name": "jina-embeddings-v4",
      "display_name": "jina-embeddings-v4",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "A general-purpose vector model with 3.8 billion parameters, used for multimodal and multilingual retrieval, supporting both unidirectional and multi-vector embedding outputs.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-reranker-v3",
      "name": "jina-reranker-v3",
      "display_name": "jina-reranker-v3",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "Multimodal multilingual document reranker, 131K context, 0.6B parameters, for visual document sorting.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "llama-4-maverick",
      "name": "llama-4-maverick",
      "display_name": "llama-4-maverick",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Llama 4 Maverick is a high-capacity Mixture-of-Experts (MoE) model from Meta, featuring 400B total parameters and 128 experts, while activating an efficient 17B parameters per inference. Engineered for peak performance, it excels at advanced multimodal tasks.\n\nMaverick natively supports text and image input, producing multilingual text and code. With a 1-million-token context window and instruction tuning, it is optimized for complex image reasoning and general-purpose assistant-like interactions.\n\nReleased under the Llama 4 Community License, Maverick is ideal for research and commercial applications demanding state-of-the-art multimodal understanding and high throughput.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "llama-4-scout",
      "name": "llama-4-scout",
      "display_name": "llama-4-scout",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 131000,
        "output": 131000
      },
      "metadata": {
        "description": "Llama 4 Scout is a highly efficient Mixture-of-Experts (MoE) model from Meta, activating 17B out of 109B total parameters per inference. It natively supports multimodal input (text and image) and multilingual output (text and code) across 12 languages.\n\nDesigned for assistant-style interaction and visual reasoning, Scout features a massive 10-million-token context window. It is instruction-tuned for tasks like multilingual chat and image understanding and is released under the Llama 4 Community License for local or commercial deployment.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen-image",
      "name": "qwen-image",
      "display_name": "qwen-image",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen-Image is a foundational image generation model in the Qwen series, achieving significant progress in complex text rendering and precise image editing. Experiments show that the model has strong general capabilities in image generation and editing, especially excelling in Chinese text rendering.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen-image-edit",
      "name": "qwen-image-edit",
      "display_name": "qwen-image-edit",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen-Image-Edit is the image editing version of Qwen-Image. Based on the 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image's unique text rendering capabilities to image editing tasks, achieving precise text editing. Additionally, Qwen-Image-Edit can input the same image into Qwen2.5-VL (for visual semantic control) and the VAE encoder (for visual appearance control), enabling both semantic and appearance editing functionalities.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen-image-plus",
      "name": "qwen-image-plus",
      "display_name": "qwen-image-plus",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen-Image is a foundational image generation model in the Qwen series, achieving significant progress in complex text rendering and precise image editing. Experiments show that the model has strong general capabilities in image generation and editing, especially excelling in Chinese text rendering.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen-mt-plus",
      "name": "qwen-mt-plus",
      "display_name": "qwen-mt-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.492,
        "output": 1.476
      },
      "limit": {
        "context": 16000,
        "output": 16000
      },
      "metadata": {
        "description": "Based on the comprehensive upgrade of Qwen3, this flagship translation large model supports bidirectional translation across 92 languages. It offers fully enhanced model performance and translation quality, along with more stable terminology customization, format fidelity, and domain-prompting capabilities, making translations more accurate and natural.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.492,
          "output": 1.476
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-mt-turbo",
      "name": "qwen-mt-turbo",
      "display_name": "qwen-mt-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.192,
        "output": 0.534912
      },
      "limit": {
        "context": 16000,
        "output": 16000
      },
      "metadata": {
        "description": "Based on the comprehensive upgrade of Qwen3, this flagship translation large model supports bidirectional translation across 92 languages. It offers fully enhanced model performance and translation quality, along with more stable terminology customization, format fidelity, and domain-prompting capabilities, making translations more accurate and natural.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.192,
          "output": 0.534912
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-embedding-0.6b",
      "name": "qwen3-embedding-0.6b",
      "display_name": "qwen3-embedding-0.6b",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 Embedding model series is the latest proprietary model family from Qwen, specifically designed for text embedding and ranking tasks. Based on the dense base models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the excellent multilingual capabilities, long-text understanding, and reasoning skills of its base models. The Qwen3 Embedding series demonstrates significant advancements in various text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-embedding-4b",
      "name": "qwen3-embedding-4b",
      "display_name": "qwen3-embedding-4b",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 Embedding model series is the latest proprietary model family from Qwen, specifically designed for text embedding and ranking tasks. Based on the dense base models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the excellent multilingual capabilities, long-text understanding, and reasoning skills of its base models. The Qwen3 Embedding series demonstrates significant advancements in various text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-embedding-8b",
      "name": "qwen3-embedding-8b",
      "display_name": "qwen3-embedding-8b",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 Embedding model series is the latest proprietary model family from Qwen, specifically designed for text embedding and ranking tasks. Based on the dense base models of the Qwen3 series, it offers comprehensive text embedding and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the excellent multilingual capabilities, long-text understanding, and reasoning skills of its base models. The Qwen3 Embedding series demonstrates significant advancements in various text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bilingual text mining.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-reranker-0.6b",
      "name": "qwen3-reranker-0.6b",
      "display_name": "qwen3-reranker-0.6b",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.11
      },
      "limit": {
        "context": 16000,
        "output": 16000
      },
      "metadata": {
        "description": "Based on the dense foundational model of the Qwen3 series, it is specifically designed for ranking tasks. It inherits the base model’s outstanding multilingual capabilities, long-text understanding, and reasoning skills, achieving significant advancements in ranking tasks.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.11
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-reranker-4b",
      "name": "qwen3-reranker-4b",
      "display_name": "qwen3-reranker-4b",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.11
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Based on the dense foundational model of the Qwen3 series, it is specifically designed for ranking tasks. It inherits the base model’s outstanding multilingual capabilities, long-text understanding, and reasoning skills, achieving significant advancements in ranking tasks.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.11
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-reranker-8b",
      "name": "qwen3-reranker-8b",
      "display_name": "qwen3-reranker-8b",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.11
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Based on the dense foundational model of the Qwen3 series, it is specifically designed for ranking tasks. It inherits the base model’s outstanding multilingual capabilities, long-text understanding, and reasoning skills, achieving significant advancements in ranking tasks.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.11
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "tao-8k",
      "name": "tao-8k",
      "display_name": "tao-8k",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "bce-reranker-base",
      "name": "bce-reranker-base",
      "display_name": "bce-reranker-base",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Based on the dense foundational model of the Qwen3 series, it is specifically designed for ranking tasks. It inherits the base model’s outstanding multilingual capabilities, long-text understanding, and reasoning skills, achieving significant advancements in ranking tasks.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "codex-mini-latest",
      "name": "codex-mini-latest",
      "display_name": "codex-mini-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.5,
        "output": 6,
        "cache_read": 0.375
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Only supports v1/responses API calls.https://docs.aihubmix.com/en/api/Responses-API\ncodex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct use in the API, we recommend starting with gpt-4.1.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.375,
          "input": 1.5,
          "output": 6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seedream-4-0",
      "name": "doubao-seedream-4-0",
      "display_name": "doubao-seedream-4-0",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Seedream 4.0 is a SOTA-level multimodal image creation model based on leading architecture. It breaks the creative boundaries of traditional text-to-image models by natively supporting text, single image, and multiple image inputs. Users can freely combine text and images to achieve various creative styles within the same model, such as multi-image fusion creation based on subject consistency, image editing, and set image generation, making image creation more flexible and controllable.\nSeedream 4.0 supports composite editing with up to 10 images in a single input. Through deep reasoning of prompt words, it automatically adapts the optimal image aspect ratio and generation quantity, enabling continuous output of up to 15 content-related images at one time. Additionally, the model significantly improves the accuracy and content diversity of Chinese generation, supports 4K ultra-high-definition output, and provides a one-stop solution from generation to editing for professional image creation.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "embedding-v1",
      "name": "embedding-v1",
      "display_name": "embedding-v1",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Embedding-V1 is a text representation model based on Baidu's Wenxin large model technology, capable of converting text into numerical vector forms for applications such as text retrieval, information recommendation, and knowledge mining. Embedding-V1 provides an Embeddings interface that generates corresponding vector representations based on the input content. By calling this interface, you can input text into the model and obtain the corresponding vector representations for subsequent text processing and analysis.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ernie-4.5-turbo-latest",
      "name": "ernie-4.5-turbo-latest",
      "display_name": "ernie-4.5-turbo-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.44
      },
      "limit": {
        "context": 135000,
        "output": 135000
      },
      "metadata": {
        "description": "Wenxin 4.5 Turbo also has significant improvements in hallucination reduction, logical reasoning, and coding capabilities. Compared to Wenxin 4.5, it is faster and more affordable.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.44
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-irag-edit",
      "name": "ernie-irag-edit",
      "display_name": "ernie-irag-edit",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Baidu's self-developed ERNIE iRAG Edit image editing model supports operations based on images such as erase (object removal), repaint (object redrawing), and variation (variant generation).",
        "typeHints": [
          "image_generation"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-clip-v2",
      "name": "jina-clip-v2",
      "display_name": "jina-clip-v2",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Multi-modal Embeddings Model, multilingual, 1024-dimensional, 865M parameters.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-reranker-m0",
      "name": "jina-reranker-m0",
      "display_name": "jina-reranker-m0",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Multimodal multilingual document reranker, 10K context, 2.4B parameters, for visual document sorting.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-colbert-v2",
      "name": "jina-colbert-v2",
      "display_name": "jina-colbert-v2",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Multi-language ColBERT embeddings model, 560M parameters, used for embedding and reranking.",
        "typeHints": [
          "embedding",
          "rerank"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-search-preview",
      "name": "gpt-4o-search-preview",
      "display_name": "gpt-4o-search-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 1.25
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Using the Chat Completions API, you can directly access the fine-tuned models and tool used by Search in ChatGPT.\n\nWhen using Chat Completions, the model always retrieves information from the web before responding to your query. To use web_search_preview as a tool that models like gpt-4o and gpt-4o-mini invoke only when necessary, switch to using the Responses API.\n\nCurrently, you need to use one of these models to use web search in Chat Completions:\n\ngpt-4o-search-preview\ngpt-4o-mini-search-preview\nWeb search parameter example\nimport OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst completion = await client.chat.completions.create({\n    model: \"gpt-4o-search-preview\",\n    web_search_options: {},\n    messages: [{\n        \"role\": \"user\",\n        \"content\": \"What was a positive news story from today?\"\n    }],\n});\n\nconsole.log(completion.choices[0].message.content);\nOutput and citations\nThe API response item in the choices array will include:\n\nmessage.content with the text result from the model, inclusive of any inline citations\nannotations with a list of cited URLs\nBy default, the model's response will include inline citations for URLs found in the web search results. In addition to this, the url_citation annotation object will contain the URL and title of the cited source, as well as the start and end index characters in the model's response where those sources were used.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "web",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 1.25,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "DeepSeek-R1",
      "name": "DeepSeek-R1",
      "display_name": "DeepSeek-R1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 2
      },
      "limit": {
        "context": 1638000,
        "output": 1638000
      },
      "metadata": {
        "description": "DeepSeek R1 is a new open-source model with performance on par with OpenAI's o1 and features fully open reasoning tokens. It is a 671B-parameter Mixture-of-Experts (MoE) model that activates 37B parameters during inference.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-mini-search-preview",
      "name": "gpt-4o-mini-search-preview",
      "display_name": "gpt-4o-mini-search-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.15,
        "output": 0.6,
        "cache_read": 0.075
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Using the Chat Completions API, you can directly access the fine-tuned models and tool used by Search in ChatGPT.\n\nWhen using Chat Completions, the model always retrieves information from the web before responding to your query. To use web_search_preview as a tool that models like gpt-4o and gpt-4o-mini invoke only when necessary, switch to using the Responses API.\n\nCurrently, you need to use one of these models to use web search in Chat Completions:\n\ngpt-4o-search-preview\ngpt-4o-mini-search-preview\nWeb search parameter example\nimport OpenAI from \"openai\";\nconst client = new OpenAI();\n\nconst completion = await client.chat.completions.create({\n    model: \"gpt-4o-search-preview\",\n    web_search_options: {},\n    messages: [{\n        \"role\": \"user\",\n        \"content\": \"What was a positive news story from today?\"\n    }],\n});\n\nconsole.log(completion.choices[0].message.content);\nOutput and citations\nThe API response item in the choices array will include:\n\nmessage.content with the text result from the model, inclusive of any inline citations\nannotations with a list of cited URLs\nBy default, the model's response will include inline citations for URLs found in the web search results. In addition to this, the url_citation annotation object will contain the URL and title of the cited source, as well as the start and end index characters in the model's response where those sources were used.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "web",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.15,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-embeddings-v3",
      "name": "jina-embeddings-v3",
      "display_name": "jina-embeddings-v3",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Text Embeddings Model, multilingual, 1024-dimensional, 570M parameters.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "FLUX.1-Kontext-pro",
      "name": "FLUX.1-Kontext-pro",
      "display_name": "FLUX.1-Kontext-pro",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 40,
        "output": 40
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Generate and edit images through both text and image prompts. Flux.1 Kontext is a multimodal flow matching model that enables both text-to-image generation and in-context image editing. Modify images while maintaining character consistency and performing local edits up to 8x faster than other leading models.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 40,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-7-sonnet",
      "name": "claude-3-7-sonnet",
      "display_name": "claude-3-7-sonnet",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Support for the thinking parameter through the original Claude SDK.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-4.5",
      "name": "ernie-4.5",
      "display_name": "ernie-4.5",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.272
      },
      "limit": {
        "context": 160000,
        "output": 160000
      },
      "metadata": {
        "description": "Wenxin Large Model 4.5 is a next-generation native multimodal foundational model independently developed by Baidu. It achieves collaborative optimization through joint modeling of multiple modalities, demonstrating excellent multimodal understanding capabilities; it possesses more advanced language abilities, with comprehensive improvements in comprehension, generation, logic, and memory, as well as significant enhancements in hallucination reduction, logical reasoning, and coding capabilities.ERNIE-4.5-21B-A3B is an aligned open-source model with a MoE structure, having a total of 21 billion parameters and 3 billion activated parameters.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.272
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-4.5-turbo-vl",
      "name": "ernie-4.5-turbo-vl",
      "display_name": "ernie-4.5-turbo-vl",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 1.2
      },
      "limit": {
        "context": 139000,
        "output": 139000
      },
      "metadata": {
        "description": "The new version of the Wenxin Yiyan large model significantly improves capabilities in image understanding, creation, translation, and coding. It supports a context length of up to 32K tokens for the first time, with a notable reduction in the latency of the first token.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.4,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.0-flash",
      "name": "gemini-2.0-flash",
      "display_name": "gemini-2.0-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.25
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Gemini 2.0 Flash is Google's latest lightweight model featuring extremely low hallucination rates while maintaining fast response times, offering developers high-precision and efficient AI solutions particularly suited for applications requiring high factual accuracy.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.25,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.0-flash-preview-image-generation",
      "name": "gemini-2.0-flash-preview-image-generation",
      "display_name": "gemini-2.0-flash-preview-image-generation",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemini 2.0 Flash EXP is the official preview version of the drawing model. Compared to Imagen 3.0, Gemini’s image generation is better suited for scenarios that require contextual understanding and reasoning, rather than the pursuit of ultimate artistic performance and visual quality.",
        "typeHints": [
          "llm",
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "FLUX-1.1-pro",
      "name": "FLUX-1.1-pro",
      "display_name": "FLUX-1.1-pro",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 40,
        "output": 40,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "FLUX-1.1-pro is an AI image generation tool for professional creators and content workflows. It understands complex semantic and structural instructions to deliver high consistency, multi-image coherence, and style customization from text prompts.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 40,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "o3-mini",
      "name": "o3-mini",
      "display_name": "o3-mini",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 4.4,
        "cache_read": 0.55
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "OpenAI's latest fast inference model excels at STEAM tasks and offers exceptional cost-effectiveness. Official support for cache hits reduces input prices by half.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.55,
          "input": 1.1,
          "output": 4.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seed-1-6",
      "name": "doubao-seed-1-6",
      "display_name": "doubao-seed-1-6",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.18,
        "output": 1.8,
        "cache_read": 0.036
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Doubao-Seed-1.6 is a brand new multimodal deep reasoning model that supports four types of reasoning effort: minimal, low, medium, and high. It offers stronger model performance, serving complex tasks and challenging scenarios. It supports a 256k context window, with output length up to a maximum of 32k tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking，tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.036,
          "input": 0.18,
          "output": 1.8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seed-1-6-flash",
      "name": "doubao-seed-1-6-flash",
      "display_name": "doubao-seed-1-6-flash",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.044,
        "output": 0.44,
        "cache_read": 0.0088
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Doubao-Seed-1.6-flash is an extremely fast multimodal deep thinking model, with TPOT requiring only 10ms. It supports both text and visual understanding, with its text comprehension skills surpassing the previous generation lite model and its visual understanding on par with competitor's pro series models. It supports a 256k context window and an output length of up to 16k tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking，tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.0088,
          "input": 0.044,
          "output": 0.44
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seed-1-6-lite",
      "name": "doubao-seed-1-6-lite",
      "display_name": "doubao-seed-1-6-lite",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.082,
        "output": 0.656,
        "cache_read": 0.0164
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Doubao-Seed-1.6-lite is a brand new multimodal deep reasoning model that supports adjustable reasoning effort, with four modes: Minimal, Low, Medium, and High. It offers better cost performance, making it the best choice for common tasks, with a context window of up to 256k.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking，tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.0164,
          "input": 0.082,
          "output": 0.656
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seed-1-6-thinking",
      "name": "doubao-seed-1-6-thinking",
      "display_name": "doubao-seed-1-6-thinking",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.18,
        "output": 1.8,
        "cache_read": 0.036
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "The Doubao-Seed-1.6-thinking model has significantly enhanced reasoning capabilities. Compared with Doubao-1.5-thinking-pro, it has further improvements in fundamental abilities such as coding, mathematics, and logical reasoning, and now also supports visual understanding. It supports a 256k context window, with output length supporting up to 16k tokens.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking，tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0.036,
          "input": 0.18,
          "output": 1.8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-30b-a3b-instruct-2507",
      "name": "qwen3-30b-a3b-instruct-2507",
      "display_name": "qwen3-30b-a3b-instruct-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1028,
        "output": 0.4112
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.",
        "pricing": {
          "input": 0.1028,
          "output": 0.4112
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-30b-a3b-thinking-2507",
      "name": "qwen3-30b-a3b-thinking-2507",
      "display_name": "qwen3-30b-a3b-thinking-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.12,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.",
        "pricing": {
          "input": 0.12,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-embedding-001",
      "name": "gemini-embedding-001",
      "display_name": "gemini-embedding-001",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.15,
        "output": 0.15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Latest version",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.15,
          "output": 0.15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-oss-120b",
      "name": "gpt-oss-120b",
      "display_name": "gpt-oss-120b",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.18,
        "output": 0.9
      },
      "limit": {
        "context": 131072,
        "output": 131072
      },
      "metadata": {
        "description": "gpt-oss-120b is a 117B-parameter open-weight Mixture-of-Experts (MoE) language model from OpenAI, designed for high-reasoning, agentic, and general-purpose production use cases. Activating just 5.1B parameters per pass, it is optimized to run on a single H100 GPU with native MXFP4 quantization. The model features configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.18,
          "output": 0.9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-3-235b-a22b-thinking-2507",
      "name": "qwen-3-235b-a22b-thinking-2507",
      "display_name": "qwen-3-235b-a22b-thinking-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.28,
        "output": 2.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "cerebras",
        "pricing": {
          "input": 0.28,
          "output": 2.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-R1-Distill-Qwen-32B",
      "name": "DeepSeek-R1-Distill-Qwen-32B",
      "display_name": "DeepSeek-R1-Distill-Qwen-32B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.28,
        "output": 0.84
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophnet platform. Deepseek-R1-Distill-Qwen-32B is a knowledge-distilled large language model based on Qwen 2.5 32B and trained using outputs from DeepSeek R1.\nDeepSeek-R1 addresses issues such as infinite repetition, poor readability, and language mixing by introducing cold-start data before reinforcement learning.\nDeepSeek-R1’s performance in mathematics, programming, and reasoning tasks is comparable to OpenAI-o1.\nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models based on Llama and Qwen.\nDeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini on multiple benchmark tests, setting new state-of-the-art results for dense models.",
        "pricing": {
          "input": 0.28,
          "output": 0.84
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-R1-Distill-Qwen-7B",
      "name": "DeepSeek-R1-Distill-Qwen-7B",
      "display_name": "DeepSeek-R1-Distill-Qwen-7B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.06,
        "output": 0.12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophnet platform. DeepSeek-R1-Distill-Qwen-7B is a distilled model based on the Qwen architecture, optimized for high reasoning speed and low cost. It achieves approximately 70% of the performance of the original model at the 7B scale, while reducing response latency by 40%, making it suitable for real-time interactive scenarios.\nThe API call cost is only one-quarter of the original Qwen-7B.\nIt supports streaming output, making it suitable for applications like chatbots.\nIt achieves an accuracy of over 65% on the GSM8K math task.",
        "pricing": {
          "input": 0.06,
          "output": 0.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "QwQ-32B",
      "name": "QwQ-32B",
      "display_name": "QwQ-32B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.28,
        "output": 0.84
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophnet platform. QwQ is an inference model from the Qianwen series, featuring outstanding thinking and reasoning capabilities.\nCompared to traditional instruction-finetuned models, QwQ can achieve significantly enhanced performance on downstream tasks, especially on difficult problems.\nQwQ-32B is a medium-sized inference model capable of delivering competitive performance compared to state-of-the-art inference models such as DeepSeek-R1 and o1-mini.\nIt supports long context lengths of up to 128K tokens and can generate text up to 128K tokens.",
        "pricing": {
          "input": 0.28,
          "output": 0.84
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen2-VL-72B-Instruct",
      "name": "Qwen2-VL-72B-Instruct",
      "display_name": "Qwen2-VL-72B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 2.18,
        "output": 6.54
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophnet platform. Qwen2-VL-72B-Instruct is the latest iteration in the Qwen2-VL series launched by Alibaba Cloud, representing nearly a year of innovative achievements. This model has 72 billion parameters and can understand images of various resolutions and aspect ratios. Additionally, it supports video understanding of over 20 minutes, enabling high-quality video question answering, dialogue, and content creation, along with complex reasoning and decision-making capabilities.\n\n- State-of-the-art image understanding: capable of processing images of various resolutions and aspect ratios, performing excellently across multiple visual understanding benchmarks.\n- Long video understanding: supports video comprehension exceeding 20 minutes, enabling high-quality video Q&A, dialogues, and content creation.\n- Agent operation capability: equipped with complex reasoning and decision-making abilities, it can integrate with devices such as phones and robots to perform automated operations based on visual environments and textual instructions.\n- Multilingual support: in addition to English and Chinese, it supports understanding text in images in multiple languages, including most European languages, Japanese, Korean, Arabic, Vietnamese, and more.\n- Supports a maximum context length of 128K tokens, offering powerful processing capabilities.",
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 2.18,
          "output": 6.54
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "Qwen2-VL-7B-Instruct",
      "name": "Qwen2-VL-7B-Instruct",
      "display_name": "Qwen2-VL-7B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.28,
        "output": 0.7
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model provider is the Sophnet platform. Qwen2-VL-7B-Instruct is the latest vision-language model launched by Alibaba Cloud and the newest member of the Qwen family. This model is proficient not only in recognizing common objects but also in analyzing text, charts, icons, and layouts within images. As a visual agent, it can reason and dynamically guide tool usage, supporting operations on computers and mobile phones. Additionally, it can understand long videos exceeding one hour and capture key events, accurately locate objects in images, and generate structured outputs for data such as invoices and tables, making it suitable for various scenarios including finance and business.\n\n- Vision understanding capability: not only recognizes common objects but also analyzes text, charts, icons, and layouts within images.\n- Agent capability: functions as a visual agent capable of reasoning and dynamically guiding tool usage, supporting operations on computers and mobile phones.\n- Long video understanding: can comprehend video content over one hour in length and accurately localize relevant video segments.\n- Visual localization: precisely locates objects within images by generating bounding boxes or points, providing stable JSON coordinate outputs.\n- Structured output: supports structured data output for invoices, tables, and other data, suitable for finance, business, and various other scenarios.",
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.28,
          "output": 0.7
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "cc-kimi-for-coding",
      "name": "cc-kimi-for-coding",
      "display_name": "cc-kimi-for-coding",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "for claude code",
        "pricing": {
          "cache_read": 0.02,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen3-30B-A3B",
      "name": "Qwen/Qwen3-30B-A3B",
      "display_name": "Qwen/Qwen3-30B-A3B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai",
        "pricing": {
          "input": 1,
          "output": 1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen3-32B",
      "name": "Qwen/Qwen3-32B",
      "display_name": "Qwen/Qwen3-32B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen3-14B",
      "name": "Qwen/Qwen3-14B",
      "display_name": "Qwen/Qwen3-14B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai",
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen3-8B",
      "name": "Qwen/Qwen3-8B",
      "display_name": "Qwen/Qwen3-8B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai",
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-pro-exp-02-05-search",
      "name": "gemini-2.0-pro-exp-02-05-search",
      "display_name": "gemini-2.0-pro-exp-02-05-search",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Integrated with Google's official search and internet connectivity features.",
        "features": [
          "web"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.5-pro-preview-06-05",
      "name": "gemini-2.5-pro-preview-06-05",
      "display_name": "gemini-2.5-pro-preview-06-05",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 10,
        "cache_read": 0.31
      },
      "limit": {
        "context": 1048576,
        "output": 1048576
      },
      "metadata": {
        "description": "Google’s latest multimodal flagship model, combining exceptional coding and reasoning capabilities. Its massive 1 million token context window (soon to expand to 2 million) places it at the top of the WebDevArena and LMArena leaderboards. It is particularly well-suited for developing aesthetically pleasing and highly functional interactive web applications, code transformation, and complex workflows. The newly introduced \"reasoning budget\" feature cleverly balances cost and performance, while optimized tool calls and response styles further enhance development efficiency, making it the ideal choice for rapid prototyping and advanced coding.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "Aihubmix-MAI-DS-R1",
      "name": "Aihubmix-MAI-DS-R1",
      "display_name": "Aihubmix-MAI-DS-R1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 1.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "MAI-DS-R1 is a refined version of DeepSeek-R1 by Microsoft AI, designed to improve responsiveness to previously blocked topics while enhancing safety. It integrates 110k Tulu-3 SFT samples and 350k multilingual safety-alignment examples. The model retains strong reasoning and coding abilities, surpasses R1-1776 in handling sensitive queries, and reduces harmful content leakage. Based on a transformer MoE architecture, it suits general-purpose tasks—excluding legal, medical, or autonomous systems.",
        "pricing": {
          "input": 0.6,
          "output": 1.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "embedding-2",
      "name": "embedding-2",
      "display_name": "embedding-2",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.0686,
        "output": 0.0686
      },
      "limit": {
        "context": 8000,
        "output": 8000
      },
      "metadata": {
        "description": "A text vector model that converts input text information into vector representations so that, in conjunction with a vector database, it provides an external knowledge base for the large model, thereby improving the accuracy of the model’s reasoning.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.0686,
          "output": 0.0686
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "embedding-3",
      "name": "embedding-3",
      "display_name": "embedding-3",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.0686,
        "output": 0.0686
      },
      "limit": {
        "context": 8000,
        "output": 8000
      },
      "metadata": {
        "description": "A text vector model that converts input text into vector representations to work with a vector database and provide an external knowledge base for a large model. The model supports custom vector dimensions; it is recommended to choose 256, 512, 1024, or 2048 dimensions.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.0686,
          "output": 0.0686
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-search",
      "name": "gemini-2.0-flash-search",
      "display_name": "gemini-2.0-flash-search",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.25
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Integrated with Google's official search and internet connectivity features.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "web"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.25,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "Qwen/Qwen2.5-VL-72B-Instruct",
      "name": "Qwen/Qwen2.5-VL-72B-Instruct",
      "display_name": "Qwen/Qwen2.5-VL-72B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.5,
        "output": 0.5,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen2.5-VL is a visual language model from the Qwen2.5 series, equipped with strong visual understanding and reasoning capabilities. It can recognize objects, analyze text and charts, understand key events in long videos, and accurately locate targets within images. The model supports structured output, making it suitable for data such as invoices and forms, and performs excellently in multiple benchmark tests.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "o1",
      "name": "o1",
      "display_name": "o1",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 15,
        "output": 60,
        "cache_read": 7.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "OpenAI's most powerful O-series model supports official cache hits that halve the input cost.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 7.5,
          "input": 15,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o1-pro",
      "name": "o1-pro",
      "display_name": "o1-pro",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 170,
        "output": 680,
        "cache_read": 170
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 170,
          "input": 170,
          "output": 680
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "name": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "display_name": "ByteDance-Seed/Seed-OSS-36B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.534
      },
      "limit": {
        "context": 256000,
        "output": 256000
      },
      "metadata": {
        "description": "Seed-OSS is a series of open-source large language models developed by ByteDance's Seed team, designed specifically for powerful long-context processing, reasoning, agents, and general capabilities. Among this series, Seed-OSS-36B-Instruct is an instruction-tuned model with 36 billion parameters that natively supports ultra-long context lengths, enabling it to process massive documents or complex codebases in a single pass. This model is specially optimized for reasoning, code generation, and agent tasks (such as tool usage), while maintaining balanced and excellent general capabilities. A notable feature of this model is the \"Thinking Budget\" functionality, which allows users to flexibly adjust the inference length as needed, thereby effectively improving inference efficiency in practical applications.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking，tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.534
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-seed-1-6-250615",
      "name": "doubao-seed-1-6-250615",
      "display_name": "doubao-seed-1-6-250615",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.18,
        "output": 2.52,
        "cache_read": 0.036
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-Seed-1.6 is a brand new multimodal deep reasoning model that supports four types of reasoning effort: minimal, low, medium, and high. It offers stronger model performance, serving complex tasks and challenging scenarios. It supports a 256k context window, with output length up to a maximum of 32k tokens.",
        "pricing": {
          "cache_read": 0.036,
          "input": 0.18,
          "output": 2.52
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-seed-1-6-flash-250615",
      "name": "doubao-seed-1-6-flash-250615",
      "display_name": "doubao-seed-1-6-flash-250615",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.044,
        "output": 0.44,
        "cache_read": 0.0088
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-Seed-1.6-flash is an extremely fast multimodal deep thinking model, with TPOT requiring only 10ms. It supports both text and visual understanding, with its text comprehension skills surpassing the previous generation lite model and its visual understanding on par with competitor's pro series models. It supports a 256k context window and an output length of up to 16k tokens.",
        "pricing": {
          "cache_read": 0.0088,
          "input": 0.044,
          "output": 0.44
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-seed-1-6-thinking-250615",
      "name": "doubao-seed-1-6-thinking-250615",
      "display_name": "doubao-seed-1-6-thinking-250615",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.18,
        "output": 2.52,
        "cache_read": 0.036
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Doubao-Seed-1.6-thinking model has significantly enhanced reasoning capabilities. Compared with Doubao-1.5-thinking-pro, it has further improvements in fundamental abilities such as coding, mathematics, and logical reasoning, and now also supports visual understanding. It supports a 256k context window, with output length supporting up to 16k tokens.",
        "pricing": {
          "cache_read": 0.036,
          "input": 0.18,
          "output": 2.52
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-seed-1-6-vision-250815",
      "name": "doubao-seed-1-6-vision-250815",
      "display_name": "doubao-seed-1-6-vision-250815",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.10959,
        "output": 1.0959,
        "cache_read": 0.021918
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-Seed-1.6-vision is a visual deep-thinking model that demonstrates stronger general multimodal understanding and reasoning capabilities in scenarios such as education, image moderation, inspection and security, and AI search Q&A. It supports a 256K context window and an output length of up to 64K tokens.",
        "pricing": {
          "cache_read": 0.021918,
          "input": 0.10959,
          "output": 1.0959
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-1.5-thinking-pro",
      "name": "Doubao-1.5-thinking-pro",
      "display_name": "Doubao-1.5-thinking-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.62,
        "output": 2.48,
        "cache_read": 0.62
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-1.5 is a brand-new deep thinking model that excels in specialized fields such as mathematics, programming, scientific reasoning, and general tasks like creative writing. It achieves or approaches the top-tier industry level on multiple authoritative benchmarks including AIME 2024, Codeforces, and GPQA. It supports a 128k context window and 16k output.",
        "pricing": {
          "cache_read": 0.62,
          "input": 0.62,
          "output": 2.48
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-minimax-m2",
      "name": "cc-minimax-m2",
      "display_name": "cc-minimax-m2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude Code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-Prover-V2-671B",
      "name": "deepseek-ai/DeepSeek-Prover-V2-671B",
      "display_name": "deepseek-ai/DeepSeek-Prover-V2-671B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai\nDeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from DeepSeek-Prover-V1.5 Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.",
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-3-12b-it",
      "name": "gemma-3-12b-it",
      "display_name": "gemma-3-12b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-3-1b-it",
      "name": "gemma-3-1b-it",
      "display_name": "gemma-3-1b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-3-27b-it",
      "name": "gemma-3-27b-it",
      "display_name": "gemma-3-27b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-3-4b-it",
      "name": "gemma-3-4b-it",
      "display_name": "gemma-3-4b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-3n-e4b-it",
      "name": "gemma-3n-e4b-it",
      "display_name": "gemma-3n-e4b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. This model includes innovations in parameter-efficient processing, including Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture that provides the flexibility to reduce compute and memory requirements. These models feature audio input handling, as well as text and visual data.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-image-vip",
      "name": "gpt-4o-image-vip",
      "display_name": "gpt-4o-image-vip",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 7,
        "output": 7,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "First Taste of GPT-4o's Image Generation API: Perfectly mirrors the web version's raw image creation capabilities, supporting both text-to-image and image+text-to-image generation. Each creation costs as little as $0.009.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 7,
          "output": 7
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4o-image",
      "name": "gpt-4o-image",
      "display_name": "gpt-4o-image",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3,
        "output": 3,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "First Taste of GPT-4o's Image Generation API: Perfectly mirrors the web version's raw image creation capabilities, supporting both text-to-image and image+text-to-image generation. Each creation costs as little as $0.005.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 3,
          "output": 3
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "deepseek-r1-distill-llama-70b",
      "name": "deepseek-r1-distill-llama-70b",
      "display_name": "deepseek-r1-distill-llama-70b",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.8,
        "output": 1.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by Groq, the DeepSeek-R1-Distill model is fine-tuned based on an open-source model, using samples generated by DeepSeek-R1. We have made slight modifications to their configurations and tokenizers. Please use our settings to run these models.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.8,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-mini-tts",
      "name": "gpt-4o-mini-tts",
      "display_name": "gpt-4o-mini-tts",
      "type": "audio",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 15,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "OpenAI’s latest TTS model, gpt-4o-mini-tts, uses the same API endpoint (/v1/audio/speech) as tts-1. However, OpenAI introduced a new pricing method without providing billing details via API, causing discrepancies between official pricing and aihubmix’s charges—some requests may cost more, others less. Avoid using this model if precise billing accuracy is essential.",
        "typeHints": [
          "tts"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 15,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "tngtech/DeepSeek-R1T-Chimera",
      "name": "tngtech/DeepSeek-R1T-Chimera",
      "display_name": "tngtech/DeepSeek-R1T-Chimera",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai\nDeepSeek-R1T-Chimera merges DeepSeek-R1’s reasoning strengths with DeepSeek-V3 (0324)’s token-efficiency improvements into a MoE Transformer optimized for general text generation. It integrates pretrained weights from both models and is released under the MIT license for research and commercial use.",
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-exp",
      "name": "gemini-2.0-flash-exp",
      "display_name": "gemini-2.0-flash-exp",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.02,
        "output": 0.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "https://doc.aihubmix.com/en/api/Gemini%20%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%92%8C%E7%BC%96%E8%BE%91\nInstructions:\n\nNeed to add parameters to experience new features: \"modalities\":[\"text\",\"image\"]\nImages are passed and output in Base64 encoding\nAs an experimental model, it's recommended to explicitly specify \"output image\", otherwise it might only output text\nDefault height for output images is 1024px\nPython calls require the latest OpenAI SDK, run pip install -U openai first",
        "typeHints": [
          "llm",
          "image_generation"
        ],
        "features": [
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "input": 0.02,
          "output": 0.08
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-5-sonnet",
      "name": "claude-3-5-sonnet",
      "display_name": "claude-3-5-sonnet",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Claude 3.5 Sonnet delivers performance superior to Opus and speeds faster than its predecessor, all at the same price point. Its core strengths include:\n\nCoding: Autonomously writes, edits, and executes code with advanced reasoning and troubleshooting.\nData Science: Augments human expertise by analyzing unstructured data and using multiple tools to generate insights.\nVisual Processing: Excels at interpreting charts, graphs, and images, accurately transcribing text to derive high-level insights.\nAgentic Tasks: Exceptional tool use makes it highly effective for complex, multi-step agentic workflows that interact with other systems.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "o1-preview",
      "name": "o1-preview",
      "display_name": "o1-preview",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 15,
        "output": 60,
        "cache_read": 7.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The latest and most powerful inference model from OpenAI; AiHubMix uses both OpenAI and Microsoft Azure OpenAI channels simultaneously to achieve high-concurrency load balancing.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 7.5,
          "input": 15,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "o1-mini",
      "name": "o1-mini",
      "display_name": "o1-mini",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 12,
        "cache_read": 1.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "o1-mini is faster and 80% cheaper, and is competitive with o1-preview on coding tasks. AiHubMix uses both OpenAI and Microsoft Azure OpenAI channels simultaneously.",
        "pricing": {
          "cache_read": 1.5,
          "input": 3,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-thinking-exp-01-21",
      "name": "gemini-2.0-flash-thinking-exp-01-21",
      "display_name": "gemini-2.0-flash-thinking-exp-01-21",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.076,
        "output": 0.304
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The latest version, Gemini 2.0 Flash Thinking mode, is an experimental model designed to generate the \"thought process\" that the model goes through during its responses. Therefore, Gemini 2.0 Flash Thinking mode has stronger reasoning capabilities in its responses compared to the base Gemini 2.0 Flash model.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "input": 0.076,
          "output": 0.304
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4o-2024-11-20",
      "name": "gpt-4o-2024-11-20",
      "display_name": "gpt-4o-2024-11-20",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 1.25
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "The latest version of the GPT-4o model; it is recommended to use this version, as it is currently smarter than the regular 4o.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 1.25,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4o",
      "name": "gpt-4o",
      "display_name": "gpt-4o",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 1.25
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "GPT-4o (“o” stands for “omni”) is a new-generation multimodal model designed for more natural human–computer interaction. It can accept any combination of text, audio, image, and video as input, and generate multimodal outputs including text, audio, and images. With audio response latency as low as 232 milliseconds on average around 320 milliseconds, it approaches real human conversational speed. The model delivers strong performance in English text and code, significantly improved multilingual understanding, and outstanding capabilities in visual and audio perception, while offering faster API performance and substantially reduced cost for real-time and complex multimodal applications.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 1.25,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "chatgpt-4o-latest",
      "name": "chatgpt-4o-latest",
      "display_name": "chatgpt-4o-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 5,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This model will point to the latest GPT-4o model used by ChatGPT.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 5,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-4o-mini",
      "name": "gpt-4o-mini",
      "display_name": "gpt-4o-mini",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.15,
        "output": 0.6,
        "cache_read": 0.075
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "The lightweight version of GPT-4o, which is affordable and fast, suitable for handling simple tasks; our site supports the official automatic caching for this model, and charges for cache hits will be automatically halved.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.15,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "AiHubmix-mistral-medium",
      "name": "AiHubmix-mistral-medium",
      "display_name": "AiHubmix-mistral-medium",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Mistral Medium 3 is a SOTA & versatile model designed for a wide range of tasks, including programming, mathematical reasoning, understanding long documents, summarization, and dialogue.\n\nIt boasts multi-modal capabilities, enabling it to process visual inputs, and supports dozens of languages, including over 80 coding languages. Additionally, it features function calling and agentic workflows.\n\nMistral Medium 3 is optimized for single-node inference, particularly for long-context applications. Its size allows it to achieve high throughput on a single node.",
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-pro-exp-02-05",
      "name": "gemini-2.0-pro-exp-02-05",
      "display_name": "gemini-2.0-pro-exp-02-05",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The latest experimental version of Gemini-2.0-Pro",
        "typeHints": [
          "llm",
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "minimax-m2",
      "name": "minimax-m2",
      "display_name": "minimax-m2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.288,
        "output": 1.152
      },
      "limit": {
        "context": 204800,
        "output": 204800
      },
      "metadata": {
        "description": "MiniMax-M2 redefines efficiency for intelligent agents. It is a compact, fast, and cost-effective MoE model with a total of 230 billion parameters and 10 billion active parameters, designed for top performance in coding and intelligent agent tasks while maintaining strong general intelligence. With only 10 billion active parameters, MiniMax-M2 delivers the complex end-to-end tool usage performance expected from today's leading models, but in a more streamlined form factor, making deployment and scaling easier than ever before.",
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.288,
          "output": 1.152
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ERNIE-X1.1-Preview",
      "name": "ERNIE-X1.1-Preview",
      "display_name": "ERNIE-X1.1-Preview",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.136,
        "output": 0.544
      },
      "limit": {
        "context": 119000,
        "output": 119000
      },
      "metadata": {
        "description": "The Wenxin large model X1.1 has made significant improvements in question answering, tool invocation, intelligent agents, instruction following, logical reasoning, mathematics, and coding tasks, with notable enhancements in factual accuracy. The context length has been extended to 64K tokens, supporting longer inputs and dialogue history, which improves the coherence of long-chain reasoning while maintaining response speed.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/QwQ-32B",
      "name": "Qwen/QwQ-32B",
      "display_name": "Qwen/QwQ-32B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "cost": {
        "input": 0.14,
        "output": 0.56
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Silicon-based flow provision",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "pricing": {
          "input": 0.14,
          "output": 0.56
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
      "name": "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
      "display_name": "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Mistral's latest open-source small model; provided by chutes.ai.",
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ernie-x1.1-preview",
      "name": "ernie-x1.1-preview",
      "display_name": "ernie-x1.1-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.136,
        "output": 0.544
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Wenxin large model X1.1 has made significant improvements in question answering, tool invocation, intelligent agents, instruction following, logical reasoning, mathematics, and coding tasks, with notable enhancements in factual accuracy. The context length has been extended to 64K tokens, supporting longer inputs and dialogue history, which improves the coherence of long-chain reasoning while maintaining response speed.",
        "pricing": {
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "MiniMaxAI/MiniMax-M1-80k",
      "name": "MiniMaxAI/MiniMax-M1-80k",
      "display_name": "MiniMaxAI/MiniMax-M1-80k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "MiniMax-M1 is an open-source large-scale hybrid attention model with 456B total parameters (45.9B activated per token). It natively supports 1M-token context and reduces FLOPs by 75% versus DeepSeek R1 in 100K-token generation tasks via lightning attention. Built on MoE architecture and optimized by CISPO algorithm, it achieves state-of-the-art performance in long-context reasoning and real-world software engineering scenarios.",
        "pricing": {
          "input": 0.6,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-VL-32B-Instruct",
      "name": "Qwen/Qwen2.5-VL-32B-Instruct",
      "display_name": "Qwen/Qwen2.5-VL-32B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "video"
        ]
      },
      "cost": {
        "input": 0.24,
        "output": 0.24
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen2.5-VL-32B-Instruct is an advanced multimodal model from the Tongyi Qianwen team that can recognize objects, analyze text and graphics in images, operate tools, locate objects in images, and generate structured outputs. Through reinforcement learning, it has improved mathematics and problem-solving capabilities, with a more concise and natural response style.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image",
          "video"
        ],
        "pricing": {
          "input": 0.24,
          "output": 0.24
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "baidu/ERNIE-4.5-300B-A47B",
      "name": "baidu/ERNIE-4.5-300B-A47B",
      "display_name": "baidu/ERNIE-4.5-300B-A47B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.32,
        "output": 1.28,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "ERNIE-4.5-300B-A47B is a large language model developed by Baidu based on a Mixture of Experts (MoE) architecture. The model has a total of 300 billion parameters, but only activates 47 billion parameters per token during inference, which balances strong performance with computational efficiency. As one of the core models in the ERNIE 4.5 series, it demonstrates outstanding capabilities in tasks such as text understanding, generation, reasoning, and programming. The model employs an innovative multimodal heterogeneous MoE pretraining approach, leveraging joint training of textual and visual modalities to effectively enhance the model’s overall abilities, particularly excelling in instruction following and world knowledge memorization. Baidu has open-sourced this model along with other models in the series, aiming to promote the research and application of AI technology.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.32,
          "output": 1.28
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "bge-large-en",
      "name": "bge-large-en",
      "display_name": "bge-large-en",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "bge-large-en, open-sourced by the Beijing Academy of Artificial Intelligence (BAAI), is currently the most powerful vector representation model for Chinese tasks, with its semantic representation capabilities comprehensively surpassing those of similar open-source models.",
        "typeHints": [
          "embedding"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "bge-large-zh",
      "name": "bge-large-zh",
      "display_name": "bge-large-zh",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.068,
        "output": 0.068
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "bge-large-zh, open-sourced by the Beijing Academy of Artificial Intelligence (BAAI), is currently the most powerful vector representation model for Chinese tasks, with its semantic representation capabilities comprehensively surpassing those of similar open-source models.",
        "typeHints": [
          "embedding"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.068,
          "output": 0.068
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "codestral-latest",
      "name": "codestral-latest",
      "display_name": "codestral-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Mistral has launched a new code model - Codestral 25.01; https://mistral.ai/news/codestral-2501/",
        "pricing": {
          "input": 0.4,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ernie-4.5-0.3b",
      "name": "ernie-4.5-0.3b",
      "display_name": "ernie-4.5-0.3b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.0136,
        "output": 0.0544
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Wenxin Large Model 4.5 is a next-generation native multimodal foundational large model independently developed by Baidu. It achieves collaborative optimization through joint modeling of multiple modalities, demonstrating excellent multimodal understanding capabilities. The model possesses enhanced language abilities, with comprehensive improvements in understanding, generation, reasoning, and memory. It significantly reduces hallucinations and shows notable advancements in logical reasoning and coding skills.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.0136,
          "output": 0.0544
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-4.5-turbo-128k-preview",
      "name": "ernie-4.5-turbo-128k-preview",
      "display_name": "ernie-4.5-turbo-128k-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.108,
        "output": 0.432
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Wenxin 4.5 Turbo also shows significant enhancements in reducing hallucinations, logical reasoning, and coding capabilities. Compared to Wenxin 4.5, it is faster and more cost-effective.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.108,
          "output": 0.432
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ernie-x1-turbo",
      "name": "ernie-x1-turbo",
      "display_name": "ernie-x1-turbo",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.136,
        "output": 0.544
      },
      "limit": {
        "context": 50500,
        "output": 50500
      },
      "metadata": {
        "description": "Wenxin Large Model X1 possesses enhanced abilities in understanding, planning, reflection, and evolution. As a more comprehensive deep-thinking model, Wenxin X1 combines accuracy, creativity, and literary elegance, excelling particularly in Chinese knowledge Q&A, literary creation, document writing, daily conversations, logical reasoning, complex calculations, and tool invocation.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.136,
          "output": 0.544
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-exp-search",
      "name": "gemini-2.0-flash-exp-search",
      "display_name": "gemini-2.0-flash-exp-search",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The gemini-2.0-flash-exp model supports internet connectivity, but the official version requires additional request parameters to enable it. Aihubmix has integrated this by automatically calling the official API's online functionality when the model name is requested with the \"search\" parameter.",
        "typeHints": [
          "llm",
          "search"
        ],
        "features": [
          "web",
          "tools",
          "function_calling",
          "structured_outputs",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "kat-dev",
      "name": "kat-dev",
      "display_name": "kat-dev",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.137,
        "output": 0.548
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "KAT-Dev (32B) is an open-source 32B parameter model specifically designed for software engineering tasks. It achieved a 62.4% resolution rate on the SWE-Bench Verified benchmark, ranking fifth among all open-source models of various scales. The model is optimized through multiple stages, including intermediate training, supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT), as well as large-scale agent reinforcement learning (RL). Based on Qwen3-32B, its training process lays the foundation for subsequent fine-tuning and reinforcement learning stages by enhancing fundamental abilities such as tool usage, multi-turn interaction, and instruction following. During the fine-tuning phase, the model not only learns eight carefully curated task types and programming scenarios but also innovatively introduces a reinforcement fine-tuning (RFT) stage guided by human engineer-annotated “teacher trajectories.” The final agent reinforcement learning phase addresses scalability challenges through multi-level prefix caching, entropy-based trajectory pruning, and efficient architecture.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.137,
          "output": 0.548
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.3-70b",
      "name": "llama-3.3-70b",
      "display_name": "llama-3.3-70b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 65536,
        "output": 65536
      },
      "metadata": {
        "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.",
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshotai/Kimi-Dev-72B",
      "name": "moonshotai/Kimi-Dev-72B",
      "display_name": "moonshotai/Kimi-Dev-72B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.32,
        "output": 1.28,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Kimi-Dev-72B is a new generation open-source programming large model that achieved a leading performance of 60.4% on SWE-bench Verified. Through large-scale reinforcement learning optimization, it can automatically fix code in real Docker environments, receiving rewards only when passing the complete test suite, thereby ensuring the correctness and robustness of solutions and aligning more closely with real software development standards.",
        "pricing": {
          "cache_read": 0,
          "input": 0.32,
          "output": 1.28
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshotai/Moonlight-16B-A3B-Instruct",
      "name": "moonshotai/Moonlight-16B-A3B-Instruct",
      "display_name": "moonshotai/Moonlight-16B-A3B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o1-global",
      "name": "o1-global",
      "display_name": "o1-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 15,
        "output": 60,
        "cache_read": 7.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "OpenAI new model",
        "pricing": {
          "cache_read": 7.5,
          "input": 15,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qianfan-qi-vl",
      "name": "qianfan-qi-vl",
      "display_name": "qianfan-qi-vl",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qianfan-QI-VL model is a proprietary image quality inspection and visual understanding large model (Quality Inspection Large Vision Language Model, Qianfan-QI-VL) developed by Baidu Cloud’s Qianfan platform. It is designed for quality inspection of product images uploaded in e-commerce scenarios, with detection capabilities including AIGC human defect detection, mosaic recognition, watermark recognition, and trademark detection.",
        "pricing": {
          "input": 0.2,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-vl-72b-instruct",
      "name": "qwen2.5-vl-72b-instruct",
      "display_name": "qwen2.5-vl-72b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2.4,
        "output": 7.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Strong capability in Chinese domain recognition, comparable to ChatGPT-4.0.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2.4,
          "output": 7.2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "tencent/Hunyuan-A13B-Instruct",
      "name": "tencent/Hunyuan-A13B-Instruct",
      "display_name": "tencent/Hunyuan-A13B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.14,
        "output": 0.56
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Hunyuan-A13B-Instruct has 8 billion parameters and can match larger models by activating only 1.3 billion parameters, supporting \"fast thinking/slow thinking\" hybrid inference. It offers stable long text understanding. Verified by BFCL-v3 and τ-Bench, its Agent capabilities are leading in the field. Combined with GQA and multiple quantization formats, it enables efficient inference.",
        "pricing": {
          "input": 0.14,
          "output": 0.56
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "unsloth/gemma-3-27b-it",
      "name": "unsloth/gemma-3-27b-it",
      "display_name": "unsloth/gemma-3-27b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.22,
        "output": 0.22,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google's latest open-source model; provided by chutes.ai",
        "pricing": {
          "cache_read": 0,
          "input": 0.22,
          "output": 0.22
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-qwq-32b",
      "name": "qwen-qwq-32b",
      "display_name": "qwen-qwq-32b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "unsloth/gemma-3-12b-it",
      "name": "unsloth/gemma-3-12b-it",
      "display_name": "unsloth/gemma-3-12b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.8,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Provided by chutes.ai.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-exp-1206",
      "name": "gemini-exp-1206",
      "display_name": "gemini-exp-1206",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google's latest experimental model, currently Google's most powerful model.",
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-zh",
      "name": "gpt-4o-zh",
      "display_name": "gpt-4o-zh",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen-max-0125",
      "name": "qwen-max-0125",
      "display_name": "qwen-max-0125",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.38,
        "output": 1.52
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Qwen 2.5-Max latest model",
        "pricing": {
          "input": 0.38,
          "output": 1.52
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-3-5-haiku",
      "name": "claude-3-5-haiku",
      "display_name": "claude-3-5-haiku",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 5.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Claude 3.5 Haiku is the next generation of Claude's fastest model.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 1.1,
          "output": 5.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "BAAI/bge-large-en-v1.5",
      "name": "BAAI/bge-large-en-v1.5",
      "display_name": "BAAI/bge-large-en-v1.5",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.034,
        "output": 0.034
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "BAAI/bge-large-en-v1.5 is a large English text embedding model and part of the BGE (BAAI General Embedding) series. It achieves excellent performance on the MTEB benchmark, with an average score of 64.23 across 56 datasets, excelling in tasks such as retrieval, clustering, and text pair classification. The model supports a maximum input length of 512 tokens and is suitable for various natural language processing tasks, such as text retrieval and semantic similarity computation.",
        "typeHints": [
          "embedding"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.034,
          "output": 0.034
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "BAAI/bge-large-zh-v1.5",
      "name": "BAAI/bge-large-zh-v1.5",
      "display_name": "BAAI/bge-large-zh-v1.5",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.034,
        "output": 0.034
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "BAAI/bge-large-zh-v1.5 is a large Chinese text embedding model and part of the BGE (BAAI General Embedding) series. It performs excellently on the C-MTEB benchmark, achieving an average score of 64.53 across 31 datasets, with outstanding results in tasks such as retrieval, semantic similarity, and text pair classification. The model supports a maximum input length of 512 tokens and is suitable for various Chinese natural language processing tasks, such as text retrieval and semantic similarity computation.",
        "typeHints": [
          "embedding"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.034,
          "output": 0.034
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "BAAI/bge-reranker-v2-m3",
      "name": "BAAI/bge-reranker-v2-m3",
      "display_name": "BAAI/bge-reranker-v2-m3",
      "type": "rerank",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.034,
        "output": 0.034
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "BAAI/bge-reranker-v2-m3 is a lightweight multilingual reranking model. It is developed based on the bge-m3 model, offering strong multilingual capabilities, easy deployment, and fast inference. The model takes a query and documents as input and directly outputs similarity scores instead of embedding vectors. It is suitable for multilingual scenarios and performs particularly well in both Chinese and English processing.",
        "typeHints": [
          "rerank"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.034,
          "output": 0.034
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "tencent/Hunyuan-MT-7B",
      "name": "tencent/Hunyuan-MT-7B",
      "display_name": "tencent/Hunyuan-MT-7B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Hunyuan-MT-7B is a lightweight translation model with 7 billion parameters, designed to translate source text into target languages. The model supports translation among 33 languages as well as 5 Chinese minority languages. In the WMT25 International Machine Translation Competition, Hunyuan-MT-7B achieved first place in 30 out of 31 language categories it participated in, demonstrating its exceptional translation capabilities. For translation scenarios, Tencent Hunyuan proposed a complete training paradigm from pre-training to supervised fine-tuning, followed by translation reinforcement and ensemble reinforcement, enabling it to achieve industry-leading performance among models of similar scale. The model is computationally efficient, easy to deploy, and suitable for various application scenarios.",
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-lite-preview-02-05",
      "name": "gemini-2.0-flash-lite-preview-02-05",
      "display_name": "gemini-2.0-flash-lite-preview-02-05",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.075,
        "output": 0.3,
        "cache_read": 0.075
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemini 2.0 Flash lightweight version",
        "pricing": {
          "cache_read": 0.075,
          "input": 0.075,
          "output": 0.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "V3",
      "name": "V3",
      "display_name": "V3",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Fast and high-quality — top image quality in just 11 seconds per piece, with almost no extra time for batch generation.\nFlexible ratios — supports ultra-wide and tall formats like 3:1, 2:1, offering diverse perspectives.\nUnique strengths — outstanding design capabilities in the V3 and V2 series, with powerful text rendering (Chinese support coming soon).\nPrecise local editing — fine-tuned mask control for area redrawing (edit) and easy background replacement (replace-background).",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "sonar-reasoning",
      "name": "sonar-reasoning",
      "display_name": "sonar-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.6,
        "output": 8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Perplexity inference model",
        "pricing": {
          "input": 1.6,
          "output": 8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "V_2",
      "name": "V_2",
      "display_name": "V_2",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Ideogram AI drawing interface is now live. This model boasts powerful text-to-image capabilities, supporting endpoints are: /generate, /remix, /edit.\nThis model is the stable V_2 version, highly recommended for editing.\nUS $0.08/ 1 IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "V_2_TURBO",
      "name": "V_2_TURBO",
      "display_name": "V_2_TURBO",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Ideogram AI drawing interface is now live. This model boasts powerful text-to-image capabilities, supporting endpoints are: /generate, /remix, /edit.\nThis model is the fast version of V_2, offering increased speed at the slight expense of quality.\nUS $0.05/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "V_2A",
      "name": "V_2A",
      "display_name": "V_2A",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Ideogram AI drawing interface is now live. This model boasts powerful text-to-image capabilities, supporting endpoints are: /generate, /remix.\nThis model is the fast version of V_2, faster and cheaper.\nUS $0.04/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "V_2A_TURBO",
      "name": "V_2A_TURBO",
      "display_name": "V_2A_TURBO",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Ideogram AI drawing interface is now live. This model boasts powerful text-to-image capabilities, supporting endpoints are: /generate, /remix.\nThis model is the ultra-fast version of V_2, delivering the highest speed while slightly reducing quality.\nUS $0.025/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "V_1",
      "name": "V_1",
      "display_name": "V_1",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "V_1 is a text-to-image model in the Ideogram series. It delivers strong text rendering capabilities, high photorealistic image quality, and precise prompt adherence. The model also introduces Magic Prompt, a new feature that automatically refines input prompts to generate more detailed and creative visuals.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "V_1_TURBO",
      "name": "V_1_TURBO",
      "display_name": "V_1_TURBO",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Ideogram AI drawing interface is now live. This model boasts powerful text-to-image capabilities, supporting endpoints are: /generate, /remix.\nThis model is the ultra-fast version of the original V_1, offering increased speed at the slight expense of quality.\nUS $0.02/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-embedding-large-text-240915",
      "name": "doubao-embedding-large-text-240915",
      "display_name": "doubao-embedding-large-text-240915",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "doubao-embedding-large-text-240915\nDoubao Embedding is a semantic vectorization model developed by ByteDance, primarily designed for vector search scenarios. It supports both Chinese and English languages and has a maximum context length of approximately 4K tokens.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "google/gemma-3-27b-it",
      "name": "google/gemma-3-27b-it",
      "display_name": "google/gemma-3-27b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. This model is ready for commercial use.",
        "pricing": {
          "cache_read": 0,
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "kimi-thinking-preview",
      "name": "kimi-thinking-preview",
      "display_name": "kimi-thinking-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 30,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The latest kimi model.",
        "pricing": {
          "input": 30,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-2024-08-06",
      "name": "gpt-4o-2024-08-06",
      "display_name": "gpt-4o-2024-08-06",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 1.25
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Supports caching, with automatic halving of charges upon a cache hit.",
        "pricing": {
          "cache_read": 1.25,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-plus-2025-07-28",
      "name": "qwen-plus-2025-07-28",
      "display_name": "qwen-plus-2025-07-28",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.11,
        "output": 0.275,
        "cache_read": 0.11
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Tongyi Qianwen series balanced capability model has inference performance and speed between Tongyi Qianwen-Max and Tongyi Qianwen-Turbo, making it suitable for moderately complex tasks. This model adopts tiered pricing.",
        "pricing": {
          "cache_read": 0.11,
          "input": 0.11,
          "output": 0.275
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "AiHubmix-Phi-4-reasoning",
      "name": "AiHubmix-Phi-4-reasoning",
      "display_name": "AiHubmix-Phi-4-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Phi-4-Reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. The supervised fine-tuning dataset includes a blend of synthetic prompts and high-quality filtered data from public domain websites, focused on math, science, and coding skills as well as alignment data for safety and Responsible AI. The goal of this approach was to ensure that small capable models were trained with data focused on high quality and advanced reasoning.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-plus-latest",
      "name": "qwen-plus-latest",
      "display_name": "qwen-plus-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.11,
        "output": 0.275,
        "cache_read": 0.11
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen series models with balanced capabilities have inference performance and speed between Qwen-Max and Qwen-Turbo, making them suitable for moderately complex tasks. This model is a dynamically updated version, and updates will not be announced in advance. The current version is qwen-plus-2025-04-28.The model adopts tiered pricing.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0.11,
          "input": 0.11,
          "output": 0.275
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "sonar",
      "name": "sonar",
      "display_name": "sonar",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.6,
        "output": 1.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Latest Perplexity Model",
        "pricing": {
          "input": 1.6,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "stepfun-ai/step3",
      "name": "stepfun-ai/step3",
      "display_name": "stepfun-ai/step3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.1,
        "output": 2.75
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Step3 is a multimodal reasoning model released by StepFun. It uses a Mixture‑of‑Experts (MoE) architecture with 321 billion total parameters and 38 billion activation parameters. The model follows an end‑to‑end design that reduces decoding cost while delivering top‑tier performance on vision‑language reasoning tasks. Thanks to the combined use of Multi‑Head Factorized Attention (MFA) and Attention‑FFN Decoupling (AFD), Step3 remains highly efficient on both flagship and low‑end accelerators. During pre‑training, it processed over 20 trillion text tokens and 4 trillion image‑text mixed tokens, covering more than ten languages. On benchmarks for mathematics, code, and multimodal tasks, Step3 consistently outperforms other open‑source models.",
        "pricing": {
          "input": 1.1,
          "output": 2.75
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-v4",
      "name": "text-embedding-v4",
      "display_name": "text-embedding-v4",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.08,
        "output": 0.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This is the Tongyi Laboratory's multilingual unified text vector model trained based on Qwen3, which significantly improves performance in text retrieval, clustering, and classification compared to version V3; it achieves a 15% to 40% improvement on evaluation tasks such as MTEB multilingual, Chinese-English, and code retrieval; supports user-defined vector dimensions ranging from 64 to 2048.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.08,
          "output": 0.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-turbo-latest",
      "name": "qwen-turbo-latest",
      "display_name": "qwen-turbo-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.046,
        "output": 0.92,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen series model with the fastest speed and lowest cost, suitable for simple tasks. This model is a dynamically updated version, and updates will not be announced in advance. The model's overall Chinese and English abilities have been significantly improved, human preference alignment has been greatly enhanced, inference capability and complex instruction understanding have been substantially strengthened, performance on difficult tasks is better, and mathematics and coding skills have been significantly improved. The current version is qwen-turbo-2025-04-28.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.046,
          "output": 0.92
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "AiHubmix-Phi-4-mini-reasoning",
      "name": "AiHubmix-Phi-4-mini-reasoning",
      "display_name": "AiHubmix-Phi-4-mini-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.12,
        "output": 0.12
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Phi-4-mini-reasoning is a lightweight open model designed for advanced mathematical reasoning and logic-intensive problem-solving. It is particularly well-suited for tasks such as formal proofs, symbolic computation, and solving multi-step word problems. With its efficient architecture, the model balances high-quality reasoning performance with cost-effective deployment, making it ideal for educational applications, embedded tutoring, and lightweight edge or mobile systems.\n\nPhi-4-mini-reasoning supports a 128K token context length, enabling it to process and reason over long mathematical problems and proofs. Built on synthetic and high-quality math datasets, the model leverages advanced fine-tuning techniques such as supervised fine-tuning and preference modeling to enhance reasoning capabilities. Its training incorporates safety and alignment protocols, ensuring robust and reliable performance across supported use cases.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.12,
          "output": 0.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihub-Phi-4-multimodal-instruct",
      "name": "aihub-Phi-4-multimodal-instruct",
      "display_name": "aihub-Phi-4-multimodal-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio"
        ]
      },
      "cost": {
        "input": 0.12,
        "output": 0.48
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Microsoft's latest model",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio"
        ],
        "pricing": {
          "input": 0.12,
          "output": 0.48
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "qwen3-30b-a3b",
      "name": "qwen3-30b-a3b",
      "display_name": "qwen3-30b-a3b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.12,
        "output": 1.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Achieves effective integration of thinking and non-thinking modes, allowing mode switching during conversations. Its reasoning ability matches that of QwQ-32B with a smaller parameter size, and its general capability significantly surpasses Qwen2.5-14B, reaching state-of-the-art (SOTA) levels among industry models of the same scale.",
        "pricing": {
          "cache_read": 0,
          "input": 0.12,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-32b",
      "name": "qwen3-32b",
      "display_name": "qwen3-32b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.32,
        "output": 3.2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Achieves effective integration of thinking and non-thinking modes, allowing mode switching during conversations. Its reasoning ability significantly surpasses QwQ, and its general capability significantly exceeds Qwen2.5-32B-Instruct, reaching state-of-the-art (SOTA) levels among industry models of the same scale.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.32,
          "output": 3.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3",
      "name": "grok-3",
      "display_name": "grok-3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Grok's latest model",
        "pricing": {
          "input": 3,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihub-Phi-4-mini-instruct",
      "name": "aihub-Phi-4-mini-instruct",
      "display_name": "aihub-Phi-4-mini-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.12,
        "output": 0.48
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "Microsoft's latest model",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.12,
          "output": 0.48
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihub-Phi-4",
      "name": "aihub-Phi-4",
      "display_name": "aihub-Phi-4",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.12,
        "output": 0.48
      },
      "limit": {
        "context": 16400,
        "output": 16400
      },
      "metadata": {
        "description": "Phi-4 is a state-of-the-art open model based on a combination of synthetic datasets, curated public domain website data, and acquired academic books and QA datasets. The approach aims to ensure that small, efficient models are trained using data focused on high quality and advanced reasoning.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.12,
          "output": 0.48
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-3-opus-20240229",
      "name": "claude-3-opus-20240229",
      "display_name": "claude-3-opus-20240229",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 16.5,
        "output": 82.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Claude’s previous generation strongest model",
        "pricing": {
          "input": 16.5,
          "output": 82.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "dall-e-3",
      "name": "dall-e-3",
      "display_name": "dall-e-3",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 40,
        "output": 40
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "dall-e-3 is an AI image generation model that converts natural language prompts into realistic visuals and artistic content. It delivers accurate semantic understanding, supports customizable output resolutions, and produces high-quality images across a wide range of styles, making it well-suited for concept design, creative prototyping, and professional content workflows.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 40,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "doubao-embedding-text-240715",
      "name": "doubao-embedding-text-240715",
      "display_name": "doubao-embedding-text-240715",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.7,
        "output": 0.7
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "doubao-embedding-text-240715\nDoubao Embedding is a semantic vectorization model developed by ByteDance, primarily designed for vector search scenarios. It supports both Chinese and English languages and has a maximum context length of approximately 4K tokens.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.7,
          "output": 0.7
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-14b",
      "name": "qwen3-14b",
      "display_name": "qwen3-14b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 1.6,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Achieves effective integration of thinking and non-thinking modes, enabling mode switching during conversations. Its reasoning ability reaches state-of-the-art (SOTA) levels among models of the same scale, and its general capability significantly surpasses Qwen2.5-14B.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.16,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-beta",
      "name": "grok-3-beta",
      "display_name": "grok-3-beta",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 15,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Grok's latest model\nThis model ID with beta has been officially taken offline. Using this model grok-3-beta will automatically point to grok-3.",
        "pricing": {
          "cache_read": 0,
          "input": 3,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-fast",
      "name": "grok-3-fast",
      "display_name": "grok-3-fast",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5.5,
        "output": 27.5,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 5.5,
          "output": 27.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-8b",
      "name": "qwen3-8b",
      "display_name": "qwen3-8b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.08,
        "output": 0.8,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Achieves effective integration of thinking and non-thinking modes, enabling mode switching during conversations. Its reasoning ability reaches state-of-the-art (SOTA) levels among models of the same scale, and its general capability significantly surpasses Qwen2.5-7B.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.08,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-4b",
      "name": "qwen3-4b",
      "display_name": "qwen3-4b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.046,
        "output": 0.46,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Achieves effective integration of thinking and non-thinking modes, allowing mode switching during conversations. Its reasoning ability reaches state-of-the-art (SOTA) levels among models of the same scale, with significantly enhanced human preference alignment. There are notable improvements in creative writing, role-playing, multi-turn dialogue, and instruction following, resulting in a noticeably better user experience.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.046,
          "output": 0.46
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Zero",
      "name": "deepseek-ai/DeepSeek-R1-Zero",
      "display_name": "deepseek-ai/DeepSeek-R1-Zero",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.2,
        "output": 2.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Openly deployed by chutes.ai; inference with FP8; zero is the initial preliminary version of R1 without optimizations and is not recommended for use unless for research purposes.",
        "pricing": {
          "input": 2.2,
          "output": 2.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-fast-beta",
      "name": "grok-3-fast-beta",
      "display_name": "grok-3-fast-beta",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5.5,
        "output": 27.5,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 5.5,
          "output": 27.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-mini",
      "name": "grok-3-mini",
      "display_name": "grok-3-mini",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.501,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 0.3,
          "output": 0.501
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-mini-beta",
      "name": "grok-3-mini-beta",
      "display_name": "grok-3-mini-beta",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.33,
        "output": 0.5511,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This model ID with beta has been officially taken offline. Using this model grok-3-mini-beta will automatically point to grok-3-mini.",
        "pricing": {
          "cache_read": 0,
          "input": 0.33,
          "output": 0.5511
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-1.7b",
      "name": "qwen3-1.7b",
      "display_name": "qwen3-1.7b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.046,
        "output": 0.46,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Effectively integrates thinking and non-thinking modes, allowing mode switching during conversations. Its general capabilities significantly surpass those of the Qwen2.5 small-scale series, with greatly enhanced human preference alignment. There are notable improvements in creative writing, role-playing, multi-turn dialogue, and instruction following, resulting in a significantly better expected user experience.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.046,
          "output": 0.46
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen3-0.6b",
      "name": "qwen3-0.6b",
      "display_name": "qwen3-0.6b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.046,
        "output": 0.46,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Effectively integrates thinking and non-thinking modes, allowing mode switching during conversations. Its general capabilities significantly surpass those of the Qwen2.5 small-scale series.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.046,
          "output": 0.46
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-turbo-2025-04-28",
      "name": "qwen-turbo-2025-04-28",
      "display_name": "qwen-turbo-2025-04-28",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.046,
        "output": 0.92,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 series Turbo model effectively integrates thinking and non-thinking modes, allowing seamless switching between modes during conversations. With a smaller parameter size, its reasoning ability rivals that of QwQ-32B, and its general capabilities significantly surpass those of Qwen2.5-Turbo, reaching state-of-the-art (SOTA) levels among models of the same scale. This version is a snapshot model as of April 28, 2025.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.046,
          "output": 0.92
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-3-mini-fast-beta",
      "name": "grok-3-mini-fast-beta",
      "display_name": "grok-3-mini-fast-beta",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.33,
        "output": 2.20011
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.33,
          "output": 2.20011
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-3-32b",
      "name": "qwen-3-32b",
      "display_name": "qwen-3-32b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "cerebras",
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-a-03-2025",
      "name": "command-a-03-2025",
      "display_name": "command-a-03-2025",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Command A is Cohere most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-plus-2025-04-28",
      "name": "qwen-plus-2025-04-28",
      "display_name": "qwen-plus-2025-04-28",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.13,
        "output": 2.6,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 series Plus model effectively integrates thinking and non-thinking modes, allowing for mode switching during conversations. Its reasoning abilities significantly surpass those of QwQ, and its general capabilities are markedly superior to Qwen2.5-Plus, reaching state-of-the-art (SOTA) levels among models of the same scale. This version is a snapshot model as of April 28, 2025.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.13,
          "output": 2.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "THUDM/GLM-Z1-32B-0414",
      "name": "THUDM/GLM-Z1-32B-0414",
      "display_name": "THUDM/GLM-Z1-32B-0414",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.08,
        "output": 0.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-Z1-32B-0414 is a reasoning-focused AI model built on GLM-4-32B-0414. It has been enhanced through cold-start methods and reinforcement learning, with a strong emphasis on math, coding, and logic tasks. Despite having only 32B parameters, it performs comparably to the 671B DeepSeek-R1 on some benchmarks. It excels in complex reasoning tasks, as shown in evaluations like AIME 24/25, LiveCodeBench, and GPQA.",
        "pricing": {
          "input": 0.08,
          "output": 0.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Pro/THUDM/GLM-4.1V-9B-Thinking",
      "name": "Pro/THUDM/GLM-4.1V-9B-Thinking",
      "display_name": "Pro/THUDM/GLM-4.1V-9B-Thinking",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.04,
        "output": 0.16,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4.1V-9B-Thinking is an open-source Vision Language Model (VLM) jointly released by Zhipu AI and the KEG Laboratory at Tsinghua University, designed specifically for handling complex multimodal cognitive tasks. Based on the GLM-4-9B-0414 foundation model, it significantly enhances cross-modal reasoning ability and stability by introducing the “Chain-of-Thought” reasoning mechanism and using reinforcement learning strategies. As a lightweight model with 9 billion parameters, it strikes a balance between deployment efficiency and performance. In 28 authoritative benchmark evaluations, it matched or even outperformed the 72-billion-parameter Qwen-2.5-VL-72B model in 18 tasks. The model excels not only in image-text understanding, mathematical and scientific reasoning, and video understanding, but also supports images up to 4K resolution and inputs of arbitrary aspect ratios.",
        "pricing": {
          "cache_read": 0,
          "input": 0.04,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "THUDM/GLM-4.1V-9B-Thinking",
      "name": "THUDM/GLM-4.1V-9B-Thinking",
      "display_name": "THUDM/GLM-4.1V-9B-Thinking",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4.1V-9B-Thinking is an open-source Vision Language Model (VLM) jointly released by Zhipu AI and the KEG Laboratory at Tsinghua University, designed specifically for handling complex multimodal cognitive tasks. Based on the GLM-4-9B-0414 foundation model, it significantly enhances cross-modal reasoning ability and stability by introducing the “Chain-of-Thought” reasoning mechanism and using reinforcement learning strategies. As a lightweight model with 9 billion parameters, it strikes a balance between deployment efficiency and performance. In 28 authoritative benchmark evaluations, it matched or even outperformed the 72-billion-parameter Qwen-2.5-VL-72B model in 18 tasks. The model excels not only in image-text understanding, mathematical and scientific reasoning, and video understanding, but also supports images up to 4K resolution and inputs of arbitrary aspect ratios.",
        "pricing": {
          "cache_read": 0,
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-004",
      "name": "text-embedding-004",
      "display_name": "text-embedding-004",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "THUDM/GLM-4-32B-0414",
      "name": "THUDM/GLM-4-32B-0414",
      "display_name": "THUDM/GLM-4-32B-0414",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.08,
        "output": 0.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4-32B-0414 is a next-generation open-source model with 32 billion parameters, delivering performance comparable to OpenAI’s GPT series and DeepSeek V3/R1. It supports smooth local deployment.\n\nThe base model was pre-trained on 15T of high-quality data, including a large amount of reasoning-focused synthetic content, setting the stage for advanced reinforcement learning.\n\nIn the post-training phase, techniques like human preference alignment, rejection sampling, and reinforcement learning were used to improve the model’s ability to follow instructions, generate code, and handle function calls—core skills needed for agent-style tasks.\n\nGLM-4-32B-0414 has shown strong results in engineering code, artifact generation, function calling, search-based QA, and report writing—sometimes matching or even surpassing larger models like GPT-4o and DeepSeek-V3 (671B) on specific benchmarks.",
        "pricing": {
          "input": 0.08,
          "output": 0.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "THUDM/GLM-Z1-9B-0414",
      "name": "THUDM/GLM-Z1-9B-0414",
      "display_name": "THUDM/GLM-Z1-9B-0414",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-Z1-9B-0414 is a small but powerful model in the GLM series, with only 9 billion parameters. Despite its size, it delivers strong performance in math reasoning and general tasks, ranking among the best in its class of open-source models.\n\nTrained with the same techniques as larger models, it strikes an excellent balance between performance and efficiency—making it a great option for low-resource or lightweight deployment scenarios.",
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "THUDM/GLM-4-9B-0414",
      "name": "THUDM/GLM-4-9B-0414",
      "display_name": "THUDM/GLM-4-9B-0414",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4-9B-0414 is a lightweight model in the GLM family, with 9 billion parameters. It inherits the core tech from GLM-4-32B and offers an efficient option for deployment on limited resources.\n\nDespite its smaller size, it performs well in tasks like code generation, web design, SVG graphics creation, and search-based writing. It also supports function calling to interact with external tools, enhancing its versatility.\n\nGLM-4-9B-0414 strikes a solid balance between efficiency and performance, making it a strong choice for low-resource environments—while remaining competitive on various benchmarks.",
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-doubao-seed-code-preview-latest",
      "name": "cc-doubao-seed-code-preview-latest",
      "display_name": "cc-doubao-seed-code-preview-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "claude code",
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-seed-code-preview-latest",
      "name": "doubao-seed-code-preview-latest",
      "display_name": "doubao-seed-code-preview-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "chat",
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/Janus-Pro-7B",
      "name": "deepseek-ai/Janus-Pro-7B",
      "display_name": "deepseek-ai/Janus-Pro-7B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-zero-preview",
      "name": "glm-zero-preview",
      "display_name": "glm-zero-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Simply put, it is the intelligent enhanced version of O1.",
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-3-235b-a22b-instruct-2507",
      "name": "qwen-3-235b-a22b-instruct-2507",
      "display_name": "qwen-3-235b-a22b-instruct-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.28,
        "output": 1.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "cerebras",
        "pricing": {
          "input": 0.28,
          "output": 1.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-thinking-exp-1219",
      "name": "gemini-2.0-flash-thinking-exp-1219",
      "display_name": "gemini-2.0-flash-thinking-exp-1219",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.076,
        "output": 0.304
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Gemini 2.0 Flash Thinking mode is an experimental model designed to generate the \"thinking process\" that the model undergoes during its response. Therefore, the Gemini 2.0 Flash Thinking mode possesses stronger reasoning capabilities in its responses compared to the base Gemini 2.0 Flash model.",
        "pricing": {
          "input": 0.076,
          "output": 0.304
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
      "name": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
      "display_name": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 0.5,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Llama-3.1-Nemotron-Ultra-253B is a 253 billion parameter reasoning-focused language model optimized for efficiency that excels at math, coding, and general instruction-following tasks while running on a single 8xH100 node.",
        "pricing": {
          "cache_read": 0,
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4.5-air",
      "name": "glm-4.5-air",
      "display_name": "glm-4.5-air",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.14,
        "output": 0.84
      },
      "limit": {
        "context": 131072,
        "output": 131072
      },
      "metadata": {
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.14,
          "output": 0.84
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-32k",
      "name": "gpt-4-32k",
      "display_name": "gpt-4-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 60,
        "output": 120
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The smartest version of GPT-4; OpenAI no longer offers it officially. All the 32k versions on this site are provided by Microsoft, deployed on Azure OpenAI by the official Microsoft service.",
        "pricing": {
          "input": 60,
          "output": 120
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o1-preview-2024-09-12",
      "name": "o1-preview-2024-09-12",
      "display_name": "o1-preview-2024-09-12",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 15,
        "output": 60,
        "cache_read": 7.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 7.5,
          "input": 15,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "coding-glm-4.5-air",
      "name": "coding-glm-4.5-air",
      "display_name": "coding-glm-4.5-air",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.014,
        "output": 0.084
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.014,
          "output": 0.084
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/QVQ-72B-Preview",
      "name": "Qwen/QVQ-72B-Preview",
      "display_name": "Qwen/QVQ-72B-Preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.2,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.2,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/QwQ-32B-Preview",
      "name": "Qwen/QwQ-32B-Preview",
      "display_name": "Qwen/QwQ-32B-Preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-sonar-huge-128k-online",
      "name": "llama-3.1-sonar-huge-128k-online",
      "display_name": "llama-3.1-sonar-huge-128k-online",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5.6,
        "output": 5.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "On February 22, 2025, this model will be officially discontinued. The Perplexity AI official fine-tuned LLMA internet-connected interface is currently only supported at the api.aihubmix.com address.",
        "pricing": {
          "input": 5.6,
          "output": 5.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-sonar-large-128k-online",
      "name": "llama-3.1-sonar-large-128k-online",
      "display_name": "llama-3.1-sonar-large-128k-online",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.2,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "On February 22, 2025, this model will be officially discontinued; Perplexity AI's official fine-tuned LLMA internet-connected interface is currently only supported at the api.aihubmix.com address.",
        "pricing": {
          "input": 1.2,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Mistral-Large-2411",
      "name": "aihubmix-Mistral-Large-2411",
      "display_name": "aihubmix-Mistral-Large-2411",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The latest Mistral Large 2 model is deployed on Azure.",
        "pricing": {
          "input": 2,
          "output": 6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Mistral-large-2407",
      "name": "aihubmix-Mistral-large-2407",
      "display_name": "aihubmix-Mistral-large-2407",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-2-1212",
      "name": "grok-2-1212",
      "display_name": "grok-2-1212",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.8,
        "output": 9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.8,
          "output": 9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-thinking-exp",
      "name": "gemini-2.0-flash-thinking-exp",
      "display_name": "gemini-2.0-flash-thinking-exp",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.076,
        "output": 0.304
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.076,
          "output": 0.304
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4.5-flash",
      "name": "glm-4.5-flash",
      "display_name": "glm-4.5-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0,
        "output": 0,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0,
          "output": 0
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-image-test",
      "name": "gpt-image-test",
      "display_name": "gpt-image-test",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5,
        "output": 40,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 5,
          "output": 40
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "imagen-3.0-generate-002",
      "name": "imagen-3.0-generate-002",
      "display_name": "imagen-3.0-generate-002",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Imagen 3.0 is Google's latest text-to-image generation model, capable of creating high-quality images from natural language prompts. Compared to its predecessors, Imagen 3.0 offers significant improvements in detail, lighting, and reduced visual artifacts. It supports rendering in various artistic styles, from photorealism to impressionism, as well as abstract and anime styles.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "llama3.1-8b",
      "name": "llama3.1-8b",
      "display_name": "llama3.1-8b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "cerebras",
        "pricing": {
          "input": 0.3,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o1-2024-12-17",
      "name": "o1-2024-12-17",
      "display_name": "o1-2024-12-17",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 15,
        "output": 60,
        "cache_read": 7.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 7.5,
          "input": 15,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "sf-kimi-k2-thinking",
      "name": "sf-kimi-k2-thinking",
      "display_name": "sf-kimi-k2-thinking",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.548,
        "output": 2.192
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DESCRIBE",
      "name": "DESCRIBE",
      "display_name": "DESCRIBE",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "This endpoint is used to describe an image.\nSupported image formats include JPEG, PNG, and WebP.\nUS $0.01/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "UPSCALE",
      "name": "UPSCALE",
      "display_name": "UPSCALE",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The super-resolution upscale interface of the Ideogram AI drawing model is designed to enlarge low-resolution images into high-resolution ones, redrawing details (with controllable similarity and detail proportions).\nUS $0.06/ IMG.\nFor usage examples and pricing details, refer to the documentation at https://docs.aihubmix.com/cn/api/IdeogramAI.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "bai-qwen3-vl-235b-a22b-instruct",
      "name": "bai-qwen3-vl-235b-a22b-instruct",
      "display_name": "bai-qwen3-vl-235b-a22b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.274,
        "output": 1.096
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The Qwen3 series open-source models include hybrid models, thinking models, and non-thinking models, with both reasoning capabilities and general abilities reaching industry SOTA levels at the same scale.",
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "input": 0.274,
          "output": 1.096
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-MiniMax-M2",
      "name": "cc-MiniMax-M2",
      "display_name": "cc-MiniMax-M2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude Code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-deepseek-v3",
      "name": "cc-deepseek-v3",
      "display_name": "cc-deepseek-v3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "pricing": {
          "input": 0.3,
          "output": 0.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-deepseek-v3.1",
      "name": "cc-deepseek-v3.1",
      "display_name": "cc-deepseek-v3.1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.56,
        "output": 1.68
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.56,
          "output": 1.68
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-ernie-4.5-300b-a47b",
      "name": "cc-ernie-4.5-300b-a47b",
      "display_name": "cc-ernie-4.5-300b-a47b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.32,
        "output": 1.28,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 0.32,
          "output": 1.28
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "cc-kimi-dev-72b",
      "name": "cc-kimi-dev-72b",
      "display_name": "cc-kimi-dev-72b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.32,
        "output": 1.28,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "pricing": {
          "cache_read": 0,
          "input": 0.32,
          "output": 1.28
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-kimi-k2-instruct",
      "name": "cc-kimi-k2-instruct",
      "display_name": "cc-kimi-k2-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 3.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 1.1,
          "output": 3.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-kimi-k2-instruct-0905",
      "name": "cc-kimi-k2-instruct-0905",
      "display_name": "cc-kimi-k2-instruct-0905",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 3.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "For Claude code only",
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 1.1,
          "output": 3.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cc-kimi-k2-thinking",
      "name": "cc-kimi-k2-thinking",
      "display_name": "cc-kimi-k2-thinking",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.548,
        "output": 2.192
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Dedicated for Claude Code",
        "pricing": {
          "input": 0.548,
          "output": 2.192
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "computer-use-preview",
      "name": "computer-use-preview",
      "display_name": "computer-use-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Baichuan3-Turbo",
      "name": "Baichuan3-Turbo",
      "display_name": "Baichuan3-Turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.9,
        "output": 1.9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.9,
          "output": 1.9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Baichuan3-Turbo-128k",
      "name": "Baichuan3-Turbo-128k",
      "display_name": "Baichuan3-Turbo-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3.8,
        "output": 3.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3.8,
          "output": 3.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Baichuan4",
      "name": "Baichuan4",
      "display_name": "Baichuan4",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 16,
        "output": 16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 16,
          "output": 16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Baichuan4-Air",
      "name": "Baichuan4-Air",
      "display_name": "Baichuan4-Air",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Baichuan4-Turbo",
      "name": "Baichuan4-Turbo",
      "display_name": "Baichuan4-Turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.4,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2.4,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "DeepSeek-v3",
      "name": "DeepSeek-v3",
      "display_name": "DeepSeek-v3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.272,
        "output": 1.088
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.272,
          "output": 1.088
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-1.5-lite-32k",
      "name": "Doubao-1.5-lite-32k",
      "display_name": "Doubao-1.5-lite-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.05,
        "output": 0.1,
        "cache_read": 0.01
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-1.5-lite, a brand-new generation of lightweight model, offers exceptional response speed with both performance and latency reaching world-class levels. It supports a 32k context window and an output length of up to 12k tokens.",
        "pricing": {
          "cache_read": 0.01,
          "input": 0.05,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-1.5-pro-256k",
      "name": "Doubao-1.5-pro-256k",
      "display_name": "Doubao-1.5-pro-256k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 1.44,
        "cache_read": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-1.5-pro-256k, a fully upgraded version based on Doubao-1.5-Pro, delivers an overall performance improvement of 10%. It supports inference with a 256k context window and an output length of up to 12k tokens. With higher performance, larger window size, and exceptional cost-effectiveness, it is suitable for a wider range of application scenarios.",
        "pricing": {
          "cache_read": 0.8,
          "input": 0.8,
          "output": 1.44
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-1.5-pro-32k",
      "name": "Doubao-1.5-pro-32k",
      "display_name": "Doubao-1.5-pro-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.134,
        "output": 0.335,
        "cache_read": 0.0268
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-1.5-pro, a brand-new generation of flagship model, features comprehensive performance upgrades and excels in knowledge, coding, reasoning, and other aspects. It supports a 32k context window and an output length of up to 12k tokens.",
        "pricing": {
          "cache_read": 0.0268,
          "input": 0.134,
          "output": 0.335
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-1.5-vision-pro-32k",
      "name": "Doubao-1.5-vision-pro-32k",
      "display_name": "Doubao-1.5-vision-pro-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.46,
        "output": 1.38
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Doubao-1.5-vision-pro is a newly upgraded multimodal large model that supports image recognition at any resolution and extreme aspect ratios. It enhances visual reasoning, document recognition, detailed information understanding, and instruction-following capabilities. It supports a 32k context window and an output length of up to 12k tokens.",
        "pricing": {
          "input": 0.46,
          "output": 1.38
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-lite-128k",
      "name": "Doubao-lite-128k",
      "display_name": "Doubao-lite-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.14,
        "output": 0.28,
        "cache_read": 0.14
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.14,
          "input": 0.14,
          "output": 0.28
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-lite-32k",
      "name": "Doubao-lite-32k",
      "display_name": "Doubao-lite-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.06,
        "output": 0.12,
        "cache_read": 0.012
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.012,
          "input": 0.06,
          "output": 0.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-lite-4k",
      "name": "Doubao-lite-4k",
      "display_name": "Doubao-lite-4k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.06,
        "output": 0.12,
        "cache_read": 0.06
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.06,
          "input": 0.06,
          "output": 0.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-pro-128k",
      "name": "Doubao-pro-128k",
      "display_name": "Doubao-pro-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 1.44
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 1.44
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-pro-256k",
      "name": "Doubao-pro-256k",
      "display_name": "Doubao-pro-256k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 1.44,
        "cache_read": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.8,
          "input": 0.8,
          "output": 1.44
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-pro-32k",
      "name": "Doubao-pro-32k",
      "display_name": "Doubao-pro-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.14,
        "output": 0.35,
        "cache_read": 0.028
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.028,
          "input": 0.14,
          "output": 0.35
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Doubao-pro-4k",
      "name": "Doubao-pro-4k",
      "display_name": "Doubao-pro-4k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.14,
        "output": 0.35
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.14,
          "output": 0.35
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "GPT-OSS-20B",
      "name": "GPT-OSS-20B",
      "display_name": "GPT-OSS-20B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.11,
        "output": 0.55
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.11,
          "output": 0.55
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Gryphe/MythoMax-L2-13b",
      "name": "Gryphe/MythoMax-L2-13b",
      "display_name": "Gryphe/MythoMax-L2-13b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "MiniMax-Text-01",
      "name": "MiniMax-Text-01",
      "display_name": "MiniMax-Text-01",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.14,
        "output": 1.12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "features": [
          "long_context"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.14,
          "output": 1.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Mistral-large-2407",
      "name": "Mistral-large-2407",
      "display_name": "Mistral-large-2407",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2-1.5B-Instruct",
      "name": "Qwen/Qwen2-1.5B-Instruct",
      "display_name": "Qwen/Qwen2-1.5B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2-57B-A14B-Instruct",
      "name": "Qwen/Qwen2-57B-A14B-Instruct",
      "display_name": "Qwen/Qwen2-57B-A14B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.24,
        "output": 0.24
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.24,
          "output": 0.24
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2-72B-Instruct",
      "name": "Qwen/Qwen2-72B-Instruct",
      "display_name": "Qwen/Qwen2-72B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2-7B-Instruct",
      "name": "Qwen/Qwen2-7B-Instruct",
      "display_name": "Qwen/Qwen2-7B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.08,
        "output": 0.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.08,
          "output": 0.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-32B-Instruct",
      "name": "Qwen/Qwen2.5-32B-Instruct",
      "display_name": "Qwen/Qwen2.5-32B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-72B-Instruct",
      "name": "Qwen/Qwen2.5-72B-Instruct",
      "display_name": "Qwen/Qwen2.5-72B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-72B-Instruct-128K",
      "name": "Qwen/Qwen2.5-72B-Instruct-128K",
      "display_name": "Qwen/Qwen2.5-72B-Instruct-128K",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-7B-Instruct",
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "display_name": "Qwen/Qwen2.5-7B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "name": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "display_name": "Qwen/Qwen2.5-Coder-32B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Qwen3-235B-A22B-Thinking-2507",
      "name": "Qwen3-235B-A22B-Thinking-2507",
      "display_name": "Qwen3-235B-A22B-Thinking-2507",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.28,
        "output": 2.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.28,
          "output": 2.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "Stable-Diffusion-3-5-Large",
      "name": "Stable-Diffusion-3-5-Large",
      "display_name": "Stable-Diffusion-3-5-Large",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 4,
        "output": 4,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Stable Diffusion 3.5 Large, developed by Stability AI, is a text-to-image generation model that supports high-quality image creation with excellent prompt responsiveness and customization, suitable for professional applications.",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "WizardLM/WizardCoder-Python-34B-V1.0",
      "name": "WizardLM/WizardCoder-Python-34B-V1.0",
      "display_name": "WizardLM/WizardCoder-Python-34B-V1.0",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.9,
        "output": 0.9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.9,
          "output": 0.9
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ahm-Phi-3-5-MoE-instruct",
      "name": "ahm-Phi-3-5-MoE-instruct",
      "display_name": "ahm-Phi-3-5-MoE-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ahm-Phi-3-5-mini-instruct",
      "name": "ahm-Phi-3-5-mini-instruct",
      "display_name": "ahm-Phi-3-5-mini-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Phi-3.5-mini is a lightweight, state-of-the-art open model built upon the dataset used for Phi-3—which includes synthetic data and carefully curated publicly available websites—focusing on very high-quality, reasoning-intensive data. This model is part of the Phi-3 model family and supports a context length of 128K tokens.",
        "pricing": {
          "input": 1,
          "output": 3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ahm-Phi-3-5-vision-instruct",
      "name": "ahm-Phi-3-5-vision-instruct",
      "display_name": "ahm-Phi-3-5-vision-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.4,
        "output": 1.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "ahm-Phi-3-medium-128k",
      "name": "ahm-Phi-3-medium-128k",
      "display_name": "ahm-Phi-3-medium-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 6,
        "output": 18
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 6,
          "output": 18
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ahm-Phi-3-medium-4k",
      "name": "ahm-Phi-3-medium-4k",
      "display_name": "ahm-Phi-3-medium-4k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "ahm-Phi-3-small-128k",
      "name": "ahm-Phi-3-small-128k",
      "display_name": "ahm-Phi-3-small-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Codestral-2501",
      "name": "aihubmix-Codestral-2501",
      "display_name": "aihubmix-Codestral-2501",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Cohere-command-r",
      "name": "aihubmix-Cohere-command-r",
      "display_name": "aihubmix-Cohere-command-r",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.64,
        "output": 1.92
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.64,
          "output": 1.92
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Jamba-1-5-Large",
      "name": "aihubmix-Jamba-1-5-Large",
      "display_name": "aihubmix-Jamba-1-5-Large",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.2,
        "output": 8.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2.2,
          "output": 8.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-1-405B-Instruct",
      "name": "aihubmix-Llama-3-1-405B-Instruct",
      "display_name": "aihubmix-Llama-3-1-405B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 5,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-1-70B-Instruct",
      "name": "aihubmix-Llama-3-1-70B-Instruct",
      "display_name": "aihubmix-Llama-3-1-70B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.78
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.78
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-1-8B-Instruct",
      "name": "aihubmix-Llama-3-1-8B-Instruct",
      "display_name": "aihubmix-Llama-3-1-8B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.3,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-2-11B-Vision",
      "name": "aihubmix-Llama-3-2-11B-Vision",
      "display_name": "aihubmix-Llama-3-2-11B-Vision",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-2-90B-Vision",
      "name": "aihubmix-Llama-3-2-90B-Vision",
      "display_name": "aihubmix-Llama-3-2-90B-Vision",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.4,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2.4,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Llama-3-70B-Instruct",
      "name": "aihubmix-Llama-3-70B-Instruct",
      "display_name": "aihubmix-Llama-3-70B-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.7,
        "output": 0.7
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.7,
          "output": 0.7
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-Mistral-large",
      "name": "aihubmix-Mistral-large",
      "display_name": "aihubmix-Mistral-large",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-command-r-08-2024",
      "name": "aihubmix-command-r-08-2024",
      "display_name": "aihubmix-command-r-08-2024",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-command-r-plus",
      "name": "aihubmix-command-r-plus",
      "display_name": "aihubmix-command-r-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 3.84,
        "output": 19.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 3.84,
          "output": 19.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aihubmix-command-r-plus-08-2024",
      "name": "aihubmix-command-r-plus-08-2024",
      "display_name": "aihubmix-command-r-plus-08-2024",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2.8,
        "output": 11.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 2.8,
          "output": 11.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "baidu-deepseek-v3.2",
      "name": "baidu-deepseek-v3.2",
      "display_name": "baidu-deepseek-v3.2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 0.411
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.274,
          "output": 0.411
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "baidu-deepseek-v3.2-exp",
      "name": "baidu-deepseek-v3.2-exp",
      "display_name": "baidu-deepseek-v3.2-exp",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.274,
        "output": 0.411,
        "cache_read": 0.0274
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "tools",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.0274,
          "input": 0.274,
          "output": 0.411
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "chatglm_lite",
      "name": "chatglm_lite",
      "display_name": "chatglm_lite",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2858,
        "output": 0.2858
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2858,
          "output": 0.2858
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "chatglm_pro",
      "name": "chatglm_pro",
      "display_name": "chatglm_pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.4286,
        "output": 1.4286
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.4286,
          "output": 1.4286
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "chatglm_std",
      "name": "chatglm_std",
      "display_name": "chatglm_std",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.7144,
        "output": 0.7144
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.7144,
          "output": 0.7144
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "chatglm_turbo",
      "name": "chatglm_turbo",
      "display_name": "chatglm_turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.7144,
        "output": 0.7144
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.7144,
          "output": 0.7144
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-2",
      "name": "claude-2",
      "display_name": "claude-2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 8.8,
        "output": 8.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 8.8,
          "output": 8.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-2.0",
      "name": "claude-2.0",
      "display_name": "claude-2.0",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 8.8,
        "output": 39.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 8.8,
          "output": 39.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-2.1",
      "name": "claude-2.1",
      "display_name": "claude-2.1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 8.8,
        "output": 39.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 8.8,
          "output": 39.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-3-5-sonnet-20240620",
      "name": "claude-3-5-sonnet-20240620",
      "display_name": "claude-3-5-sonnet-20240620",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5
      },
      "limit": {
        "context": 200000,
        "output": 200000
      },
      "metadata": {
        "description": "Claude 3.5 Sonnet delivers performance superior to Opus and speeds faster than its predecessor, all at the same price point. Its core strengths include:\n\nCoding: Autonomously writes, edits, and executes code with advanced reasoning and troubleshooting.\nData Science: Augments human expertise by analyzing unstructured data and using multiple tools to generate insights.\nVisual Processing: Excels at interpreting charts, graphs, and images, accurately transcribing text to derive high-level insights.\nAgentic Tasks: Exceptional tool use makes it highly effective for complex, multi-step agentic workflows that interact with other systems.",
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-5-sonnet@20240620",
      "name": "claude-3-5-sonnet@20240620",
      "display_name": "claude-3-5-sonnet@20240620",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3.3,
        "output": 16.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-3-haiku-20240229",
      "name": "claude-3-haiku-20240229",
      "display_name": "claude-3-haiku-20240229",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.275,
        "output": 0.275
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.275,
          "output": 0.275
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-haiku-20240307",
      "name": "claude-3-haiku-20240307",
      "display_name": "claude-3-haiku-20240307",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.275,
        "output": 1.375
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.275,
          "output": 1.375
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-haiku@20240307",
      "name": "claude-3-haiku@20240307",
      "display_name": "claude-3-haiku@20240307",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.275,
        "output": 1.375
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 0.275,
          "output": 1.375
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-3-opus@20240229",
      "name": "claude-3-opus@20240229",
      "display_name": "claude-3-opus@20240229",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 16.5,
        "output": 82.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 16.5,
          "output": 82.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-3-sonnet-20240229",
      "name": "claude-3-sonnet-20240229",
      "display_name": "claude-3-sonnet-20240229",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 3.3,
        "output": 16.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 3.3,
          "output": 16.5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "claude-instant-1",
      "name": "claude-instant-1",
      "display_name": "claude-instant-1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.793,
        "output": 1.793
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.793,
          "output": 1.793
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "claude-instant-1.2",
      "name": "claude-instant-1.2",
      "display_name": "claude-instant-1.2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.88,
        "output": 3.96
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.88,
          "output": 3.96
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "code-davinci-edit-001",
      "name": "code-davinci-edit-001",
      "display_name": "code-davinci-edit-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cogview-3",
      "name": "cogview-3",
      "display_name": "cogview-3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 35.5,
        "output": 35.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 35.5,
          "output": 35.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "cogview-3-plus",
      "name": "cogview-3-plus",
      "display_name": "cogview-3-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 10
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command",
      "name": "command",
      "display_name": "command",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 1,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-light",
      "name": "command-light",
      "display_name": "command-light",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-light-nightly",
      "name": "command-light-nightly",
      "display_name": "command-light-nightly",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-nightly",
      "name": "command-nightly",
      "display_name": "command-nightly",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-r",
      "name": "command-r",
      "display_name": "command-r",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.64,
        "output": 1.92
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.64,
          "output": 1.92
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-r-08-2024",
      "name": "command-r-08-2024",
      "display_name": "command-r-08-2024",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.2,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-r-plus",
      "name": "command-r-plus",
      "display_name": "command-r-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 3.84,
        "output": 19.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 3.84,
          "output": 19.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "command-r-plus-08-2024",
      "name": "command-r-plus-08-2024",
      "display_name": "command-r-plus-08-2024",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 2.8,
        "output": 11.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 2.8,
          "output": 11.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "dall-e-2",
      "name": "dall-e-2",
      "display_name": "dall-e-2",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 16,
        "output": 16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 16,
          "output": 16
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "davinci",
      "name": "davinci",
      "display_name": "davinci",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "davinci-002",
      "name": "davinci-002",
      "display_name": "davinci-002",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
      "name": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
      "display_name": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.32
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.32
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.01,
        "output": 0.01
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.01,
          "output": 0.01
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.01,
        "output": 0.01
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.01,
          "output": 0.01
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Open source deployment from SiliconFlow, the model itself is obtained through knowledge distillation.",
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Open source deployment from SiliconFlow, the model itself is obtained through knowledge distillation.",
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "display_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.01,
        "output": 0.01
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Open source deployment from SiliconFlow, the model itself is obtained through knowledge distillation.",
        "pricing": {
          "input": 0.01,
          "output": 0.01
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-V2-Chat",
      "name": "deepseek-ai/DeepSeek-V2-Chat",
      "display_name": "deepseek-ai/DeepSeek-V2-Chat",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.32
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.32
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/DeepSeek-V2.5",
      "name": "deepseek-ai/DeepSeek-V2.5",
      "display_name": "deepseek-ai/DeepSeek-V2.5",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.32
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.32
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/deepseek-llm-67b-chat",
      "name": "deepseek-ai/deepseek-llm-67b-chat",
      "display_name": "deepseek-ai/deepseek-llm-67b-chat",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-ai/deepseek-vl2",
      "name": "deepseek-ai/deepseek-vl2",
      "display_name": "deepseek-ai/deepseek-vl2",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.16,
        "output": 0.16
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.16,
          "output": 0.16
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-v3",
      "name": "deepseek-v3",
      "display_name": "deepseek-v3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.272,
        "output": 1.088,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0,
          "input": 0.272,
          "output": 1.088
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "distil-whisper-large-v3-en",
      "name": "distil-whisper-large-v3-en",
      "display_name": "distil-whisper-large-v3-en",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 5.556,
        "output": 5.556
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "stt"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 5.556,
          "output": 5.556
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-1-5-thinking-vision-pro-250428",
      "name": "doubao-1-5-thinking-vision-pro-250428",
      "display_name": "doubao-1-5-thinking-vision-pro-250428",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Deep Thinking  \nImage Understanding  \nVisual Localization  \nVideo Understanding  \nTool Invocation  \nStructured Output",
        "pricing": {
          "cache_read": 2,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-001",
      "name": "gemini-2.0-flash-001",
      "display_name": "gemini-2.0-flash-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.25
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google Gemini's enterprise version VertexAI",
        "pricing": {
          "cache_read": 0.25,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-exp-image-generation",
      "name": "gemini-2.0-flash-exp-image-generation",
      "display_name": "gemini-2.0-flash-exp-image-generation",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.0-flash-lite",
      "name": "gemini-2.0-flash-lite",
      "display_name": "gemini-2.0-flash-lite",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 0.076,
        "output": 0.304,
        "cache_read": 0.076
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Gemini-2.0-flash Lightweight Official Version",
        "typeHints": [
          "llm"
        ],
        "features": [
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.076,
          "input": 0.076,
          "output": 0.304
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-2.0-flash-lite-001",
      "name": "gemini-2.0-flash-lite-001",
      "display_name": "gemini-2.0-flash-lite-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.076,
        "output": 0.304,
        "cache_read": 0.076
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google Gemini's enterprise version VertexAI",
        "pricing": {
          "cache_read": 0.076,
          "input": 0.076,
          "output": 0.304
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-2.5-pro-exp-03-25",
      "name": "gemini-2.5-pro-exp-03-25",
      "display_name": "gemini-2.5-pro-exp-03-25",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 1.25,
        "output": 5,
        "cache_read": 0.31
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google’s latest experimental model, highly unstable, for experience only.\nIt boasts strong reasoning and coding capabilities, able to \"think\" before responding, enhancing performance and accuracy in complex tasks. It supports multimodal inputs (text, audio, images, video) and a 1 million token context window, suitable for advanced programming, math, and science tasks.\n\nThis means Gemini 2.5 can handle more complex problems in coding, science and math, and support more context-aware agents.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "structured_outputs",
          "tools",
          "long_context"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0.31,
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gemini-embedding-exp-03-07",
      "name": "gemini-embedding-exp-03-07",
      "display_name": "gemini-embedding-exp-03-07",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-exp-1114",
      "name": "gemini-exp-1114",
      "display_name": "gemini-exp-1114",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-exp-1121",
      "name": "gemini-exp-1121",
      "display_name": "gemini-exp-1121",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-pro",
      "name": "gemini-pro",
      "display_name": "gemini-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemini-pro-vision",
      "name": "gemini-pro-vision",
      "display_name": "gemini-pro-vision",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma-7b-it",
      "name": "gemma-7b-it",
      "display_name": "gemma-7b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gemma2-9b-it",
      "name": "gemma2-9b-it",
      "display_name": "gemma2-9b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-3-turbo",
      "name": "glm-3-turbo",
      "display_name": "glm-3-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.71,
        "output": 0.71
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.71,
          "output": 0.71
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4",
      "name": "glm-4",
      "display_name": "glm-4",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 14.2,
        "output": 14.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 14.2,
          "output": 14.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4-flash",
      "name": "glm-4-flash",
      "display_name": "glm-4-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4-plus",
      "name": "glm-4-plus",
      "display_name": "glm-4-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 8,
        "output": 8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 8,
          "output": 8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4.5-airx",
      "name": "glm-4.5-airx",
      "display_name": "glm-4.5-airx",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 1.1,
        "output": 4.51,
        "cache_read": 0.22
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "GLM-4.5-AirX is the high-speed version of GLM-4.5-Air, with faster response times, specifically designed for large-scale high-speed demands.",
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "cache_read": 0.22,
          "input": 1.1,
          "output": 4.51
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4v",
      "name": "glm-4v",
      "display_name": "glm-4v",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 14.2,
        "output": 14.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 14.2,
          "output": 14.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "glm-4v-plus",
      "name": "glm-4v-plus",
      "display_name": "glm-4v-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "google/gemini-exp-1114",
      "name": "google/gemini-exp-1114",
      "display_name": "google/gemini-exp-1114",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "google/gemma-2-27b-it",
      "name": "google/gemma-2-27b-it",
      "display_name": "google/gemma-2-27b-it",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "google/gemma-2-9b-it:free",
      "name": "google/gemma-2-9b-it:free",
      "display_name": "google/gemma-2-9b-it:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo",
      "name": "gpt-3.5-turbo",
      "display_name": "gpt-3.5-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 1.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-0125",
      "name": "gpt-3.5-turbo-0125",
      "display_name": "gpt-3.5-turbo-0125",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 1.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-0301",
      "name": "gpt-3.5-turbo-0301",
      "display_name": "gpt-3.5-turbo-0301",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.5,
        "output": 1.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.5,
          "output": 1.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-0613",
      "name": "gpt-3.5-turbo-0613",
      "display_name": "gpt-3.5-turbo-0613",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.5,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.5,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-1106",
      "name": "gpt-3.5-turbo-1106",
      "display_name": "gpt-3.5-turbo-1106",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-16k",
      "name": "gpt-3.5-turbo-16k",
      "display_name": "gpt-3.5-turbo-16k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-16k-0613",
      "name": "gpt-3.5-turbo-16k-0613",
      "display_name": "gpt-3.5-turbo-16k-0613",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-3.5-turbo-instruct",
      "name": "gpt-3.5-turbo-instruct",
      "display_name": "gpt-3.5-turbo-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.5,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.5,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4",
      "name": "gpt-4",
      "display_name": "gpt-4",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 30,
        "output": 60
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 30,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-0125-preview",
      "name": "gpt-4-0125-preview",
      "display_name": "gpt-4-0125-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-0314",
      "name": "gpt-4-0314",
      "display_name": "gpt-4-0314",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 30,
        "output": 60
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 30,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-0613",
      "name": "gpt-4-0613",
      "display_name": "gpt-4-0613",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 30,
        "output": 60
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 30,
          "output": 60
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-1106-preview",
      "name": "gpt-4-1106-preview",
      "display_name": "gpt-4-1106-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-32k-0314",
      "name": "gpt-4-32k-0314",
      "display_name": "gpt-4-32k-0314",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 60,
        "output": 120
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 60,
          "output": 120
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-32k-0613",
      "name": "gpt-4-32k-0613",
      "display_name": "gpt-4-32k-0613",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 60,
        "output": 120
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 60,
          "output": 120
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-turbo",
      "name": "gpt-4-turbo",
      "display_name": "gpt-4-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-turbo-2024-04-09",
      "name": "gpt-4-turbo-2024-04-09",
      "display_name": "gpt-4-turbo-2024-04-09",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-turbo-preview",
      "name": "gpt-4-turbo-preview",
      "display_name": "gpt-4-turbo-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4-vision-preview",
      "name": "gpt-4-vision-preview",
      "display_name": "gpt-4-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-2024-05-13",
      "name": "gpt-4o-2024-05-13",
      "display_name": "gpt-4o-2024-05-13",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5,
        "output": 15,
        "cache_read": 5
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "pricing": {
          "cache_read": 5,
          "input": 5,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-mini-2024-07-18",
      "name": "gpt-4o-mini-2024-07-18",
      "display_name": "gpt-4o-mini-2024-07-18",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 0.15,
        "output": 0.6,
        "cache_read": 0.075
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0.075,
          "input": 0.15,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "gpt-oss-20b",
      "name": "gpt-oss-20b",
      "display_name": "gpt-oss-20b",
      "type": "chat",
      "reasoning": {
        "supported": true,
        "default": true
      },
      "tool_call": true,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.11,
        "output": 0.55
      },
      "limit": {
        "context": 128000,
        "output": 128000
      },
      "metadata": {
        "description": "gpt-oss-20b is a 21-billion parameter open-weight model released by OpenAI under the Apache 2.0 license. Its core feature is a Mixture-of-Experts (MoE) architecture that uses only 3.6B active parameters, enabling low-latency inference and deployment on consumer GPUs. The model also supports fine-tuning, function calling, tool use, and structured outputs.",
        "typeHints": [
          "llm"
        ],
        "features": [
          "thinking",
          "function_calling",
          "structured_outputs"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.11,
          "output": 0.55
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "grok-2-vision-1212",
      "name": "grok-2-vision-1212",
      "display_name": "grok-2-vision-1212",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 1.8,
        "output": 9
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 1.8,
          "output": 9
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "grok-vision-beta",
      "name": "grok-vision-beta",
      "display_name": "grok-vision-beta",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 5.6,
        "output": 16.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "input": 5.6,
          "output": 16.8
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "imagen-4.0-generate-preview-05-20",
      "name": "imagen-4.0-generate-preview-05-20",
      "display_name": "imagen-4.0-generate-preview-05-20",
      "type": "image-generation",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Google's latest raw image model",
        "typeHints": [
          "image_generation"
        ],
        "inputModalities": [
          "text",
          "image"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "jina-embeddings-v2-base-code",
      "name": "jina-embeddings-v2-base-code",
      "display_name": "jina-embeddings-v2-base-code",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.05,
        "output": 0.05
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Model optimized for code and document search, 768-dimensional, 137M parameters.",
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.05,
          "output": 0.05
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "learnlm-1.5-pro-experimental",
      "name": "learnlm-1.5-pro-experimental",
      "display_name": "learnlm-1.5-pro-experimental",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.25,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.25,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-405b-instruct",
      "name": "llama-3.1-405b-instruct",
      "display_name": "llama-3.1-405b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-405b-reasoning",
      "name": "llama-3.1-405b-reasoning",
      "display_name": "llama-3.1-405b-reasoning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-70b",
      "name": "llama-3.1-70b",
      "display_name": "llama-3.1-70b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-70b-versatile",
      "name": "llama-3.1-70b-versatile",
      "display_name": "llama-3.1-70b-versatile",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-8b-instant",
      "name": "llama-3.1-8b-instant",
      "display_name": "llama-3.1-8b-instant",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.3,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.1-sonar-small-128k-online",
      "name": "llama-3.1-sonar-small-128k-online",
      "display_name": "llama-3.1-sonar-small-128k-online",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "On February 22, 2025, this model will be officially discontinued. The Perplexity AI official fine-tuned LLMA online interface is currently supported only at the api.aihubmix.com address.",
        "pricing": {
          "input": 0.3,
          "output": 0.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.2-11b-vision-preview",
      "name": "llama-3.2-11b-vision-preview",
      "display_name": "llama-3.2-11b-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.2-1b-preview",
      "name": "llama-3.2-1b-preview",
      "display_name": "llama-3.2-1b-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.2-3b-preview",
      "name": "llama-3.2-3b-preview",
      "display_name": "llama-3.2-3b-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama-3.2-90b-vision-preview",
      "name": "llama-3.2-90b-vision-preview",
      "display_name": "llama-3.2-90b-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.4,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2.4,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama2-70b-4096",
      "name": "llama2-70b-4096",
      "display_name": "llama2-70b-4096",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama2-7b-2048",
      "name": "llama2-7b-2048",
      "display_name": "llama2-7b-2048",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-70b-8192",
      "name": "llama3-70b-8192",
      "display_name": "llama3-70b-8192",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.7,
        "output": 0.937288
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.7,
          "output": 0.937288
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-70b-8192(33)",
      "name": "llama3-70b-8192(33)",
      "display_name": "llama3-70b-8192(33)",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.65,
        "output": 2.65
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2.65,
          "output": 2.65
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-8b-8192",
      "name": "llama3-8b-8192",
      "display_name": "llama3-8b-8192",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.06,
        "output": 0.12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.06,
          "output": 0.12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-8b-8192(33)",
      "name": "llama3-8b-8192(33)",
      "display_name": "llama3-8b-8192(33)",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.3,
          "output": 0.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-groq-70b-8192-tool-use-preview",
      "name": "llama3-groq-70b-8192-tool-use-preview",
      "display_name": "llama3-groq-70b-8192-tool-use-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.00089,
        "output": 0.00089
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.00089,
          "output": 0.00089
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "llama3-groq-8b-8192-tool-use-preview",
      "name": "llama3-groq-8b-8192-tool-use-preview",
      "display_name": "llama3-groq-8b-8192-tool-use-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.00019,
        "output": 0.00019
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.00019,
          "output": 0.00019
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/Llama-3.2-90B-Vision-Instruct",
      "name": "meta-llama/Llama-3.2-90B-Vision-Instruct",
      "display_name": "meta-llama/Llama-3.2-90B-Vision-Instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/llama-3.1-405b-instruct:free",
      "name": "meta-llama/llama-3.1-405b-instruct:free",
      "display_name": "meta-llama/llama-3.1-405b-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/llama-3.1-70b-instruct:free",
      "name": "meta-llama/llama-3.1-70b-instruct:free",
      "display_name": "meta-llama/llama-3.1-70b-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/llama-3.1-8b-instruct:free",
      "name": "meta-llama/llama-3.1-8b-instruct:free",
      "display_name": "meta-llama/llama-3.1-8b-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/llama-3.2-11b-vision-instruct:free",
      "name": "meta-llama/llama-3.2-11b-vision-instruct:free",
      "display_name": "meta-llama/llama-3.2-11b-vision-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama/llama-3.2-3b-instruct:free",
      "name": "meta-llama/llama-3.2-3b-instruct:free",
      "display_name": "meta-llama/llama-3.2-3b-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta/llama-3.1-405b-instruct",
      "name": "meta/llama-3.1-405b-instruct",
      "display_name": "meta/llama-3.1-405b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 5,
        "output": 5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 5,
          "output": 5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta/llama3-8B-chat",
      "name": "meta/llama3-8B-chat",
      "display_name": "meta/llama3-8B-chat",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.3,
        "output": 0.3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.3,
          "output": 0.3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "mistralai/mistral-7b-instruct:free",
      "name": "mistralai/mistral-7b-instruct:free",
      "display_name": "mistralai/mistral-7b-instruct:free",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.002,
        "output": 0.002
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.002,
          "output": 0.002
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-128k",
      "name": "moonshot-v1-128k",
      "display_name": "moonshot-v1-128k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 10
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-128k-vision-preview",
      "name": "moonshot-v1-128k-vision-preview",
      "display_name": "moonshot-v1-128k-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 10,
        "output": 10
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 10,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-32k",
      "name": "moonshot-v1-32k",
      "display_name": "moonshot-v1-32k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-32k-vision-preview",
      "name": "moonshot-v1-32k-vision-preview",
      "display_name": "moonshot-v1-32k-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-8k",
      "name": "moonshot-v1-8k",
      "display_name": "moonshot-v1-8k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "moonshot-v1-8k-vision-preview",
      "name": "moonshot-v1-8k-vision-preview",
      "display_name": "moonshot-v1-8k-vision-preview",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "nvidia/llama-3.1-nemotron-70b-instruct",
      "name": "nvidia/llama-3.1-nemotron-70b-instruct",
      "display_name": "nvidia/llama-3.1-nemotron-70b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 0.6
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o1-mini-2024-09-12",
      "name": "o1-mini-2024-09-12",
      "display_name": "o1-mini-2024-09-12",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 12,
        "cache_read": 1.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 1.5,
          "input": 3,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "omni-moderation-latest",
      "name": "omni-moderation-latest",
      "display_name": "omni-moderation-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-flash",
      "name": "qwen-flash",
      "display_name": "qwen-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.2,
        "cache_read": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model adopts tiered pricing.",
        "pricing": {
          "cache_read": 0.02,
          "input": 0.02,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-flash-2025-07-28",
      "name": "qwen-flash-2025-07-28",
      "display_name": "qwen-flash-2025-07-28",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.02,
        "output": 0.2,
        "cache_read": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "The model adopts tiered pricing.",
        "pricing": {
          "cache_read": 0.02,
          "input": 0.02,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-long",
      "name": "qwen-long",
      "display_name": "qwen-long",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-max",
      "name": "qwen-max",
      "display_name": "qwen-max",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.38,
        "output": 1.52
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.38,
          "output": 1.52
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-max-longcontext",
      "name": "qwen-max-longcontext",
      "display_name": "qwen-max-longcontext",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 7,
        "output": 21
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 7,
          "output": 21
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-plus",
      "name": "qwen-plus",
      "display_name": "qwen-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.7,
        "output": 2.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.7,
          "output": 2.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-turbo",
      "name": "qwen-turbo",
      "display_name": "qwen-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.36,
        "output": 1.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "long_context"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.36,
          "output": 1.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen-turbo-2024-11-01",
      "name": "qwen-turbo-2024-11-01",
      "display_name": "qwen-turbo-2024-11-01",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.36,
        "output": 1.08
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "llm"
        ],
        "features": [
          "long_context"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.36,
          "output": 1.08
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-14b-instruct",
      "name": "qwen2.5-14b-instruct",
      "display_name": "qwen2.5-14b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-32b-instruct",
      "name": "qwen2.5-32b-instruct",
      "display_name": "qwen2.5-32b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.6,
        "output": 1.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.6,
          "output": 1.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-3b-instruct",
      "name": "qwen2.5-3b-instruct",
      "display_name": "qwen2.5-3b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-72b-instruct",
      "name": "qwen2.5-72b-instruct",
      "display_name": "qwen2.5-72b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-7b-instruct",
      "name": "qwen2.5-7b-instruct",
      "display_name": "qwen2.5-7b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-coder-1.5b-instruct",
      "name": "qwen2.5-coder-1.5b-instruct",
      "display_name": "qwen2.5-coder-1.5b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-coder-7b-instruct",
      "name": "qwen2.5-coder-7b-instruct",
      "display_name": "qwen2.5-coder-7b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-math-1.5b-instruct",
      "name": "qwen2.5-math-1.5b-instruct",
      "display_name": "qwen2.5-math-1.5b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-math-72b-instruct",
      "name": "qwen2.5-math-72b-instruct",
      "display_name": "qwen2.5-math-72b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.8,
        "output": 2.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.8,
          "output": 2.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qwen2.5-math-7b-instruct",
      "name": "qwen2.5-math-7b-instruct",
      "display_name": "qwen2.5-math-7b-instruct",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "sonar-reasoning-pro",
      "name": "sonar-reasoning-pro",
      "display_name": "sonar-reasoning-pro",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 12
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 12
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "step-2-16k",
      "name": "step-2-16k",
      "display_name": "step-2-16k",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-ada-001",
      "name": "text-ada-001",
      "display_name": "text-ada-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-babbage-001",
      "name": "text-babbage-001",
      "display_name": "text-babbage-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.5,
        "output": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-curie-001",
      "name": "text-curie-001",
      "display_name": "text-curie-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-davinci-002",
      "name": "text-davinci-002",
      "display_name": "text-davinci-002",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-davinci-003",
      "name": "text-davinci-003",
      "display_name": "text-davinci-003",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-davinci-edit-001",
      "name": "text-davinci-edit-001",
      "display_name": "text-davinci-edit-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-3-large",
      "name": "text-embedding-3-large",
      "display_name": "text-embedding-3-large",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.13,
        "output": 0.13
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.13,
          "output": 0.13
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-3-small",
      "name": "text-embedding-3-small",
      "display_name": "text-embedding-3-small",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.02,
        "output": 0.02
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.02,
          "output": 0.02
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-ada-002",
      "name": "text-embedding-ada-002",
      "display_name": "text-embedding-ada-002",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-embedding-v1",
      "name": "text-embedding-v1",
      "display_name": "text-embedding-v1",
      "type": "embedding",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text"
        ]
      },
      "cost": {
        "input": 0.1,
        "output": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "embedding"
        ],
        "inputModalities": [
          "text"
        ],
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-moderation-007",
      "name": "text-moderation-007",
      "display_name": "text-moderation-007",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-moderation-latest",
      "name": "text-moderation-latest",
      "display_name": "text-moderation-latest",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-moderation-stable",
      "name": "text-moderation-stable",
      "display_name": "text-moderation-stable",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "text-search-ada-doc-001",
      "name": "text-search-ada-doc-001",
      "display_name": "text-search-ada-doc-001",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 20
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 20
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "tts-1",
      "name": "tts-1",
      "display_name": "tts-1",
      "type": "audio",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 15,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "tts"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 15,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "tts-1-1106",
      "name": "tts-1-1106",
      "display_name": "tts-1-1106",
      "type": "audio",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 15,
        "output": 15
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "tts"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 15,
          "output": 15
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "tts-1-hd",
      "name": "tts-1-hd",
      "display_name": "tts-1-hd",
      "type": "audio",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 30,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "tts"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 30,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "tts-1-hd-1106",
      "name": "tts-1-hd-1106",
      "display_name": "tts-1-hd-1106",
      "type": "audio",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 30,
        "output": 30
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "tts"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 30,
          "output": 30
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "veo-3",
      "name": "veo-3",
      "display_name": "veo-3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "veo3 reverse access with a total cost of just $0.41 per video generation., OpenAI chat port compatible format.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "veo3",
      "name": "veo3",
      "display_name": "veo3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "text",
          "image",
          "audio",
          "video"
        ]
      },
      "cost": {
        "input": 2,
        "output": 2,
        "cache_read": 0
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "veo3 reverse access with a total cost of just $0.41 per video generation., OpenAI chat port compatible format.\nNote that this is a reverse interface, and charges are based on the number of requests. As long as a request is initiated, even if it returns a failure, you will be charged. If you cannot accept this, please do not use it.",
        "typeHints": [
          "video"
        ],
        "inputModalities": [
          "text",
          "image",
          "audio",
          "video"
        ],
        "pricing": {
          "cache_read": 0,
          "input": 2,
          "output": 2
        },
        "source": "public-provider-conf"
      },
      "vision": true
    },
    {
      "id": "whisper-1",
      "name": "whisper-1",
      "display_name": "whisper-1",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 100,
        "output": 100
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "description": "Ignore the displayed price on the page; the actual charge for this model request is consistent with the official, so you can use it with confidence.",
        "typeHints": [
          "stt"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 100,
          "output": 100
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "whisper-large-v3",
      "name": "whisper-large-v3",
      "display_name": "whisper-large-v3",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 30.834,
        "output": 30.834
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "stt"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 30.834,
          "output": 30.834
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "whisper-large-v3-turbo",
      "name": "whisper-large-v3-turbo",
      "display_name": "whisper-large-v3-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "modalities": {
        "input": [
          "audio"
        ]
      },
      "cost": {
        "input": 5.556,
        "output": 5.556
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "typeHints": [
          "stt"
        ],
        "inputModalities": [
          "audio"
        ],
        "pricing": {
          "input": 5.556,
          "output": 5.556
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-large",
      "name": "yi-large",
      "display_name": "yi-large",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 3,
        "output": 3
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 3,
          "output": 3
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-large-rag",
      "name": "yi-large-rag",
      "display_name": "yi-large-rag",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4,
        "output": 4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4,
          "output": 4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-large-turbo",
      "name": "yi-large-turbo",
      "display_name": "yi-large-turbo",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.8,
        "output": 1.8
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 1.8,
          "output": 1.8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-lightning",
      "name": "yi-lightning",
      "display_name": "yi-lightning",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.2,
        "output": 0.2
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-medium",
      "name": "yi-medium",
      "display_name": "yi-medium",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 0.4
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.4,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "yi-vl-plus",
      "name": "yi-vl-plus",
      "display_name": "yi-vl-plus",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.000852,
        "output": 0.000852
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.000852,
          "output": 0.000852
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-2024-08-06-global",
      "name": "gpt-4o-2024-08-06-global",
      "display_name": "gpt-4o-2024-08-06-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2.5,
        "output": 10,
        "cache_read": 1.25
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 1.25,
          "input": 2.5,
          "output": 10
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "gpt-4o-mini-global",
      "name": "gpt-4o-mini-global",
      "display_name": "gpt-4o-mini-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.15,
        "output": 0.6,
        "cache_read": 0.075
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.075,
          "input": 0.15,
          "output": 0.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama-3-70b",
      "name": "meta-llama-3-70b",
      "display_name": "meta-llama-3-70b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 4.795,
        "output": 4.795
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 4.795,
          "output": 4.795
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "meta-llama-3-8b",
      "name": "meta-llama-3-8b",
      "display_name": "meta-llama-3-8b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.548,
        "output": 0.548
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.548,
          "output": 0.548
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o3-global",
      "name": "o3-global",
      "display_name": "o3-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 2,
        "output": 8,
        "cache_read": 0.5
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.5,
          "input": 2,
          "output": 8
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o3-mini-global",
      "name": "o3-mini-global",
      "display_name": "o3-mini-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 1.1,
        "output": 4.4,
        "cache_read": 0.55
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.55,
          "input": 1.1,
          "output": 4.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "o3-pro-global",
      "name": "o3-pro-global",
      "display_name": "o3-pro-global",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 20,
        "output": 80
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 20,
          "output": 80
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qianfan-chinese-llama-2-13b",
      "name": "qianfan-chinese-llama-2-13b",
      "display_name": "qianfan-chinese-llama-2-13b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.822,
        "output": 0.822
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.822,
          "output": 0.822
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "qianfan-llama-vl-8b",
      "name": "qianfan-llama-vl-8b",
      "display_name": "qianfan-llama-vl-8b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.274,
        "output": 0.685
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.274,
          "output": 0.685
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aistudio_gemini-2.0-flash",
      "name": "aistudio_gemini-2.0-flash",
      "display_name": "aistudio_gemini-2.0-flash",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.1,
        "output": 0.4,
        "cache_read": 0.25
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.25,
          "input": 0.1,
          "output": 0.4
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "aistudio_gpt-4.1-mini",
      "name": "aistudio_gpt-4.1-mini",
      "display_name": "aistudio_gpt-4.1-mini",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.4,
        "output": 1.6,
        "cache_read": 0.1
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "cache_read": 0.1,
          "input": 0.4,
          "output": 1.6
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "deepseek-r1-distill-qianfan-llama-8b",
      "name": "deepseek-r1-distill-qianfan-llama-8b",
      "display_name": "deepseek-r1-distill-qianfan-llama-8b",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.137,
        "output": 0.548
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.137,
          "output": 0.548
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-1-5-pro-256k-250115",
      "name": "doubao-1-5-pro-256k-250115",
      "display_name": "doubao-1-5-pro-256k-250115",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.684,
        "output": 1.2312
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.684,
          "output": 1.2312
        },
        "source": "public-provider-conf"
      },
      "vision": false
    },
    {
      "id": "doubao-1-5-pro-32k-250115",
      "name": "doubao-1-5-pro-32k-250115",
      "display_name": "doubao-1-5-pro-32k-250115",
      "type": "chat",
      "reasoning": {
        "supported": false
      },
      "tool_call": false,
      "cost": {
        "input": 0.108,
        "output": 0.27
      },
      "limit": {
        "context": 8192,
        "output": 8192
      },
      "metadata": {
        "pricing": {
          "input": 0.108,
          "output": 0.27
        },
        "source": "public-provider-conf"
      },
      "vision": false
    }
  ],
  "metadata": {
    "source": "public-provider-conf"
  }
}