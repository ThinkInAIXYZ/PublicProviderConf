{"id":"jiekou","name":"Jiekou","display_name":"Jiekou","updated_at":"2025-10-22T10:05:33.936Z","models":[{"id":"baichuan/baichuan-m2-32b","name":"BaiChuan M2 32B","display_name":"BaiChuan M2 32B","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Baichuan-M2 is a medically-enhanced reasoning model specifically designed for real-world medical reasoning tasks. We begin with real-world medical questions and conduct reinforcement learning training based on a large-scale verifier system. While maintaining the model's general capabilities, the medical effectiveness of Baichuan-M2 has achieved breakthrough improvements.  Baichuan-M2 is currently the world's best open-source medical model. On the HealthBench Benchmark, it surpasses all open-source models, including GPT-OSS-120B, as well as many cutting-edge closed-source models. It is the open-source model closest to GPT-5 in terms of medical capabilities.  Our research demonstrates that a robust verifier is crucial for aligning model capabilities with real-world applications, and an end-to-end reinforcement learning approach fundamentally enhances the model's medical reasoning abilities. The release of Baichuan-M2 represents a significant advancement in the field of medical artificial intelligence, pushing t","endpoints":["completion"],"features":["structured-outputs","reasoning"],"pricing":{"inputPerM":700,"outputPerM":700},"source":"public-provider-conf"},"vision":false},{"id":"claude-3-5-haiku-20241022","name":"claude-3-5-haiku-20241022","display_name":"claude-3-5-haiku-20241022","type":"chat","context_length":200000,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"claude-3-5-haiku-20241022","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":8000,"outputPerM":40000},"source":"public-provider-conf"},"vision":true},{"id":"claude-3-5-sonnet-20241022","name":"claude-3-5-sonnet-20241022","display_name":"claude-3-5-sonnet-20241022","type":"chat","context_length":200000,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"支持prompt cache的模型","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"claude-3-7-sonnet-20250219","name":"claude-3-7-sonnet-20250219","display_name":"claude-3-7-sonnet-20250219","type":"chat","context_length":200000,"max_output_tokens":64000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"支持prompt cache的模型","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"claude-3-haiku-20240307","name":"claude-3-haiku-20240307","display_name":"claude-3-haiku-20240307","type":"chat","context_length":200000,"max_output_tokens":4096,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"claude-3-haiku-20240307","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":2500,"outputPerM":12500},"source":"public-provider-conf"},"vision":true},{"id":"claude-haiku-4-5-20251001","name":"claude-haiku-4-5-20251001","display_name":"claude-haiku-4-5-20251001","type":"chat","context_length":20000,"max_output_tokens":20000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"endpoints":["anthropic","completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":10000,"outputPerM":50000},"source":"public-provider-conf"},"vision":true},{"id":"claude-opus-4-1-20250805","name":"claude-opus-4-1-20250805","display_name":"claude-opus-4-1-20250805","type":"chat","context_length":200000,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":150000,"outputPerM":750000},"source":"public-provider-conf"},"vision":true},{"id":"claude-opus-4-20250514","name":"claude-opus-4-20250514","display_name":"claude-opus-4-20250514","type":"chat","context_length":200000,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"支持prompt cache的模型","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":150000,"outputPerM":750000},"source":"public-provider-conf"},"vision":true},{"id":"claude-sonnet-4-20250514","name":"claude-sonnet-4-20250514","display_name":"claude-sonnet-4-20250514","type":"chat","context_length":200000,"max_output_tokens":64000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"支持prompt cache的模型","features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"claude-sonnet-4-5-20250929","name":"claude-sonnet-4-5-20250929","display_name":"claude-sonnet-4-5-20250929","type":"chat","context_length":200000,"max_output_tokens":64000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"deepseek/deepseek-r1-0528","name":"DeepSeek R1 0528","display_name":"DeepSeek R1 0528","type":"chat","context_length":163840,"max_output_tokens":163840,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"DeepSeek R1 0528 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.","endpoints":["completion","batch-api"],"features":["function-calling","structured-outputs","reasoning"],"pricing":{"inputPerM":7000,"outputPerM":25000},"source":"public-provider-conf"},"vision":false},{"id":"deepseek/deepseek-v3-0324","name":"DeepSeek V3 0324","display_name":"DeepSeek V3 0324","type":"chat","context_length":163840,"max_output_tokens":163840,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.","endpoints":["completion","anthropic"],"features":["function-calling","structured-outputs"],"pricing":{"inputPerM":2800,"outputPerM":11400},"source":"public-provider-conf"},"vision":false},{"id":"deepseek/deepseek-v3.1","name":"DeepSeek V3.1","display_name":"DeepSeek V3.1","type":"chat","context_length":163840,"max_output_tokens":163840,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode.DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.","endpoints":["completion","anthropic"],"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":2700,"outputPerM":10000},"source":"public-provider-conf"},"vision":false},{"id":"baidu/ernie-4.5-300b-a47b-paddle","name":"ERNIE 4.5 300B A47B","display_name":"ERNIE 4.5 300B A47B","type":"chat","context_length":123000,"max_output_tokens":12000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio","endpoints":["completion"],"features":["function-calling","structured-outputs"],"pricing":{"inputPerM":2800,"outputPerM":11000},"source":"public-provider-conf"},"vision":false},{"id":"baidu/ernie-4.5-vl-424b-a47b","name":"ERNIE 4.5 VL 424B A47B","display_name":"ERNIE 4.5 VL 424B A47B","type":"chat","context_length":123000,"max_output_tokens":16000,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio","endpoints":["completion"],"features":["function-calling","vision","reasoning"],"pricing":{"inputPerM":4200,"outputPerM":12500},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.0-flash-20250609","name":"gemini-2.0-flash-20250609","display_name":"gemini-2.0-flash-20250609","type":"chat","context_length":1048576,"max_output_tokens":200000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":1500,"outputPerM":6000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.0-flash-lite","name":"gemini-2.0-flash-lite","display_name":"gemini-2.0-flash-lite","type":"chat","context_length":1048576,"max_output_tokens":65535,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":750,"outputPerM":3000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-flash","name":"gemini-2.5-flash","display_name":"gemini-2.5-flash","type":"chat","context_length":1048576,"max_output_tokens":65535,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":3000,"outputPerM":25000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-flash-lite","name":"gemini-2.5-flash-lite","display_name":"gemini-2.5-flash-lite","type":"chat","context_length":1048576,"max_output_tokens":65535,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":1000,"outputPerM":4000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-flash-lite-preview-06-17","name":"gemini-2.5-flash-lite-preview-06-17","display_name":"gemini-2.5-flash-lite-preview-06-17","type":"chat","context_length":1048576,"max_output_tokens":65535,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":1000,"outputPerM":4000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-flash-lite-preview-09-2025","name":"gemini-2.5-flash-lite-preview-09-2025","display_name":"gemini-2.5-flash-lite-preview-09-2025","type":"chat","context_length":1048576,"max_output_tokens":65536,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":1000,"outputPerM":4000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-flash-preview-05-20","name":"gemini-2.5-flash-preview-05-20","display_name":"gemini-2.5-flash-preview-05-20","type":"chat","context_length":1048576,"max_output_tokens":200000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":1500,"outputPerM":35000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-pro","name":"gemini-2.5-pro","display_name":"gemini-2.5-pro","type":"chat","context_length":1048576,"max_output_tokens":65535,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":12500,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"gemini-2.5-pro-preview-06-05","name":"gemini-2.5-pro-preview-06-05","display_name":"gemini-2.5-pro-preview-06-05","type":"chat","context_length":1048576,"max_output_tokens":200000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":12500,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"google/gemma-3-27b-it","name":"Gemma 3 27B","display_name":"Gemma 3 27B","type":"chat","context_length":32768,"max_output_tokens":32768,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 32k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs. Gemma 3 27B is Google's latest open source model, successor to Gemma.","endpoints":["completion"],"features":["structured-outputs","vision"],"pricing":{"inputPerM":1190,"outputPerM":2000},"source":"public-provider-conf"},"vision":true},{"id":"google/gemma-3-12b-it","name":"Gemma3 12B","display_name":"Gemma3 12B","type":"chat","context_length":131072,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","vision"],"pricing":{"inputPerM":500,"outputPerM":1000},"source":"public-provider-conf"},"vision":true},{"id":"thudm/glm-4.1v-9b-thinking","name":"GLM 4.1V 9B Thinking","display_name":"GLM 4.1V 9B Thinking","type":"chat","context_length":65536,"max_output_tokens":8000,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"GLM-4.1V-9B-Thinking is an open-source Vision-Language Model (VLM) jointly released by Zhipu AI and Tsinghua University’s KEG Lab, specifically designed to handle complex multimodal cognitive tasks. Built upon the GLM-4-9B-0414 base model, it integrates Chain-of-Thought (CoT) reasoning and employs reinforcement learning strategies, significantly enhancing its cross-modal reasoning capabilities and stability. As a lightweight model with 9B parameters, it strikes an optimal balance between deployment efficiency and performance. Across 28 authoritative benchmark evaluations, it matches or surpasses the performance of the 72B-parameter Qwen-2.5-VL-72B in 18 metrics. The model excels in tasks such as image-text understanding, mathematical and scientific reasoning, and video comprehension, while also supporting 4K-resolution images and arbitrary aspect ratios.","features":["vision","reasoning"],"pricing":{"inputPerM":350,"outputPerM":1380},"source":"public-provider-conf"},"vision":true},{"id":"zai-org/glm-4.5v","name":"GLM 4.5V","display_name":"GLM 4.5V","type":"chat","context_length":65536,"max_output_tokens":16384,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Z.ai's GLM-4.5V sets a new standard in visual reasoning, achieving SOTA performance across 42 benchmarks among open-source models. Beyond benchmarks, it excels in real-world applications through hybrid training, enabling comprehensive visual understanding—from image/video analysis and GUI interaction to complex document processing and precise visual grounding.  In China's GeoGuessr challenge, GLM-4.5V surpassed 99% of 21,000 human players within 16 hours, reaching 66th place in a week. Built on the GLM-4.5-Air foundation and inheriting GLM-4.1V-Thinking's approach, it leverages a 106B-parameter MoE architecture for scalable, efficient performance. This model bridges advanced AI research with practical deployment, delivering unmatched visual intelligence","endpoints":["completion"],"features":["structured-outputs","function-calling","vision","reasoning","video"],"pricing":{"inputPerM":6000,"outputPerM":18000},"source":"public-provider-conf"},"vision":true},{"id":"zai-org/glm-4.5","name":"GLM-4.5","display_name":"GLM-4.5","type":"chat","context_length":131072,"max_output_tokens":98304,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"GLM-4.5 Series Models are foundation models specifically engineered for intelligent agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application demands. As a hybrid reasoning system, it offers dual operational modes: - Thinking Mode: Enables complex reasoning, tool invocation, and strategic planning - Non-Thinking Mode: Delivers low-latency responses for real-time interactions This architecture bridges high-performance AI with adaptive functionality for dynamic agent environments.","endpoints":["completion"],"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":6000,"outputPerM":22000},"source":"public-provider-conf"},"vision":false},{"id":"gpt-4.1","name":"gpt-4.1","display_name":"gpt-4.1","type":"chat","context_length":1047576,"max_output_tokens":32768,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["function-calling","structured-outputs","vision"],"pricing":{"inputPerM":20000,"outputPerM":80000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-4.1-mini","name":"gpt-4.1-mini","display_name":"gpt-4.1-mini","type":"chat","context_length":1047576,"max_output_tokens":32768,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["function-calling","structured-outputs","vision"],"pricing":{"inputPerM":4000,"outputPerM":16000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-4.1-nano","name":"gpt-4.1-nano","display_name":"gpt-4.1-nano","type":"chat","context_length":1047576,"max_output_tokens":32768,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":1000,"outputPerM":4000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-4o","name":"gpt-4o","display_name":"gpt-4o","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-4o","features":["function-calling","structured-outputs","vision"],"pricing":{"inputPerM":25000,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-4o-mini","name":"gpt-4o-mini","display_name":"gpt-4o-mini","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-4o-mini","features":["function-calling","structured-outputs","vision"],"pricing":{"inputPerM":1500,"outputPerM":6000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5","name":"gpt-5","display_name":"gpt-5","type":"chat","context_length":400000,"max_output_tokens":128000,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision","reasoning"],"pricing":{"inputPerM":12500,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5-chat-latest","name":"gpt-5-chat-latest","display_name":"gpt-5-chat-latest","type":"chat","context_length":400000,"max_output_tokens":128000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":12500,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5-codex","name":"gpt-5-codex","display_name":"gpt-5-codex","type":"chat","context_length":400000,"max_output_tokens":128000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":12500,"outputPerM":100000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5-mini","name":"gpt-5-mini","display_name":"gpt-5-mini","type":"chat","context_length":400000,"max_output_tokens":128000,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision","reasoning"],"pricing":{"inputPerM":2500,"outputPerM":20000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5-nano","name":"gpt-5-nano","display_name":"gpt-5-nano","type":"chat","context_length":400000,"max_output_tokens":128000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":500,"outputPerM":4000},"source":"public-provider-conf"},"vision":true},{"id":"gpt-5-pro","name":"gpt-5-pro","display_name":"gpt-5-pro","type":"chat","context_length":400000,"max_output_tokens":272000,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":150000,"outputPerM":1200000},"source":"public-provider-conf"},"vision":true},{"id":"grok-3","name":"grok-3","display_name":"grok-3","type":"chat","context_length":131072,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"grok-3-mini","name":"grok-3-mini","display_name":"grok-3-mini","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":3000,"outputPerM":5000},"source":"public-provider-conf"},"vision":false},{"id":"grok-4-0709","name":"grok-4-0709","display_name":"grok-4-0709","type":"chat","context_length":256000,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"features":["structured-outputs","function-calling","vision"],"pricing":{"inputPerM":30000,"outputPerM":150000},"source":"public-provider-conf"},"vision":true},{"id":"grok-4-fast-non-reasoning","name":"grok-4-fast-non-reasoning","display_name":"grok-4-fast-non-reasoning","type":"chat","context_length":2000000,"max_output_tokens":2000000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":2000,"outputPerM":5000},"source":"public-provider-conf"},"vision":true},{"id":"grok-4-fast-reasoning","name":"grok-4-fast-reasoning","display_name":"grok-4-fast-reasoning","type":"chat","context_length":2000000,"max_output_tokens":2000000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text","image"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":2000,"outputPerM":5000},"source":"public-provider-conf"},"vision":true},{"id":"grok-code-fast-1","name":"grok-code-fast-1","display_name":"grok-code-fast-1","type":"chat","context_length":256000,"max_output_tokens":256000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":2000,"outputPerM":15000},"source":"public-provider-conf"},"vision":false},{"id":"moonshotai/kimi-k2-0905","name":"Kimi K2 0905","display_name":"Kimi K2 0905","type":"chat","context_length":262144,"max_output_tokens":262144,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.","endpoints":["completion","anthropic"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":6000,"outputPerM":25000},"source":"public-provider-conf"},"vision":false},{"id":"moonshotai/kimi-k2-instruct","name":"Kimi K2 Instruct","display_name":"Kimi K2 Instruct","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.Specifically designed for tool use, reasoning, and autonomous problem-solving.","endpoints":["anthropic"],"features":["function-calling","structured-outputs"],"pricing":{"inputPerM":5700,"outputPerM":23000},"source":"public-provider-conf"},"vision":false},{"id":"sao10k/l3-70b-euryale-v2.1","name":"L3 70B Euryale V2.1","display_name":"L3 70B Euryale V2.1","type":"chat","context_length":8192,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The uncensored llama3 model is a powerhouse of creativity, excelling in both roleplay and story writing. It offers a liberating experience during roleplays, free from any restrictions. This model stands out for its immense creativity, boasting a vast array of unique ideas and plots, truly a treasure trove for those seeking originality. Its unrestricted nature during roleplays allows for the full breadth of imagination to unfold, akin to an enhanced, big-brained version of Stheno. Perfect for creative minds seeking a boundless platform for their imaginative expressions, the uncensored llama3 model is an ideal choice","endpoints":["completion"],"features":["function-calling"],"pricing":{"inputPerM":14800,"outputPerM":14800},"source":"public-provider-conf"},"vision":false},{"id":"Sao10K/L3-8B-Stheno-v3.2","name":"L3 8B Stheno V3.2","display_name":"L3 8B Stheno V3.2","type":"chat","context_length":8192,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Sao10K/L3-8B-Stheno-v3.2 is a highly skilled actor that excels at fully immersing itself in any role assigned.","endpoints":["completion"],"features":["function-calling"],"pricing":{"inputPerM":500,"outputPerM":500},"source":"public-provider-conf"},"vision":false},{"id":"sao10k/l31-70b-euryale-v2.2","name":"L31 70B Euryale V2.2","display_name":"L31 70B Euryale V2.2","type":"chat","context_length":8192,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Euryale L3.1 70B v2.2 is a model focused on creative roleplay from Sao10k. It is the successor of Euryale L3 70B v2.1.","endpoints":["completion"],"features":["function-calling"],"pricing":{"inputPerM":14800,"outputPerM":14800},"source":"public-provider-conf"},"vision":false},{"id":"meta-llama/llama-3.1-8b-instruct","name":"Llama 3.1 8B Instruct","display_name":"Llama 3.1 8B Instruct","type":"chat","context_length":16384,"max_output_tokens":16384,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Meta's latest class of models, Llama 3.1, launched with a variety of sizes and configurations. The 8B instruct-tuned version is particularly fast and efficient. It has demonstrated strong performance in human evaluations, outperforming several leading closed-source models.","endpoints":["completion"],"pricing":{"inputPerM":200,"outputPerM":500},"source":"public-provider-conf"},"vision":false},{"id":"meta-llama/llama-3.2-3b-instruct","name":"Llama 3.2 3B Instruct","display_name":"Llama 3.2 3B Instruct","type":"chat","context_length":32768,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out)","endpoints":["completion"],"features":["function-calling"],"pricing":{"inputPerM":300,"outputPerM":500},"source":"public-provider-conf"},"vision":false},{"id":"meta-llama/llama-3.3-70b-instruct","name":"Llama 3.3 70B Instruct","display_name":"Llama 3.3 70B Instruct","type":"chat","context_length":131072,"max_output_tokens":120000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.  Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.","endpoints":["completion"],"features":["function-calling"],"pricing":{"inputPerM":1300,"outputPerM":3900},"source":"public-provider-conf"},"vision":false},{"id":"meta-llama/llama-4-maverick-17b-128e-instruct-fp8","name":"Llama 4 Maverick Instruct","display_name":"Llama 4 Maverick Instruct","type":"chat","context_length":1048576,"max_output_tokens":1048576,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.  Maverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.","endpoints":["completion"],"features":["function-calling","vision"],"pricing":{"inputPerM":1700,"outputPerM":8500},"source":"public-provider-conf"},"vision":true},{"id":"meta-llama/llama-4-scout-17b-16e-instruct","name":"Llama 4 Scout Instruct","display_name":"Llama 4 Scout Instruct","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.  Built for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.","endpoints":["completion"],"features":["function-calling","vision"],"pricing":{"inputPerM":1000,"outputPerM":5000},"source":"public-provider-conf"},"vision":true},{"id":"minimaxai/minimax-m1-80k","name":"MiniMax M1","display_name":"MiniMax M1","type":"chat","context_length":1000000,"max_output_tokens":40000,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"MiniMax-M1: The World's First Open-Weight, Large-Scale Hybrid Attention Inference Model  MiniMax-M1 adopts a Mixture of Experts (MoE) architecture and integrates the Flash Attention mechanism. The model contains a total of 456 billion parameters, with 45.9 billion parameters activated per token.  Natively, the M1 model supports a context length of 1 million tokens—8 times that of DeepSeek R1. Additionally, by combining the CISPO algorithm with an efficient hybrid attention design for reinforcement learning training, MiniMax-M1 achieves industry-leading performance in long-context reasoning and real-world software engineering scenarios.","features":["function-calling","reasoning"],"pricing":{"inputPerM":5500,"outputPerM":22000},"source":"public-provider-conf"},"vision":false},{"id":"mistralai/mistral-7b-instruct","name":"Mistral 7B Instruct","display_name":"Mistral 7B Instruct","type":"chat","context_length":32768,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.","endpoints":["completion"],"pricing":{"inputPerM":290,"outputPerM":590},"source":"public-provider-conf"},"vision":false},{"id":"mistralai/mistral-nemo","name":"Mistral Nemo","display_name":"Mistral Nemo","type":"chat","context_length":60288,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA. The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. It supports function calling and is released under the Apache 2.0 license.","endpoints":["completion"],"features":["structured-outputs"],"pricing":{"inputPerM":400,"outputPerM":1700},"source":"public-provider-conf"},"vision":false},{"id":"gryphe/mythomax-l2-13b","name":"Mythomax L2 13B","display_name":"Mythomax L2 13B","type":"chat","context_length":4096,"max_output_tokens":4096,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time).","endpoints":["completion"],"pricing":{"inputPerM":900,"outputPerM":900},"source":"public-provider-conf"},"vision":false},{"id":"o1","name":"o1","display_name":"o1","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-o1","features":["function-calling"],"pricing":{"inputPerM":150000,"outputPerM":600000},"source":"public-provider-conf"},"vision":false},{"id":"o1-mini","name":"o1-mini","display_name":"o1-mini","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-o1-mini","features":["function-calling"],"pricing":{"inputPerM":11000,"outputPerM":44000},"source":"public-provider-conf"},"vision":false},{"id":"o3","name":"o3","display_name":"o3","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-o3","features":["function-calling"],"pricing":{"inputPerM":100000,"outputPerM":400000},"source":"public-provider-conf"},"vision":false},{"id":"o3-mini","name":"o3-mini","display_name":"o3-mini","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"chatgpt-o3-mini","features":["function-calling"],"pricing":{"inputPerM":11000,"outputPerM":44000},"source":"public-provider-conf"},"vision":false},{"id":"openai/gpt-oss-120b","name":"OpenAI GPT OSS 120B","display_name":"OpenAI GPT OSS 120B","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.","endpoints":["completion"],"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":1000,"outputPerM":5000},"source":"public-provider-conf"},"vision":false},{"id":"openai/gpt-oss-20b","name":"OpenAI: GPT OSS 20B","display_name":"OpenAI: GPT OSS 20B","type":"chat","context_length":131072,"max_output_tokens":32768,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.","endpoints":["completion"],"features":["structured-outputs","reasoning"],"pricing":{"inputPerM":500,"outputPerM":2000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen-2.5-72b-instruct","name":"Qwen 2.5 72B Instruct","display_name":"Qwen 2.5 72B Instruct","type":"chat","context_length":32000,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.","endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":3800,"outputPerM":4000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen-mt-plus","name":"Qwen MT Plus","display_name":"Qwen MT Plus","type":"chat","context_length":4096,"max_output_tokens":2048,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen-MT is a large language model optimized for machine translation, built upon the foundation of the Tongyi Qianwen model. It supports translation across 92 languages — including Chinese, English, Japanese, Korean, French, Spanish, German, Thai, Indonesian, Vietnamese, Arabic, and more — enabling seamless multilingual communication.","pricing":{"inputPerM":2500,"outputPerM":7500},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen2.5-7b-instruct","name":"Qwen2.5 7B Instruct","display_name":"Qwen2.5 7B Instruct","type":"chat","context_length":32000,"max_output_tokens":32000,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. - Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. - Long-context Support up to 128K tokens and can generate up to 8K tokens. - Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.","endpoints":["completion"],"features":["function-calling","structured-outputs"],"pricing":{"inputPerM":700,"outputPerM":700},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen2.5-vl-72b-instruct","name":"Qwen2.5 VL 72B Instruct","display_name":"Qwen2.5 VL 72B Instruct","type":"chat","context_length":32768,"max_output_tokens":32768,"reasoning":{"supported":false},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen2.5-VL, the latest vision-language model in the Qwen2.5 series, delivers enhanced multimodal capabilities including advanced visual comprehension for object/text recognition, chart/layout analysis, and agent-based dynamic tool orchestration. It processes long-form videos (>1 hour) with key event detection while enabling precise spatial annotation through bounding boxes or coordinate points. The model specializes in structured data extraction from scanned documents (invoices, tables, etc.) and achieves state-of-the-art performance across multimodal benchmarks encompassing image understanding, temporal video analysis, and agent task evaluations.","endpoints":["completion"],"features":["vision","video"],"pricing":{"inputPerM":8000,"outputPerM":8000},"source":"public-provider-conf"},"vision":true},{"id":"qwen/qwen3-235b-a22b-fp8","name":"Qwen3 235B A22B","display_name":"Qwen3 235B A22B","type":"chat","context_length":40960,"max_output_tokens":20000,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Achieves effective integration of inference and non-inference modes, enabling seamless switching between modes during conversations. The model's inference capability significantly surpasses that of QwQ, and its general capabilities exceed those of Qwen2.5-72B-Instruct, reaching the state-of-the-art (SOTA) level among models of the same scale.","endpoints":["completion"],"features":["reasoning"],"pricing":{"inputPerM":2000,"outputPerM":8000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-235b-a22b-instruct-2507","name":"Qwen3 235B A22B Instruct 2507","display_name":"Qwen3 235B A22B Instruct 2507","type":"chat","context_length":131072,"max_output_tokens":16384,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks). Compared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.","endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":1500,"outputPerM":8000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-235b-a22b-thinking-2507","name":"Qwen3 235B A22b Thinking 2507","display_name":"Qwen3 235B A22b Thinking 2507","type":"chat","context_length":131072,"max_output_tokens":131072,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"The Qwen3-235B-A22B-Thinking-2507 represents the newest thinking-enabled model in the Qwen3 series, delivering groundbreaking improvements in reasoning capabilities. This advanced AI demonstrates significantly enhanced performance across logical reasoning, mathematics, scientific analysis, coding tasks, and academic benchmarks - matching or even surpassing human-expert level performance to achieve state-of-the-art results among open-source thinking models. Beyond its exceptional reasoning skills, the model shows markedly better general capabilities including more precise instruction following, sophisticated tool usage, highly natural text generation, and improved alignment with human preferences. It also features enhanced 256K long-context understanding, allowing it to maintain coherence and depth across extended documents and complex discussions.","endpoints":["completion","anthropic"],"features":["function-calling","structured-outputs","reasoning"],"pricing":{"inputPerM":3000,"outputPerM":30000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-30b-a3b-fp8","name":"Qwen3 30B A3B","display_name":"Qwen3 30B A3B","type":"chat","context_length":40960,"max_output_tokens":20000,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.","endpoints":["completion"],"features":["reasoning"],"pricing":{"inputPerM":900,"outputPerM":4500},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-32b-fp8","name":"Qwen3 32B","display_name":"Qwen3 32B","type":"chat","context_length":40960,"max_output_tokens":20000,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.","endpoints":["completion"],"features":["reasoning"],"pricing":{"inputPerM":1000,"outputPerM":4500},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-8b-fp8","name":"Qwen3 8B","display_name":"Qwen3 8B","type":"chat","context_length":128000,"max_output_tokens":20000,"reasoning":{"supported":true,"default":true},"tool_call":false,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Achieves effective integration of reasoning and non-reasoning modes, allowing seamless mode switching during conversations. Its reasoning capability reaches state-of-the-art (SOTA) performance among models of the same scale, and its general capabilities significantly outperform those of Qwen2.5-7B.","endpoints":["completion"],"features":["reasoning"],"pricing":{"inputPerM":350,"outputPerM":1380},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-coder-480b-a35b-instruct","name":"Qwen3 Coder 480B A35B Instruct","display_name":"Qwen3 Coder 480B A35B Instruct","type":"chat","context_length":262144,"max_output_tokens":65536,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet’s performance in agentic programming, browser automation, and core development tasks. With native 256K context (extendable to 1M tokens via YaRN), it excels at repository-scale analysis and features specialized function-call support for platforms like Qwen Code and CLINE—making it ideal for complex, real-world development workflows.","endpoints":["completion","anthropic"],"features":["function-calling","structured-outputs"],"pricing":{"inputPerM":2900,"outputPerM":12000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-next-80b-a3b-instruct","name":"Qwen3 Next 80B A3B Instruct","display_name":"Qwen3 Next 80B A3B Instruct","type":"chat","context_length":65536,"max_output_tokens":65536,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Instruct performs comparably to our flagship model Qwen3-235B-A22B-Instruct-2507, and shows clear advantages in tasks requiring ultra-long context (up to 256K tokens).","endpoints":["completion","anthropic"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":1500,"outputPerM":15000},"source":"public-provider-conf"},"vision":false},{"id":"qwen/qwen3-next-80b-a3b-thinking","name":"Qwen3 Next 80B A3B Thinking","display_name":"Qwen3 Next 80B A3B Thinking","type":"chat","context_length":65536,"max_output_tokens":65536,"reasoning":{"supported":true,"default":true},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance. The Qwen3-Next-80B-A3B-Thinking excels at complex reasoning tasks — outperforming higher-cost models like Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking, outpeforming the closed-source Gemini-2.5-Flash-Thinking on multiple benchmarks, and approaching the performance of our top-tier model Qwen3-235B-A22B-Thinking-2507.","endpoints":["completion","anthropic"],"features":["structured-outputs","function-calling","reasoning"],"pricing":{"inputPerM":1500,"outputPerM":15000},"source":"public-provider-conf"},"vision":false},{"id":"sao10k/l3-8b-lunaris","name":"Sao10k L3 8B Lunaris","display_name":"Sao10k L3 8B Lunaris","type":"chat","context_length":8192,"max_output_tokens":8192,"reasoning":{"supported":false},"tool_call":true,"modalities":{"input":["text"],"output":["text"]},"metadata":{"description":"A generalist / roleplaying model merge based on Llama 3.","endpoints":["completion"],"features":["structured-outputs","function-calling"],"pricing":{"inputPerM":500,"outputPerM":500},"source":"public-provider-conf"},"vision":false}],"metadata":{"source":"public-provider-conf"}}